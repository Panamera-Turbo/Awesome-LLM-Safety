<!DOCTYPE html>
<html lang="zh-CN">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   Jailbreaks&amp;Attack - Awesome LLM-Safety
  </title>
  <link href="../style.css" rel="stylesheet"/>
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet"/>
  <style>
   .markdown-content {
            padding: 20px;
        }
        .markdown-content h1 {
            font-size: 2rem;
            color: var(--primary-color);
            margin-bottom: 1.5rem;
            padding-bottom: 0.5rem;
            border-bottom: 2px solid var(--border-color);
        }
        .markdown-content h2 {
            font-size: 1.6rem;
            color: var(--secondary-color);
            margin-top: 2rem;
            margin-bottom: 1rem;
        }
        .markdown-content table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            font-size: 0.9rem;
        }
        .markdown-content th, .markdown-content td {
            padding: 12px 15px;
            border: 1px solid var(--border-color);
        }
        .markdown-content th {
            background-color: var(--primary-color);
            color: white;
            font-weight: bold;
            text-align: left;
        }
        .markdown-content tr:nth-child(even) {
            background-color: var(--light-bg);
        }
        .markdown-content tr:hover {
            background-color: #ddd;
        }
        .back-to-home {
            display: inline-block;
            margin: 20px 0;
            padding: 10px 15px;
            background-color: var(--primary-color);
            color: white;
            border-radius: 4px;
            text-decoration: none;
            font-weight: bold;
        }
        .back-to-home:hover {
            background-color: var(--secondary-color);
            color: white;
        }
  </style>
 </head>
 <body>
  <header>
   <div class="container">
    <h1>
     üõ°Ô∏è Awesome LLM-Safety üõ°Ô∏è
    </h1>
    <div class="language-switch">
     <a href="../index.html">
      English
     </a>
     |
     <a class="active" href="../index_cn.html">
      ‰∏≠Êñá
     </a>
    </div>
   </div>
  </header>
  <div class="container">
   <a class="back-to-home" href="../index_cn.html">
    <i class="fas fa-arrow-left">
    </i>
    ËøîÂõûÈ¶ñÈ°µ
   </a>
   <div class="markdown-content">
    <h1>
     Jailbreaks&amp;Attack
    </h1>
    <h2>
     Different from the main READMEüïµÔ∏è
    </h2>
    <ul>
     <li>
      Within this subtopic, we will be updating with the latest articles. This will help researchers in this area to quickly understand recent trends.
     </li>
     <li>
      In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
     </li>
     <li>
      Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"
     </li>
    </ul>
    <h2>
     üìëPapers
    </h2>
    <table>
     <thead>
      <tr>
       <th style="text-align: center;">
        Date
       </th>
       <th style="text-align: center;">
        Institute
       </th>
       <th style="text-align: center;">
        Publication
       </th>
       <th style="text-align: center;">
        Paper
       </th>
       <th style="text-align: center;">
        Keywords
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td style="text-align: center;">
        20.12
       </td>
       <td style="text-align: center;">
        Google
       </td>
       <td style="text-align: center;">
        USENIX Security 2021
       </td>
       <td style="text-align: center;">
        <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting">
         Extracting Training Data from Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Verbatim Text Sequences
        </strong>
        &amp;
        <strong>
         Rank Likelihood
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        22.11
       </td>
       <td style="text-align: center;">
        AE Studio
       </td>
       <td style="text-align: center;">
        NIPS2022(ML Safety Workshop)
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2211.09527">
         Ignore Previous Prompt: Attack Techniques For Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection
        </strong>
        &amp;
        <strong>
         Misaligned
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.02
       </td>
       <td style="text-align: center;">
        Saarland University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2302.12173">
         Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Prompting
        </strong>
        &amp;
        <strong>
         Indirect Prompt Injection
        </strong>
        &amp;
        <strong>
         LLM-Integrated Applications
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.04
       </td>
       <td style="text-align: center;">
        Hong Kong University of Science and Technology
       </td>
       <td style="text-align: center;">
        EMNLP2023(findings)
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2304.05197">
         Multi-step Jailbreaking Privacy Attacks on ChatGPT
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Privacy
        </strong>
        &amp;
        <strong>
         Jailbreaks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.04
       </td>
       <td style="text-align: center;">
        University of Michigan&amp;Arizona State University&amp;NVIDIA
       </td>
       <td style="text-align: center;">
        NAACL2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2304.14475">
         ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Textual Backdoor Attack
        </strong>
        &amp;
        <strong>
         Blackbox Generative Model
        </strong>
        &amp;
        <strong>
         Trigger Detection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.05
       </td>
       <td style="text-align: center;">
        Jinan University, Hong Kong University of Science and Technology, Nanyang Technological University, Zhejiang University
       </td>
       <td style="text-align: center;">
        EMNLP 2023
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2305.01219">
         Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.05
       </td>
       <td style="text-align: center;">
        Nanyang Technological University, University of New South Wales, Virginia Tech
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2305.13860">
         Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study
        </a>
       </td>
       <td style="text-align: center;">
        Large
        <strong>
         Jailbreak
        </strong>
        &amp;
        <strong>
         Prompt Engineering
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.06
       </td>
       <td style="text-align: center;">
        Princeton University
       </td>
       <td style="text-align: center;">
        AAAI 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2306.13213">
         Visual Adversarial Examples Jailbreak Aligned Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Visual Language Models
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         AI Alignment
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.06
       </td>
       <td style="text-align: center;">
        Nanyang Technological University, University of New South Wales, Huazhong University of Science and Technology, Southern University of Science and Technology, Tianjin University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2306.05499">
         Prompt Injection attack against LLM-integrated Applications
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         &amp;LLM-integrated Applications
        </strong>
        &amp;
        <strong>
         Security Risks
        </strong>
        &amp;
        <strong>
         Prompt Injection Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.06
       </td>
       <td style="text-align: center;">
        Google
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2306.15447">
         Are aligned neural networks adversarially aligned?
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal
        </strong>
        &amp;
        <strong>
         Jailbreak
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.07
       </td>
       <td style="text-align: center;">
        CMU
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2307.15043">
         Universal and Transferable Adversarial Attacks on Aligned Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak
        </strong>
        &amp;
        <strong>
         Transferable Attack
        </strong>
        &amp;
        <strong>
         Adversarial Attack
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.07
       </td>
       <td style="text-align: center;">
        Language Technologies Institute Carnegie Mellon University
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2307.06865">
         Prompts Should not be Seen as Secrets: Systematically Measuring Prompt Extraction Attack Success
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Extraction
        </strong>
        &amp;
        <strong>
         Attack Success Measurement
        </strong>
        &amp;
        <strong>
         Defensive Strategies
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.07
       </td>
       <td style="text-align: center;">
        Nanyang Technological University
       </td>
       <td style="text-align: center;">
        NDSS2023
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2307.08715">
         MasterKey: Automated Jailbreak Across Multiple Large Language Model Chatbots
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak
        </strong>
        &amp;
        <strong>
         Reverse-Engineering
        </strong>
        &amp;
        <strong>
         Automatic Generation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.07
       </td>
       <td style="text-align: center;">
        Cornell Tech
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2307.10490">
         Abusing Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multi-Modal LLMs
        </strong>
        &amp;
        <strong>
         Indirect Instruction Injection
        </strong>
        &amp;
        <strong>
         Adversarial Perturbations
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.07
       </td>
       <td style="text-align: center;">
        UNC Chapel Hill, Google DeepMind, ETH Zurich
       </td>
       <td style="text-align: center;">
        AdvML Frontiers Workshop 2023
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2307.14692">
         Backdoor Attacks for In-Context Learning with Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Attacks
        </strong>
        &amp;
        <strong>
         In-Context Learning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.07
       </td>
       <td style="text-align: center;">
        Google DeepMind
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2307.15008">
         Large language models (LLMs) are now highly capable at a diverse range of tasks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Machine Learning
        </strong>
        &amp;
        <strong>
         AI-Guardian
        </strong>
        &amp;
        <strong>
         Defense Robustness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.08
       </td>
       <td style="text-align: center;">
        CISPA Helmholtz Center for Information Security; NetApp
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2308.03825">
         ‚ÄúDo Anything Now‚Äù: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Prompts
        </strong>
        &amp;
        <strong>
         Adversarial Prompts
        </strong>
        &amp;
        <strong>
         Proactive Detection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.09
       </td>
       <td style="text-align: center;">
        Ben-Gurion University, DeepKeep
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2309.01446">
         OPEN SESAME! UNIVERSAL BLACK BOX JAILBREAKING OF LARGE LANGUAGE MODELS
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Genetic Algorithm
        </strong>
        &amp;
        <strong>
         Adversarial Prompt
        </strong>
        &amp;
        <strong>
         Black Box Jailbreak
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.10
       </td>
       <td style="text-align: center;">
        Princeton University, Virginia Tech, IBM Research, Stanford University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2310.03693">
         FINE-TUNING ALIGNED LANGUAGE MODELS COMPROMISES SAFETY EVEN WHEN USERS DO NOT INTEND TO!
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Fine-tuning
        </strong>
        <strong>
         Safety Risks
        </strong>
        &amp;
        <strong>
         Adversarial Training
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.10
       </td>
       <td style="text-align: center;">
        University of California Santa Barbara, Fudan University, Shanghai AI Laboratory
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2310.02949">
         SHADOW ALIGNMENT: THE EASE OF SUBVERTING SAFELY-ALIGNED LANGUAGE MODELS
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         AI Safety
        </strong>
        &amp;
        <strong>
         Malicious Use
        </strong>
        &amp;
        <strong>
         Fine-tuning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.10
       </td>
       <td style="text-align: center;">
        Peking University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2310.06387">
         Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         In-Context Learning
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         In-Context Demonstrations
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.10
       </td>
       <td style="text-align: center;">
        University of Pennsylvania
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2310.08419">
         Jailbreaking Black Box Large Language Models in Twenty Queries
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Automatic Iterative Refinement
        </strong>
        &amp;
        <strong>
         Jailbreak
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.10
       </td>
       <td style="text-align: center;">
        University of Maryland College Park, Adobe Research
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2310.15140">
         AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         Interpretabilty
        </strong>
        &amp;
        <strong>
         Jailbreaking
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        MBZUAI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.00508">
         Robustness Tests for Automatic Machine Translation Metrics with Adversarial Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarially-synthesized Texts
        </strong>
        &amp;
        <strong>
         Word-level Attacks
        </strong>
        &amp;
        <strong>
         Evaluation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Palisade Research
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.00117">
         BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Remove Safety Fine-tuning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        University of Twente
       </td>
       <td style="text-align: center;">
        ICNLSP 2023
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.01873">
         Efficient Black-Box Adversarial Attacks on Neural Text Detectors
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Misclassification
        </strong>
        &amp;
        <strong>
         Adversarial attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        PRISM AI&amp;Harmony Intelligenc&amp;Leap Laboratories
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.03348">
         Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Persona-modulation Attacks
        </strong>
        &amp;
        <strong>
         Jailbreaks
        </strong>
        &amp;
        <strong>
         Automated Prompt
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Tsinghua University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.05608">
         Jailbreaking Large Vision-Language Models via Typographic Visual Prompts
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Typographic Attack
        </strong>
        &amp;
        <strong>
         Multi-modal
        </strong>
        &amp;
        <strong>
         Safety Evaluation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Huazhong University of Science and Technology, Tsinghua University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.06062">
         Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Membership Inference Attacks
        </strong>
        &amp;
        <strong>
         Privacy and Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Nanjing University, Meituan Inc
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.08268">
         A Wolf in Sheep‚Äôs Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Prompts
        </strong>
        &amp;
        <strong>
         Safety Alignment
        </strong>
        &amp;
        <strong>
         Safeguard Effectiveness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Google DeepMind
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.07587">
         Frontier Language Models Are Not Robust to Adversarial Arithmetic or "What Do I Need To Say So You Agree 2+2=5?"
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Arithmetic
        </strong>
        &amp;
        <strong>
         Model Robustness
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        University of Illinois Chicago, Texas A&amp;M University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.08598">
         DALA: A Distribution-Aware LoRA-Based Adversarial Attack against Pre-trained Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Attack
        </strong>
        &amp;
        <strong>
         Distribution-Aware
        </strong>
        &amp;
        <strong>
         LoRA-Based Attack
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Illinois Institute of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.09433">
         Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment
        </a>
       </td>
       <td style="text-align: center;">
        Backdoor Activation Attack&amp;Large Language Models&amp;AI Safety&amp;Activation Steering&amp;Trojan Steering Vectors
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Wayne State University
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.09948">
         Hijacking Large Language Models via Adversarial In-Context Learning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         Gradient-Based Prompt Search
        </strong>
        &amp;
        <strong>
         Adversarial Suffixes
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Hong Kong Baptist University, Shanghai Jiao Tong University, Shanghai AI Laboratory, The University of Sydney
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.03191">
         DeepInception: Hypnotize Large Language Model to Be Jailbreaker
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak
        </strong>
        &amp;
        <strong>
         DeepInception
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Xi‚Äôan Jiaotong-Liverpool University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.11861">
         Generating Valid and Natural Adversarial Examples with Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial examples
        </strong>
        &amp;
        <strong>
         Text classification
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Michigan State University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.11796">
         Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Transferable Attacks
        </strong>
        &amp;
        <strong>
         AI Systems
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Tsinghua University &amp; Kuaishou Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.11855v1">
         Evil Geniuses: Delving into the Safety of LLM-based Agents
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM-based Agents
        </strong>
        &amp;
        <strong>
         Safety
        </strong>
        &amp;
        <strong>
         Malicious Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Cornell University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.13647">
         Language Model Inversion
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Model Inversion
        </strong>
        &amp;
        <strong>
         Prompt Reconstruction
        </strong>
        &amp;
        <strong>
         Privacy
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        ETH Zurich
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.14455">
         Universal Jailbreak Backdoors from Poisoned Human Feedback
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         RLHF
        </strong>
        &amp;
        <strong>
         Backdoor Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        UC Santa Cruz, UNC-Chapel Hill
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.16101">
         How Many Are in This Image? A Safety Evaluation Benchmark for Vision LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision Large Language Models
        </strong>
        &amp;
        <strong>
         Safety Evaluation
        </strong>
        &amp;A
        <strong>
         dversarial Robustness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Texas Tech University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.14876">
         Exploiting Large Language Models (LLMs) through Deception Techniques and Persuasion Principles
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Social Engineering
        </strong>
        &amp;
        <strong>
         Security
        </strong>
        &amp;
        <strong>
         Prompt Engineering
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Johns Hopkins University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.15551">
         Instruct2Attack: Language-Guided Semantic Adversarial Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Language-guided Attacks
        </strong>
        &amp;
        <strong>
         Latent Diffusion Models
        </strong>
        &amp;
        <strong>
         Adversarial Attack
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Google DeepMind, University of Washington, Cornell, CMU, UC Berkeley, ETH Zurich
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.17035">
         Scalable Extraction of Training Data from (Production) Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Extractable Memorization
        </strong>
        &amp;
        <strong>
         Data Extraction
        </strong>
        &amp;
        <strong>
         Adversary Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        University of Maryland, Mila, Towards AI, Stanford, Technical University of Sofia, University of Milan, NYU
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.16119">
         Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Hacking
        </strong>
        &amp;
        <strong>
         Security Threats
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        University of Washington, UIUC, Pennsylvania State University, University of Chicago
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.16153">
         IDENTIFYING AND MITIGATING VULNERABILITIES IN LLM-INTEGRATED APPLICATIONS
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM-Integrated Applications
        </strong>
        &amp;
        <strong>
         Attack Surfaces
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Jinan University, Guangzhou Xuanyuan Research Institute Co. Ltd., The Hong Kong Polytechnic University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.17429">
         TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP Models via GPT4
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt-based Learning
        </strong>
        &amp;
        <strong>
         Backdoor Attack
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Nanjing University&amp;Meituan Inc.
       </td>
       <td style="text-align: center;">
        NAACL2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.08268">
         A Wolf in Sheep‚Äôs Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Prompts
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
        &amp;
        <strong>
         Automated Framework
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        University of Southern California
       </td>
       <td style="text-align: center;">
        NAACL2024(findings)
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.09827">
         Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Large Language Models
        </strong>
        &amp;
        <strong>
         Cognitive Overload
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        The Pennsylvania State University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.00027">
         Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Injection
        </strong>
        &amp;
        <strong>
         Safety Alignment
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        Drexel University
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.02003">
         A Survey on Large Language Model (LLM) Security and Privacy: The Good the Bad and the Ugly
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Security
        </strong>
        &amp;
        <strong>
         Privacy
        </strong>
        &amp;
        <strong>
         Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        Yale University, Robust Intelligence
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.02119">
         Tree of Attacks: Jailbreaking Black-Box LLMs Automatically
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Tree of Attacks with Pruning (TAP)
        </strong>
        &amp;
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Prompt Generation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        Independent (Now at Google DeepMind)
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.02780">
         Scaling Laws for Adversarial Attacks on Language Model Activations
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         Language Model Activations
        </strong>
        &amp;
        <strong>
         Scaling Laws
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        Harbin Institute of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.04127">
         Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Inherent Response Tendency
        </strong>
        &amp;
        <strong>
         Affirmation Tendency
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        University of Wisconsin-Madison
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.04730">
         DeceptPrompt: Exploiting LLM-driven Code Generation via Adversarial Natural Language Instructions
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Code Generation
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         Cybersecurity
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        Carnegie Melon University, IBM Research
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.04748">
         Forcing Generative Models to Degenerate Ones: The Power of Data Poisoning Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Data Poisoning Attacks
        </strong>
        &amp;
        <strong>
         Natural Language Generation
        </strong>
        &amp;
        <strong>
         Cybersecurity
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        Purdue University
       </td>
       <td style="text-align: center;">
        NIPS2023ÔºàWorkshopÔºâ
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.04782">
         Make Them Spill the Beans! Coercive Knowledge Extraction from (Production) LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Knowledge Extraction
        </strong>
        &amp;
        <strong>
         Interrogation Techniques
        </strong>
        &amp;
        <strong>
         Cybersecurity
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        Sungkyunkwan University, University of Tennessee
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.06227">
         Poisoned ChatGPT Finds Work for Idle Hands: Exploring Developers‚Äô Coding Practices with Insecure Suggestions from Poisoned AI Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Poisoning Attacks
        </strong>
        &amp;
        <strong>
         Software Development
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        North Carolina State University, New York University, Stanford University
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.05720">
         BEYOND GRADIENT AND PRIORS IN PRIVACY ATTACKS: LEVERAGING POOLER LAYER INPUTS OF LANGUAGE MODELS IN FEDERATED LEARNING
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Federated Learning
        </strong>
        &amp;P
        <strong>
         rivacy Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        Korea Advanced Institute of Science, Graduate School of AI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.07553">
         Hijacking Context in Large Multi-modal Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Large Multi-modal Models
        </strong>
        &amp;
        <strong>
         Context Hijacking
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        Xi‚Äôan Jiaotong University, Nanyang Technological University, Singapore Management University
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.10766">
         A Mutation-Based Method for Multi-Modal Jailbreaking Attack Detection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking Detection
        </strong>
        &amp;
        <strong>
         Multi-Modal
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        Logistic and Supply Chain MultiTech R&amp;D Centre (LSCM)
       </td>
       <td style="text-align: center;">
        UbiSec-2023
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.10982">
         A Comprehensive Survey of Attack Techniques Implementation and Mitigation Strategies in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Cybersecurity Attacks
        </strong>
        &amp;
        <strong>
         Defense Strategies
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        University of Illinois Urbana-Champaign, VMware Research
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.12321">
         BYPASSING THE SAFETY TRAINING OF OPEN-SOURCE LLMS WITH PRIMING ATTACKS
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Training
        </strong>
        &amp;
        <strong>
         Priming Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        Delft University of Technology
       </td>
       <td style="text-align: center;">
        ICSE 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.11658">
         Traces of Memorisation in Large Language Models for Code
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Code Memorisation
        </strong>
        &amp;
        <strong>
         Data Extraction Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        University of Science and Technology of China, Hong Kong University of Science and Technology, Microsoft
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.14197">
         Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Indirect Prompt Injection Attacks
        </strong>
        &amp;
        <strong>
         BIPIA Benchmark
        </strong>
        &amp;
        <strong>
         Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        Nanjing University of Aeronautics and Astronautics
       </td>
       <td style="text-align: center;">
        NLPCC2023
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.15867">
         Punctuation Matters! Stealthy Backdoor Attack for Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Attack
        </strong>
        &amp;
        <strong>
         PuncAttack
        </strong>
        &amp;
        <strong>
         Stealthiness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        FAR AI, McGill University, MILA, Jagiellonian University
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.14302">
         Exploiting Novel GPT-4 APIs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Fine-Tuning
        </strong>
        &amp;
        <strong>
         Knowledge Retrieval
        </strong>
        &amp;
        <strong>
         Security Vulnerabilities
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        EPFL
       </td>
       <td style="text-align: center;">
       </td>
       <td style="text-align: center;">
        <a href="https://www.andriushchenko.me/gpt4adv.pdf">
         Adversarial Attacks on GPT-4 via Simple Random Search
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         Random Search
        </strong>
        &amp;
        <strong>
         Jailbreak
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        Logistic and Supply Chain MultiTech R&amp;D Centre (LSCM)
       </td>
       <td style="text-align: center;">
        CSDE2023
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.00991">
         A Novel Evaluation Framework for Assessing Resilience Against Prompt Injection Attacks in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Evaluation
        </strong>
        &amp;
        <strong>
         Prompt Injection
        </strong>
        &amp;
        <strong>
         Cyber Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        University of Southern California
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.03729">
         The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Engineering
        </strong>
        &amp;
        <strong>
         Text Classification
        </strong>
        &amp;
        <strong>
         Jailbreaks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        Virginia Tech, Renmin University of China, UC Davis, Stanford University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/ÈìæÊé•ÂæÖÂÆö">
         How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         AI Safety
        </strong>
        &amp;
        <strong>
         Persuasion Adversarial Prompts
        </strong>
        &amp;
        <strong>
         Jailbreak
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        Anthropic, Redwood Research, Mila Quebec AI Institute, University of Oxford
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.05566">
         SLEEPER AGENTS: TRAINING DECEPTIVE LLMS THAT PERSIST THROUGH SAFETY TRAINING
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Deceptive Behavior
        </strong>
        &amp;
        <strong>
         Safety Training
        </strong>
        &amp;
        <strong>
         Backdoored Behavior
        </strong>
        &amp;
        <strong>
         Adversarial Training
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        Jinan University,Nanyang Technological University, Beijing Institute of Technology, Pazhou Lab
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.05949">
         UNIVERSAL VULNERABILITIES IN LARGE LANGUAGE MODELS: IN-CONTEXT LEARNING BACKDOOR ATTACKS
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         In-context Learning
        </strong>
        &amp;
        <strong>
         Security
        </strong>
        &amp;
        <strong>
         Backdoor Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        Carnegie Mellon University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.05998">
         Combating Adversarial Attacks with Multi-Agent Debate
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         Multi-Agent Debate
        </strong>
        &amp;
        <strong>
         Red Team
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        Fudan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.06824">
         Open the Pandora‚Äôs Box of LLMs: Jailbreaking LLMs through Representation Engineering
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Security
        </strong>
        &amp;
        <strong>
         Representation Engineering
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        Northwestern University, New York University, University of Liverpool, Rutgers University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.09002">
         AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Evaluation Frameworks
        </strong>
        &amp;
        <strong>
         Ground Truth Dataset
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        Kyushu Institute of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.09798">
         ALL IN HOW YOU ASK FOR IT: SIMPLE BLACK-BOX METHOD FOR JAILBREAK ATTACKS
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Black-box Method
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        MIT
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.10862">
         Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Model Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        Aalborg University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.12192">
         Text Embedding Inversion Attacks on Multilingual Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Text Embedding
        </strong>
        &amp;
        <strong>
         Inversion Attacks
        </strong>
        &amp;
        <strong>
         Multilingual Language Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        University of Illinois Urbana-Champaign, University of Washington, Western Washington University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.12242">
         BADCHAIN: BACKDOOR CHAIN-OF-THOUGHT PROMPTING FOR LARGE LANGUAGE MODELS
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Chain-of-Thought Prompting
        </strong>
        &amp;
        <strong>
         Backdoor Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        The University of Hong Kong, Zhejiang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.12915">
         Red Teaming Visual Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Red Teaming
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        University of California Santa Barbara,Sea AI Lab Singapore, Carnegie Mellon University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.17256">
         Weak-to-Strong Jailbreaking on Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Adversarial Prompts
        </strong>
        &amp;
        <strong>
         AI Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Boston University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.00626">
         Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Large Vision-Language Models
        </strong>
        &amp;
        <strong>
         Typographic Attacks
        </strong>
        &amp;
        <strong>
         Self-Generated Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Copenhagen Business School, Temple University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.00898">
         An Early Categorization of Prompt Injection Attacks on Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection
        </strong>
        &amp;
        <strong>
         Categorization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Michigan State University, Okinawa Institute of Science and Technology (OIST)
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.02160">
         Data Poisoning for In-context Learning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         In-context learning
        </strong>
        &amp;
        <strong>
         Data poisoning
        </strong>
        &amp;
        <strong>
         Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        CISPA Helmholtz Center for Information Security
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.02987">
         Conversation Reconstruction Attack Against GPT Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Conversation Reconstruction Attack
        </strong>
        &amp;
        <strong>
         Privacy risks
        </strong>
        &amp;
        <strong>
         Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        University of Illinois Urbana-Champaign, Center for AI Safety, Carnegie Mellon University, UC Berkeley, Microsoft
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.04249">
         HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Automated Red Teaming
        </strong>
        &amp;
        <strong>
         Robust Refusal
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        University of Washington, University of Virginia, Allen Institute for Artificial Intelligence
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.07841">
         Do Membership Inference Attacks Work on Large Language Models?
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Membership Inference Attacks
        </strong>
        &amp;
        <strong>
         Privacy
        </strong>
        &amp;
        <strong>
         Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Pennsylvania State University, Wuhan University, Illinois Institute of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.07867">
         PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Knowledge Poisoning Attacks
        </strong>
        &amp;
        <strong>
         Retrieval-Augmented Generation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Purdue University, University of Massachusetts at Amherst
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.05467">
         RAPID OPTIMIZATION FOR JAILBREAKING LLMS VIA SUBCONSCIOUS EXPLOITATION AND ECHOPRAXIA
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking LLM
        </strong>
        &amp;
        <strong>
         Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        CISPA Helmholtz Center for Information Security
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.05668">
         Comprehensive Assessment of Jailbreak Attacks Against LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Attack Methods
        </strong>
        &amp;
        <strong>
         Policy Alignment
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        UC Berkeley
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.06363">
         StruQ: Defending Against Prompt Injection with Structured Queries
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection Attacks
        </strong>
        &amp;
        <strong>
         Structured Queries
        </strong>
        &amp;
        <strong>
         Defense Mechanisms
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Nanyang Technological University, Huazhong University of Science and Technology, University of New South Wales
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.08416">
         PANDORA: Jailbreak GPTs by Retrieval Augmented Generation Poisoning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Retrieval Augmented Generation (RAG)
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Sea AI Lab, Southern University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.08577">
         Test-Time Backdoor Attacks on Multimodal Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Attacks
        </strong>
        &amp;
        <strong>
         Multimodal Large Language Models (MLLMs)
        </strong>
        &amp;
        <strong>
         Adversarial Test Images
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        University of Illinois at Urbana‚ÄìChampaign, University of California, San Diego, Allen Institute for AI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.08679">
         COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaks
        </strong>
        &amp;
        <strong>
         Controllable Attack Generation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        ISCAS, NTU
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.09091">
         Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Indirect Attack
        </strong>
        &amp;
        <strong>
         Puzzler
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        √âcole Polytechnique F√©d√©rale de Lausanne, University of Wisconsin-Madison
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.09177">
         Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking Attacks
        </strong>
        &amp;
        <strong>
         Contextual Interaction
        </strong>
        &amp;
        <strong>
         Multi-Round Interactions
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        University of Electronic Science and Technology of China, CISPA Helmholtz Center for Information Security, NetApp
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.09179">
         Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Customization
        </strong>
        &amp;
        <strong>
         Instruction Backdoor Attacks
        </strong>
        &amp;
        <strong>
         GPTs
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Shanghai Artificial Intelligence Laboratory
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.09283">
         Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Conversation Safety
        </strong>
        &amp;
        <strong>
         Attacks
        </strong>
        &amp;
        <strong>
         Defenses
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        UC Berkeley, New York University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.09674">
         PAL: Proxy-Guided Black-Box Attack on Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Black-Box Attack
        </strong>
        &amp;
        <strong>
         Proxy-Guided Attack
        </strong>
        &amp;
        <strong>
         PAL
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Center for Human-Compatible AI, UC Berkeley
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.10260">
         A STRONGREJECT for Empty Jailbreaks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaks
        </strong>
        &amp;
        <strong>
         Benchmarking
        </strong>
        &amp;
        <strong>
         StrongREJECT
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Arizona State University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.10601">
         Jailbreaking Proprietary Large Language Models using Word Substitution Cipher
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak
        </strong>
        &amp;
        <strong>
         Word Substitution Cipher
        </strong>
        &amp;
        <strong>
         Attack Success Rate
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Renmin University of China, Beijing, Peking University, WeChat AI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.11208">
         Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Attacks
        </strong>
        &amp;
        <strong>
         Agent Safety
        </strong>
        &amp;
        <strong>
         Framework
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        University of Washington, UIUC, Western Washington University, University of Chicago
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.11753">
         ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         ASCII Art
        </strong>
        &amp;
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Safety Alignment
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Jinan University, Nanyang Technological University, Zhejiang University, Hong Kong University of Science and Technology, Beijing Institute of Technology, Sony Research
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.12168">
         Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Weight-Poisoning Backdoor Attacks
        </strong>
        &amp;
        <strong>
         Parameter-Efficient Fine-Tuning (PEFT)
        </strong>
        &amp;
        <strong>
         Poisoned Sample Identification Module (PSIM)
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        CISPA Helmholtz Center for Information Security
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.12959">
         Prompt Stealing Attacks Against Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Engineering
        </strong>
        &amp;
        <strong>
         Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        University of New South Wales Australia, Delft University of Technology The Netherlands&amp;Nanyang Technological University Singapore
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.13457">
         LLM Jailbreak Attack versus Defense Techniques - A Comprehensive Study
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Defense Techniques
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Wayne State University, University of Michigan-Flint
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.13459">
         Learning to Poison Large Language Models During Instruction Tuning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Data Poisoning
        </strong>
        &amp;
        <strong>
         Backdoor Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Nanyang Technological University, Zhejiang University, The Chinese University of Hong Kong
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.13532">
         Backdoor Attacks on Dense Passage Retrievers for Disseminating Misinformation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Dense Passage Retrieval
        </strong>
        &amp;
        <strong>
         Backdoor Attacks
        </strong>
        &amp;
        <strong>
         Misinformation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        University of Michigan
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.15911">
         PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Universal Adversarial Prefixes
        </strong>
        &amp;
        <strong>
         Guard Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Meta
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.16822">
         Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Prompts
        </strong>
        &amp;
        <strong>
         Quality-Diversity
        </strong>
        &amp;
        <strong>
         Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Fudan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.16717">
         CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Personalized Encryption
        </strong>
        &amp;
        <strong>
         Safety Mechanisms
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Carnegie Mellon University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.16187">
         Attacking LLM Watermarks by Exploiting Their Strengths
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Watermarks
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Beihang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.16006">
         From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Translation of Text Embeddings
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Suffix
        </strong>
        &amp;
        <strong>
         Text Embedding Translation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        University of Maryland College Park
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.15570">
         Fast Adversarial Attacks on Language Models In One GPU Minute
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         BEAST
        </strong>
        &amp;
        <strong>
         Computational Efficiency
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Shanghai Artificial Intelligence Laboratory
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.09283">
         Attacks Defenses and Evaluations for LLM Conversation Safety: A Survey
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Conversation Safety
        </strong>
        &amp;
        <strong>
         Survey
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Beijing University of Posts and Telecommunications, University of Michigan
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.17262">
         Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multi-turn Dialogue
        </strong>
        &amp;
        <strong>
         Safety Vulnerability
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        University of California, The Hongkong University of Science and Technology, University of Maryland
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.16914">
         DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking Attacks
        </strong>
        &amp;
        <strong>
         Prompt Decomposition
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Massachusetts Institute of Technology, MIT-IBM Watson AI Lab
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.19464">
         CURIOSITY-DRIVEN RED-TEAMING FOR LARGE LANGUAGE MODELS
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Curiosity-Driven Exploration
        </strong>
        &amp;
        <strong>
         Red Teaming
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        SKLOIS Institute of Information Engineering Chinese Academy of Science, School of Cyber Security University of Chinese Academy of Sciences,Tsinghua University,RealAI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.18104">
         Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Large Language Models
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        Rice University, Samsung Electronics America
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.00108">
         LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Low-Rank Adaptation (LoRA)
        </strong>
        &amp;
        <strong>
         Backdoor Attacks
        </strong>
        &amp;
        <strong>
         Model Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        The University of Hong Kong
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.02910">
         ImgTrojan: Jailbreaking Vision-Language Models with ONE Image
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Data Poisoning
        </strong>
        &amp;
        <strong>
         Jailbreaking Attack
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        SPRING Lab EPFL
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.03792">
         Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection Attacks
        </strong>
        &amp;
        <strong>
         Optimization-Based Approach
        </strong>
        &amp;
        <strong>
         Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        Shanghai University of Finance and Economics, Southern University of Science and Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.08424">
         Tastle: Distract Large Language Models for Automatic Jailbreak Attack
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Black-box Framework
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        Google DeepMind, ETH Zurich, University of Washington, OpenAI, McGill University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.06634">
         Stealing Part of a Production Language Model
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Model Stealing
        </strong>
        &amp;
        <strong>
         Language Models
        </strong>
        &amp;
        <strong>
         Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        University of Edinburgh
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.09832">
         Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection Attacks
        </strong>
        &amp;
        <strong>
         Machine Translation
        </strong>
        &amp;
        <strong>
         Inverse Scaling
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        Nanyang Technological University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.13355">
         BADEDIT: BACKDOORING LARGE LANGUAGE MODELS BY MODEL EDITING
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Attacks
        </strong>
        &amp;
        <strong>
         Model Editing
        </strong>
        &amp;
        <strong>
         Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        Fudan University, Shanghai AI Laboratory
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.12171">
         EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Security
        </strong>
        &amp;
        <strong>
         Framework
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        Shanghai Key Lab of Intelligent Information Processing, Fudan University, Shanghai Engineering Research Center of AI &amp; Robotics
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.10883">
         Improving Adversarial Transferability of Visual-Language Pre-training Models through Collaborative Multimodal Interaction
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision-Language Pre-trained Model
        </strong>
        &amp;
        <strong>
         Adversarial Transferability
        </strong>
        &amp;
        <strong>
         Black-Box Attack
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        Microsoft
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.12503">
         Securing Large Language Models: Threats, Vulnerabilities, and Responsible Practices
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Security Risks
        </strong>
        &amp;
        <strong>
         Vulnerabilities
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        Carnegie Mellon University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.14725">
         Jailbreaking is Best Solved by Definition
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Adaptive Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        ShanghaiTech University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.16432">
         LinkPrompt: Natural and Universal Adversarial Attacks on Prompt-based Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Universal Adversarial Triggers
        </strong>
        &amp;
        <strong>
         Prompt-based Learning
        </strong>
        &amp;
        <strong>
         Natural Language Attack
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        Huazhong University of Science and Technology, Lehigh University, University of Notre Dame &amp; Duke University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.17710">
         Optimization-based Prompt Injection Attack to LLM-as-a-Judge
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection Attack
        </strong>
        &amp;
        <strong>
         LLM-as-a-Judge
        </strong>
        &amp;
        <strong>
         Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        Washington University in St. Louis, University of Wisconsin - Madison, John Burroughs School
       </td>
       <td style="text-align: center;">
        USENIX Security 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.19260">
         Don‚Äôt Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Prompts
        </strong>
        &amp;
        <strong>
         Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        School of Information Science and Technology, ShanghaiTech University
       </td>
       <td style="text-align: center;">
        NAACL2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.16432">
         LinkPrompt: Natural and Universal Adversarial Attacks on Prompt-based Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt-based Language Models
        </strong>
        &amp;
        <strong>
         Universal Adversarial Triggers
        </strong>
        &amp;
        <strong>
         Natural Language Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        University of Pennsylvania, ETH Zurich, EPFL, Sony AI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.01318">
         JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking Attacks
        </strong>
        &amp;
        <strong>
         Robustness Benchmark
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        Microsoft Azure, Microsoft, Microsoft Research
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.01833">
         The Crescendo Multi-Turn LLM Jailbreak Attack
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Multi-Turn Interaction
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        EPFL
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.02151">
         Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adaptive Attacks
        </strong>
        &amp;
        <strong>
         Jailbreaking
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        The Ohio State University, University of Wisconsin-Madison
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.03027">
         JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal Large Language Models
        </strong>
        &amp;
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Benchmark
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        Enkrypt AI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.04392">
         INCREASED LLM VULNERABILITIES FROM FINE-TUNING AND QUANTIZATION
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Fine-tuning
        </strong>
        &amp;
        <strong>
         Quantization
        </strong>
        &amp;
        <strong>
         LLM Vulnerabilities
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        The Pennsylvania State University, Carnegie Mellon University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.04849">
         Hidden You Malicious Goal Into Benigh Narratives: Jailbreak Large Language Models through Logic Chain Injection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM
        </strong>
        &amp;
        <strong>
         Jailbreak
        </strong>
        &amp;
        <strong>
         Prompt Injection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        Technical University of Darmstadt, Google Research
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.05530">
         Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Reinforcement Learning from Human Feedback
        </strong>
        &amp;
        <strong>
         Poisoned Preference Data
        </strong>
        &amp;
        <strong>
         Language Model Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        Purdue University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.06407">
         Rethinking How to Evaluate Language Model Jailbreak
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak
        </strong>
        &amp;
        <strong>
         Evaluation Metrics
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        Xi‚Äôan Jiaotong-Liverpool University, Rutgers University, University of Liverpool
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.07234">
         Goal-guided Generative Prompt Injection Attack on Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection
        </strong>
        &amp;
        <strong>
         Robustness
        </strong>
        &amp;
        <strong>
         Mahalanobis Distance
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        University of New Haven
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.07242">
         SANDWICH ATTACK: MULTI-LANGUAGE MIXTURE ADAPTIVE ATTACK ON LLMS
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multi-language Mixture
        </strong>
        &amp;
        <strong>
         Adaptive Attack
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        The Ohio State University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.07921">
         AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Suffix Generation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        Renmin University of China, Microsoft Research
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.12038">
         Uncovering Safety Risks in Open-source LLMs through Concept Activation Vector
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety
        </strong>
        &amp;
        <strong>
         Attack Methods
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        Zhejiang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.08793">
         JailbreakLens: Visual Analysis of Jailbreak Attacks Against Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Visual Analytics
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        Shanghaitech University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.16369">
         Don‚Äôt Say No: Jailbreaking LLM by Suppressing Refusal
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking Attacks
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        University of Electronic Science and Technology of China,  Chengdu University of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.14795">
         TALK TOO MUCH: Poisoning Large Language Models under Token Limit
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Token Limitation
        </strong>
        &amp;
        <strong>
         Poisoning Attack
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        ETH Zurich,  EPFL, University of Twente, Georgia Institute of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.14461">
         Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Aligned LLMs
        </strong>
        &amp;
        <strong>
         Universal Jailbreak Backdoors
        </strong>
        &amp;
        <strong>
         Poisoning Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        N/A
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.13660">
         Trojan Detection in Large Language Models: Insights from The Trojan Detection Challenge
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Trojan Detection
        </strong>
        &amp;
        <strong>
         Model Robustness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        Shanghai Jiao Tong University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.12916">
         Physical Backdoor Attack can Jeopardize Driving with Vision-Large-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision-Large-Language Models
        </strong>
        &amp;
        <strong>
         Autonomous Driving
        </strong>
        &amp;
        <strong>
         Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        Max-Planck-Institute for Intelligent Systems, AI at Meta (FAIR)
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.16873">
         AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Prompting
        </strong>
        &amp;
        <strong>
         Safety in AI
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        Singapore Management University, Shanghai Institute for Advanced Study of Zhejiang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.18534">
         Evaluating and Mitigating Linguistic Discrimination in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        =
        <strong>
         Linguistic Discrimination
        </strong>
        &amp;
        <strong>
         Jailbreak
        </strong>
        &amp;
        <strong>
         Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        University of Louisiana at Lafayette, Beijing Electronic Science and Technology Institute, The Johns Hopkins University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.18567">
         Assessing Cybersecurity Vulnerabilities in Code Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Code LLMs
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        University College London, The University of Melbourne, Macquarie University, University of Edinburgh
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.19597">
         Transferring Troubles: Cross-Lingual Transferability of Backdoor Attacks in LLMs with Instruction Tuning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Cross-Lingual Transferability
        </strong>
        &amp;
        <strong>
         Backdoor Attacks
        </strong>
        &amp;
        <strong>
         Instruction Tuning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        University of Cambridge, Indian Institute of Technology Bombay, University of Melbourne, University College London, Macquarie University
       </td>
       <td style="text-align: center;">
        ICLR 2024 Workshop
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.16891">
         Attacks on Third-Party APIs of Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Third-Party API
        </strong>
        &amp;
        <strong>
         Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        Purdue University, Fort Wayne
       </td>
       <td style="text-align: center;">
        NAACL2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.08538">
         Vert Attack: Taking advantage of Text Classifiers‚Äô horizontal vision
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Text Classifiers
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         VertAttack
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        The University of Melbourne&amp;Macquarie University&amp;University College London
       </td>
       <td style="text-align: center;">
        NAACL2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.02393">
         Backdoor Attacks on Multilingual Machine Translation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multilingual Machine Translation
        </strong>
        &amp;
        <strong>
         Security
        </strong>
        &amp;
        <strong>
         Backdoor Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        Institute of Information Engineering, Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.03654">
         Can LLMs Deeply Detect Complex Malicious Queries? A Framework for Jailbreaking via Obfuscating Intent
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Red Team
        </strong>
        &amp;
        <strong>
         Black-box Attack
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        University of Texas at Austin
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.05418">
         Mitigating Exaggerated Safety in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Model Safety
        </strong>
        &amp;
        <strong>
         Utility
        </strong>
        &amp;
        <strong>
         Exaggerated Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        Institute of Information Engineering, Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.05610">
         Chain of Attack: A Semantic-Driven Contextual Multi-Turn Attacker for LLM
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multi-Turn Dialogue Attack
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
        &amp;
        <strong>
         Semantic-Driven Contextual Attack
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        Peking University
       </td>
       <td style="text-align: center;">
        ICLR 2024 Workshop
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.01229">
         BOOSTING JAILBREAK ATTACK WITH MOMENTUM
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Momentum Method
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        √âcole Polytechnique F√©d√©rale de Lausanne
       </td>
       <td style="text-align: center;">
        ICML 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.04346">
         Revisiting character-level adversarial attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Character-level Adversarial Attack
        </strong>
        &amp;
        <strong>
         Robustness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        Johns Hopkins University
       </td>
       <td style="text-align: center;">
        CCS 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.06823">
         PLeak: Prompt Leaking Attacks against Large Language Model Applications
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Leaking Attacks
        </strong>
        &amp;
        <strong>
         Adversarial Queries
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        IT University of Copenhagen
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.15902">
         Hacc-Man: An Arcade Game for Jailbreaking LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         creative problem solving
        </strong>
        &amp;
        <strong>
         jailbreaking
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        The Hong Kong Polytechnic University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.16229">
         No Two Devils Alike: Unveiling Distinct Mechanisms of Fine-tuning Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Fine-tuning Attacks
        </strong>
        &amp;
        <strong>
         LLM Safeguarding
        </strong>
        &amp;
        <strong>
         Mechanistic Interpretability
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        KAIST
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.16567">
         Automatic Jailbreaking of the Text-to-Image Generative AI Systems
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Text-to-Image
        </strong>
        &amp;
        <strong>
         Generative AI
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        Fudan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.17894">
         White-box Multimodal Jailbreaks Against Large Vision-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Fine-tuning Attacks
        </strong>
        &amp;
        <strong>
         Multimodal Models
        </strong>
        &amp;
        <strong>
         Adversarial Robustness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        Singapore Management University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.18166">
         Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Layer-specific Editing
        </strong>
        &amp;
        <strong>
         LLM Safeguarding
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        Mila ‚Äì Qu√©bec AI Institute
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.18540">
         Learning Diverse Attacks on Large Language Models for Robust Red-Teaming and Safety Tuning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Red-Teaming
        </strong>
        &amp;
        <strong>
         Safety Tuning
        </strong>
        &amp;
        <strong>
         GFlowNet Fine-tuning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        CISPA Helmholtz Center for Information Security
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.19103">
         Voice Jailbreak Attacks Against GPT-4o
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Voice Mode
        </strong>
        &amp;
        <strong>
         GPT-4o
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        Nanyang Technological University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.19360">
         ART: Automatic Red-teaming for Text-to-Image Models to Protect Benign Users
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Red-Teaming
        </strong>
        &amp;
        <strong>
         Text-to-Image Models
        </strong>
        &amp;
        <strong>
         Generative AI Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        Xidian University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.20015">
         Efficient LLM-Jailbreaking by Introducing Visual Modality
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Multimodal Models
        </strong>
        &amp;
        <strong>
         Visual Modality
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        Institute of Information Engineering, Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.20234">
         Context Injection Attacks on Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Context Injection Attacks
        </strong>
        &amp;
        <strong>
         Misleading Context
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        University of Illinois at Urbana-Champaign
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.20413">
         Jailbreaking Large Language Models Against Moderation Guardrails via Cipher Characters
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak
        </strong>
        &amp;
        <strong>
         Moderation Guardrails
        </strong>
        &amp;
        <strong>
         Cipher Characters
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        Northeastern University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.20485">
         Phantom: General Trigger Attacks on Retrieval Augmented Language Generation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Trigger Attacks
        </strong>
        &amp;
        <strong>
         Retrieval Augmented Generation
        </strong>
        &amp;
        <strong>
         Poisoning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        Northwestern University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.20653">
         Enhancing Jailbreak Attack Against Large Language Models through Silent Tokens
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Silent Tokens
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        Peking University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.20773">
         Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Character
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         MultiModal Large Language Models
        </strong>
        &amp;
        <strong>
         Role-playing
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        Northwestern University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.20774">
         Exploring Backdoor Attacks against Large Language Model-based Decision Making
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Attacks
        </strong>
        &amp;
        <strong>
         Decision Making
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        Beihang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.20775">
         Cross-Modality Jailbreak and Mismatched Attacks on Medical Multimodal Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak
        </strong>
        &amp;
        <strong>
         Multimodal Large Language Models
        </strong>
        &amp;
        <strong>
         Medical Contexts
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        Harbin Institute of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.20778">
         Improved Generation of Adversarial Examples Against Safety-aligned LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Examples
        </strong>
        &amp;
        <strong>
         Safety-aligned LLMs
        </strong>
        &amp;
        <strong>
         Gradient-based Methods
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        Nanyang Technological University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.21018">
         Improved Techniques for Optimization-Based Jailbreaking on Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Optimization Techniques
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        University of Central Florida
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.00083">
         BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Retrieval-Augmented Generation
        </strong>
        &amp;
        <strong>
         Poisoning Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Zscaler, Inc.
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.00240">
         Exploring Vulnerabilities and Protections in Large Language Models: A Survey
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Hacking
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         Suvery
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Singapore Management University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.01288">
         Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their Defenses
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Few-Shot Jailbreaking
        </strong>
        &amp;
        <strong>
         Aligned Language Models
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Capgemini Invent, Paris
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.02044">
         QROA: A Black-Box Query-Response Optimization Attack on LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Query-Response Optimization Attack
        </strong>
        &amp;
        <strong>
         Black-Box
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Huazhong University of Science and Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.03805">
         AutoJailbreak: Exploring Jailbreak Attacks and Defenses through a Dependency Lens
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Dependency Analysis
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Beihang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.04031">
         Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Vision Language Models
        </strong>
        &amp;
        <strong>
         Bi-Modal Adversarial Prompt
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Zhengzhou University
       </td>
       <td style="text-align: center;">
        ACL 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.03007">
         BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Attacks
        </strong>
        &amp;
        <strong>
         LLM Agents
        </strong>
        &amp;
        <strong>
         Data Poisoning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Ludwig-Maximilians-University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.09289">
         Understanding Jailbreak Success: A Study of Latent Space Dynamics in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Success
        </strong>
        &amp;
        <strong>
         Latent Space Dynamics
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Alibaba Group
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.05644">
         How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Safety
        </strong>
        &amp;
        <strong>
         Alignment
        </strong>
        &amp;
        <strong>
         Jailbreak
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Beihang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.06302">
         Unveiling the Safety of GPT-4O: An Empirical Study Using Jailbreak Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         GPT-4O
        </strong>
        &amp;
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Safety Evaluation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Nanyang Technological University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.06852">
         A Survey of Backdoor Attacks and Defenses on Large Language Models: Implications for Security Measures
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Attacks
        </strong>
        &amp;
        <strong>
         Defenses
        </strong>
        &amp;
        <strong>
         Survey
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Anomalee Inc.
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.07778">
         On Trojans in Refined Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Trojans
        </strong>
        &amp;
        <strong>
         Refined Language Models
        </strong>
        &amp;
        <strong>
         Data Poisoning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Purdue University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.08705">
         When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-Guided Search
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Deep Reinforcement Learning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Xidian University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.08754">
         StructuralSleight: Automated Jailbreak Attacks on Large Language Models Utilizing Uncommon Text-Encoded Structure
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         StructuralSleight
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Tsinghua University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.09321">
         JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts Against Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attempts
        </strong>
        &amp;
        <strong>
         Evaluation Toolkit
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        The Hong Kong University of Science and Technology (Guangzhou)
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.09324">
         Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Benchmarking
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Pennsylvania State University
       </td>
       <td style="text-align: center;">
        NAACL 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.04478">
         PromptFix: Few-shot Backdoor Removal via Adversarial Prompt Tuning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Removal
        </strong>
        &amp;
        <strong>
         Adversarial Prompt Tuning
        </strong>
        &amp;
        <strong>
         Few-shot Learning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Shanghai Jiao Tong University, Peking University, Shanghai AI Laboratory
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.10630">
         Emerging Safety Attack and Defense in Federated Instruction Tuning of Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Federated Instruction Tuning
        </strong>
        &amp;
        <strong>
         Safety Attack
        </strong>
        &amp;
        <strong>
         Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Michigan State University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.10794">
         Towards Understanding Jailbreak Attacks in LLMs: A Representation Space Analysis
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Representation Space Analysis
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.11668">
         ‚ÄúNot Aligned‚Äù is Not ‚ÄúMalicious‚Äù: Being Careful about Hallucinations of Large Language Models‚Äô Jailbreak
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak
        </strong>
        &amp;
        <strong>
         Hallucinations
        </strong>
        &amp;
        <strong>
         LLMs
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Tsinghua University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.11682">
         Knowledge-to-Jailbreak: One Knowledge Point Worth One Attack
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Knowledge-to-Jailbreak
        </strong>
        &amp;
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Domain-Specific Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        University of Maryland
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.12091">
         Is Poisoning a Real Threat to LLM Alignment? Maybe More So Than You Think
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Poisoning Attacks
        </strong>
        &amp;
        <strong>
         Direct Policy Optimization
        </strong>
        &amp;
        <strong>
         Reinforcement Learning with Human Feedback
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Carnegie Mellon University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.12702">
         Jailbreak Paradox: The Achilles‚Äô Heel of LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Paradox
        </strong>
        &amp;
        <strong>
         Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Carnegie Mellon University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.12814">
         Adversarial Attacks on Multimodal Agents
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         Multimodal Agents
        </strong>
        &amp;
        <strong>
         Vision-Language Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        University of Washington, Allen Institute for AI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.12935">
         ChatBug: A Common Vulnerability of Aligned LLMs Induced by Chat Templates
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Vulnerabilities
        </strong>
        &amp;
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Adversarial Training
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        University of Notre Dame, Huazhong University of Science and Technology, Tsinghua University, Lehigh University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.13662">
         ObscurePrompt: Jailbreaking Large Language Models via Obscure Input
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         Out-of-Distribution Data
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        The University of Hong Kong, Huawei Noah‚Äôs Ark Lab
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.14393">
         Jailbreaking as a Reward Misspecification Problem
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Reward Misspecification
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        UC Berkeley
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.14595">
         Adversaries Can Misuse Combinations of Safe Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Model Misuse
        </strong>
        &amp;
        <strong>
         AI Safety
        </strong>
        &amp;
        <strong>
         Task Decomposition
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        UC Santa Barbara
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.14711">
         MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         MultiAgent Collaboration
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        University of Southern California
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.14859">
         From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal Jailbreaking
        </strong>
        &amp;
        <strong>
         MLLMs
        </strong>
        &amp;
        <strong>
         Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        KAIST
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.15481">
         CSRT: Evaluation and Analysis of LLMs using Code-Switching Red-Teaming Dataset
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Code-Switching
        </strong>
        &amp;
        <strong>
         Red-Teaming
        </strong>
        &amp;
        <strong>
         Multilingualism
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        China University of Geosciences
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.16963">
         Large Language Models for Link Stealing Attacks Against Graph Neural Networks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Link Stealing Attacks
        </strong>
        &amp;
        <strong>
         Graph Neural Networks
        </strong>
        &amp;
        <strong>
         Privacy Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        The Hong Kong Polytechnic University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.17626">
         CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Evaluation
        </strong>
        &amp;
        <strong>
         Dialogue Coreference
        </strong>
        &amp;
        <strong>
         LLM Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Imperial College London
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.17975">
         Inherent Challenges of Post-Hoc Membership Inference for Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Membership Inference Attacks
        </strong>
        &amp;
        <strong>
         Post-Hoc Evaluation
        </strong>
        &amp;
        <strong>
         Distribution Shift
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Hubei University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.18122">
         Poisoned LangChain: Jailbreak LLMs by LangChain
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak
        </strong>
        &amp;
        <strong>
         Retrieval-Augmented Generation
        </strong>
        &amp;
        <strong>
         LangChain
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        University of Central Florida
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.18725">
         Jailbreaking LLMs with Arabic Transliteration and Arabizi
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Arabic Transliteration
        </strong>
        &amp;
        <strong>
         Arabizi
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Hubei University
       </td>
       <td style="text-align: center;">
        TRAC 2024 Workshop
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.19234">
         SEEING IS BELIEVING: BLACK-BOX MEMBERSHIP INFERENCE ATTACKS AGAINST RETRIEVAL AUGMENTED GENERATION
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Membership Inference Attacks
        </strong>
        &amp;
        <strong>
         Retrieval-Augmented Generation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Huazhong University of Science and Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.19845">
         Virtual Context: Enhancing Jailbreak Attacks with Special Token Injection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Special Tokens
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        UC Berkeley
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.20053">
         Covert Malicious Finetuning: Challenges in Safeguarding LLM Adaptation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         AI Safety
        </strong>
        &amp;
        <strong>
         Backdoors
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        University of Illinois Chicago
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.00869">
         Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Fallacious Reasoning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Palisade Research
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.01376">
         Badllama 3: Removing Safety Finetuning from Llama 3 in Minutes
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Finetuning
        </strong>
        &amp;
        <strong>
         Jailbreak Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        University of Illinois Urbana-Champaign
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.01599">
         JailbreakZoo: Survey, Landscapes, and Horizons in Jailbreaking Large Language and Vision-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Vision-Language Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Shanghai University of Finance and Economics
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.01902">
         SoP: Unlock the Power of Social Facilitation for Automatic Jailbreak Attack
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Large Language Models
        </strong>
        &amp;
        <strong>
         Social Facilitation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        University of Exeter
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.02534">
         Image-to-Text Logic Jailbreak: Your Imagination can Help You Do Anything
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Machine Learning
        </strong>
        &amp;
        <strong>
         ICML
        </strong>
        &amp;
        <strong>
         Jailbreak Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Hong Kong University of Science and Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.03045">
         JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM Conversational Datasets
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Visual Analytics
        </strong>
        &amp;
        <strong>
         Jailbreak Prompts
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        CISPA Helmholtz Center for Information Security
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.03160">
         SOS! Soft Prompt Attack Against Open-Source Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Soft Prompt Attack
        </strong>
        &amp;
        <strong>
         Open-Source Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        National University of Singapore
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.03232">
         Single Character Perturbations Break LLM Alignment
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Model Alignment
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Deutsches Forschungszentrum f√ºr K√ºnstliche Intelligenz
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.03391">
         Soft Begging: Modular and Efficient Shielding of LLMs against Prompt Injection and Jailbreaking based on Prompt Tuning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection
        </strong>
        &amp;
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Soft Prompts
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        UC Davis
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.04151">
         Securing Multi-turn Conversational Language Models Against Distributed Backdoor Triggers
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multi-turn Conversation
        </strong>
        &amp;
        <strong>
         Backdoor Triggers
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Tsinghua University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.04295">
         Jailbreak Attacks and Defenses Against Large Language Models: A Survey
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Defenses
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Zhejiang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.09164">
         TAPI: Towards Target-Specific and Adversarial Prompt Injection against Code LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Target-Specific Attacks
        </strong>
        &amp;
        <strong>
         Adversarial Prompt Injection
        </strong>
        &amp;
        <strong>
         Malicious Code Generation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Northwestern University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.09292">
         CEIPA: Counterfactual Explainable Incremental Prompt Attack Analysis on Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Counterfactual Explanation
        </strong>
        &amp;
        <strong>
         Prompt Attack Analysis
        </strong>
        &amp;
        <strong>
         Incremental Prompt Injection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        EPFL
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.11969">
         Does Refusal Training in LLMs Generalize to the Past Tense?
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Refusal Training
        </strong>
        &amp;
        <strong>
         Past Tense Reformulation
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        University of Chicago
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.12784">
         AGENTPOISON: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Red-teaming
        </strong>
        &amp;
        <strong>
         LLM Agents
        </strong>
        &amp;
        <strong>
         Poisoning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Wuhan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.13757">
         Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Black-box Attacks
        </strong>
        &amp;
        <strong>
         RAG
        </strong>
        &amp;
        <strong>
         Opinion Manipulation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        University of New South Wales
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.13796">
         Continuous Embedding Attacks via Clipped Inputs in Jailbreaking Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Continuous Embedding
        </strong>
        &amp;
        <strong>
         Jailbreaking
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Bloomberg
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.14937">
         Operationalizing a Threat Model for Red-Teaming Large Language Models (LLMs)
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Threat Model
        </strong>
        &amp;
        <strong>
         Red-Teaming
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Stanford University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.15211">
         When Do Universal Image Jailbreaks Transfer Between Vision-Language Models?
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Universal Image Jailbreaks
        </strong>
        &amp;
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Transferability
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Michigan State University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.15286">
         Intrinsic Self-correction for Enhanced Morality: An Analysis of Internal Mechanisms and the Superficial Hypothesis
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Moral Self-Correction
        </strong>
        &amp;
        <strong>
         Intrinsic Mechanisms
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Meetyou AI Lab
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.15399">
         Imposter.AI: Adversarial Attacks with Hidden Intentions towards Aligned Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         Hidden Intentions
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Zhejiang Gongshang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.16205">
         Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Analyzing-based Jailbreak
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Zhejiang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.16667">
         RedAgent: Red Teaming Large Language Models with Context-aware Autonomous Language Agent
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Red Teaming
        </strong>
        &amp;
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Context-aware Prompts
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Confirm Labs
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.17447">
         Fluent Student-Teacher Redteaming
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Fluent Student-Teacher Redteaming
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        City University of Hong Kong
       </td>
       <td style="text-align: center;">
        ACM MM 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.15050">
         Arondight: Red Teaming Large Vision Language Models with Auto-generated Multi-modal Jailbreak Prompts
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Large Vision Language Model
        </strong>
        &amp;
        <strong>
         Red Teaming
        </strong>
        &amp;
        <strong>
         Jailbreak Attack
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Huazhong University of Science and Technology
       </td>
       <td style="text-align: center;">
        NAACL 2024 Workshop
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.16686">
         Can Large Language Models Automatically Jailbreak GPT-4V?
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak
        </strong>
        &amp;
        <strong>
         Multimodal Information
        </strong>
        &amp;
        <strong>
         Facial Recognition
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Illinois Institute of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.20224">
         Can Editing LLMs Inject Harm?
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Knowledge Editing
        </strong>
        &amp;
        <strong>
         Misinformation Injection
        </strong>
        &amp;
        <strong>
         Bias Injection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        KAIST
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.20657">
         Prompt-Driven Contrastive Learning for Transferable Adversarial Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Attack
        </strong>
        &amp;
        <strong>
         Vision-Language Model
        </strong>
        &amp;
        <strong>
         Contrastive Learning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        CISPA Helmholtz Center for Information Security
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.20859">
         Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Agents
        </strong>
        &amp;
        <strong>
         Security Vulnerability
        </strong>
        &amp;
        <strong>
         Autonomous Systems
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        CISPA Helmholtz Center for Information Security
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.00129">
         Vera Verto: Multimodal Hijacking Attack
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal Hijacking Attack
        </strong>
        &amp;
        <strong>
         Model Hijacking
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Shandong University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.00523">
         Jailbreaking Text-to-Image Models with LLM-Based Agents
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Vision-Language Models (VLMs)
        </strong>
        &amp;
        <strong>
         Generative AI Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Technological University Dublin
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.00722">
         Pathway to Secure and Trustworthy 6G for LLMs: Attacks, Defense, and Opportunities
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         6G Networks
        </strong>
        &amp;
        <strong>
         Security
        </strong>
        &amp;
        <strong>
         Membership Inference Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Microsoft
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.00925">
         WHITE PAPER: A Brief Exploration of Data Exfiltration using GCG Suffixes
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Cross-Prompt Injection Attack
        </strong>
        &amp;
        <strong>
         Greedy Coordinate Gradient
        </strong>
        &amp;
        <strong>
         Data Exfiltration
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        NYU &amp; Meta AI, FAIR
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.01420">
         Mission Impossible: A Statistical Perspective on Jailbreaking LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Reinforcement Learning with Human Feedback
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Beihang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.02882">
         Compromising Embodied Agents with Contextual Backdoor Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Embodied Agents
        </strong>
        &amp;
        <strong>
         Contextual Backdoor Attacks
        </strong>
        &amp;
        <strong>
         Adversarial In-Context Generation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        FAR AI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.02946">
         Scaling Laws for Data Poisoning in LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Data Poisoning
        </strong>
        &amp;
        <strong>
         Scaling Laws
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        The University of Western Australia
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.03515">
         A Study on Prompt Injection Attack Against LLM-Integrated Mobile Robotic Systems
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Mobile Robot
        </strong>
        &amp;
        <strong>
         Prompt Injection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Fudan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.03603">
         EnJa: Ensemble Jailbreak on Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Bocconi University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.04522">
         Compromesso! Italian Many-Shot Jailbreaks Undermine the Safety of Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Multilingual Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Xidian University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.04686">
         Multi-Turn Context Jailbreak Attack on Large Language Models From First Principles
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multi-Turn Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Contextual Fusion Attack
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Cornell Tech
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.05061">
         A Jailbroken GenAI Model Can Cause Substantial Harm: GenAI-powered Applications are Vulnerable to PromptWares
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbroken GenAI Models
        </strong>
        &amp;
        <strong>
         PromptWares
        </strong>
        &amp;
        <strong>
         GenAI-powered Applications
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        University of California Irvine
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.05855">
         Using Retriever Augmented Large Language Models for Attack Graph Generation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Retriever Augmented Generation
        </strong>
        &amp;
        <strong>
         Attack Graphs
        </strong>
        &amp;
        <strong>
         Cybersecurity
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        University of California, Los Angeles
       </td>
       <td style="text-align: center;">
        CCS 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.07362">
         BadMerging: Backdoor Attacks Against Model Merging
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Attack
        </strong>
        &amp;
        <strong>
         Model Merging
        </strong>
        &amp;
        <strong>
         AI Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Stanford University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.08899">
         Kov: Transferable and Naturalistic Black-Box LLM Attacks using Markov Decision Processes and Tree Search
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Black-Box Attacks
        </strong>
        &amp;
        <strong>
         Markov Decision Processes
        </strong>
        &amp;
        <strong>
         Monte Carlo Tree Search
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Shanghai Jiao Tong University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.09878">
         Transferring Backdoors between Large Language Models by Knowledge Distillation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Attacks
        </strong>
        &amp;
        <strong>
         Knowledge Distillation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Tsinghua University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.10668">
         Probing the Safety Response Boundary of Large Language Models via Unsafe Decoding Path Generation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Response Boundary
        </strong>
        &amp;
        <strong>
         Unsafe Decoding Path
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Singapore University of Technology and Design
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.10701">
         FERRET: Faster and Effective Automated Red Teaming with Reward-Based Scoring Technique
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Automated Red Teaming
        </strong>
        &amp;
        <strong>
         Adversarial Prompts
        </strong>
        &amp;
        <strong>
         Reward-Based Scoring
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Shanghai Jiao Tong University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.10722">
         MEGen: Generative Backdoor in Large Language Models via Model Editing
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Attacks
        </strong>
        &amp;
        <strong>
         Model Editing
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.11071">
         DiffZOO: A Purely Query-Based Black-Box Attack for Red-Teaming Text-to-Image Generative Model via Zeroth Order Optimization
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Black-Box Attack
        </strong>
        &amp;
        <strong>
         Text-to-Image Generative Model
        </strong>
        &amp;
        <strong>
         Zeroth Order Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        The Pennsylvania State University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.11182">
         Hide Your Malicious Goal Into Benign Narratives: Jailbreak Large Language Models through Neural Carrier Articles
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Prompt Injection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Xi'an Jiaotong University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.11313">
         Unlocking Adversarial Suffix Optimization Without Affirmative Phrases: Efficient Black-box Jailbreaking via LLM as Optimizer
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Adversarial Suffixes
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Nanjing University of Information Science and Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.11587">
         Large Language Models are Good Attackers: Efficient and Stealthy Textual Backdoor Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Textual Backdoor Attacks
        </strong>
        &amp;
        <strong>
         Sample Selection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Nankai University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.13896">
         RT-Attack: Jailbreaking Text-to-Image Models via Random Token
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak
        </strong>
        &amp;
        <strong>
         Text-to-Image
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Harbin Institute of Technology, Shenzhen
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.13985">
         TF-Attack: Transferable and Fast Adversarial Attacks on Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Attack
        </strong>
        &amp;
        <strong>
         Transferability
        </strong>
        &amp;
        <strong>
         Efficiency
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Shenzhen Research Institute of Big Data
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.14853">
         Detecting AI Flaws: Target-Driven Attacks on Internal Faults in Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Target-Driven Attacks
        </strong>
        &amp;
        <strong>
         Internal Faults
        </strong>
        &amp;
        <strong>
         Reinforcement Learning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        National University of Singapore
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.14866">
         Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Suffixes
        </strong>
        &amp;
        <strong>
         Transfer Learning
        </strong>
        &amp;
        <strong>
         Jailbreak
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Scale AI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.15221">
         LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multi-Turn Jailbreaks
        </strong>
        &amp;
        <strong>
         LLM Defense
        </strong>
        &amp;
        <strong>
         Human Red Teaming
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        University of California, Berkeley
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.00137">
         Emerging Vulnerabilities in Frontier Models: Multi-Turn Jailbreak Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multi-Turn Jailbreak
        </strong>
        &amp;
        <strong>
         Frontier Models
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        University of Southern California
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.00399">
         Rethinking Backdoor Detection Evaluation for Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Attacks
        </strong>
        &amp;
        <strong>
         Detection Robustness
        </strong>
        &amp;
        <strong>
         Training Intensity
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Michigan State University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.00787">
         The Dark Side of Human Feedback: Poisoning Large Language Models via User Inputs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         User-Guided Poisoning
        </strong>
        &amp;
        <strong>
         RLHF
        </strong>
        &amp;
        <strong>
         Toxicity Manipulation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        University of Cambridge
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.01247">
         Conversational Complexity for Assessing Risk in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Conversational Complexity
        </strong>
        &amp;
        <strong>
         Risk Assessment
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Independent
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.03131">
         Well, that escalated quickly: The Single-Turn Crescendo Attack (STCA)
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Single-Turn Crescendo Attack
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        CISPA Helmholtz Center for Information Security
       </td>
       <td style="text-align: center;">
        CCS 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.01380">
         Membership Inference Attacks Against In-Context Learning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Membership Inference Attacks
        </strong>
        &amp;
        <strong>
         In-Context Learning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Radboud University, Ikerlan Research Centre
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.04142">
         Context is the Key: Backdoor Attacks for In-Context Learning with Vision Transformers
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Attacks
        </strong>
        &amp;
        <strong>
         In-Context Learning
        </strong>
        &amp;
        <strong>
         Vision Transformers
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Institute of Information Engineering, Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.07503">
         AdaPPA: Adaptive Position Pre-Fill Jailbreak Attack Approach Targeting LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Adaptive Position Pre-Fill
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Technion - Israel Institute of Technology, Intuit, Cornell Tech
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.08045">
         Unleashing Worms and Extracting Data: Escalating the Outcome of Attacks against RAG-based Inference in Scale and Severity Using Jailbreaking
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         RAG Inference
        </strong>
        &amp;
        <strong>
         Data Extraction
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        University of Texas at San Antonio
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.11445">
         Jailbreaking Large Language Models with Symbolic Mathematics
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Symbolic Mathematics
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Beihang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.14177">
         PathSeeker: Exploring LLM Security Vulnerabilities with a Reinforcement Learning-Based Jailbreak Approach
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Security Vulnerabilities
        </strong>
        &amp;
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Reinforcement Learning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        AWS AI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.14513">
         Order of Magnitude Speedups for LLM Membership Inference
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Membership Inference
        </strong>
        &amp;
        <strong>
         Quantile Regression
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Nanyang Technological University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.14866">
         Effective and Evasive Fuzz Testing-Driven Jailbreaking Attacks against LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking Attacks
        </strong>
        &amp;
        <strong>
         Fuzz Testing
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Hippocratic AI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.17458">
         RED QUEEN: Safeguarding Large Language Models against Concealed Multi-Turn Jailbreaking
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Multi-Turn Attacks
        </strong>
        &amp;
        <strong>
         Concealed Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Nanyang Technological University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.17946">
         Weak-to-Strong Backdoor Attacks for LLMs with Contrastive Knowledge Distillation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Attacks
        </strong>
        &amp;
        <strong>
         Contrastive Knowledge Distillation
        </strong>
        &amp;
        <strong>
         Parameter-Efficient Fine-Tuning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Georgia Institute of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.18169">
         Harmful Fine-tuning Attacks and Defenses for Large Language Models: A Survey
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Harmful Fine-tuning
        </strong>
        &amp;
        <strong>
         LLM Attacks
        </strong>
        &amp;
        <strong>
         LLM Defenses
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Institut Polytechnique de Paris
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.18708">
         Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with ASCII Art to Mask Profanity
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         ASCII Art
        </strong>
        &amp;
        <strong>
         LLM Attacks
        </strong>
        &amp;
        <strong>
         Toxicity Detection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        LMU Munich
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.19149">
         Multimodal Pragmatic Jailbreak on Text-to-image Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal Pragmatic Jailbreak
        </strong>
        &amp;
        <strong>
         Text-to-image Models
        </strong>
        &amp;
        <strong>
         Safety Filters
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Stony Brook University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.02220">
         BUCKLE UP: ROBUSTIFYING LLMS AT EVERY CUSTOMIZATION STAGE VIA DATA CURATION
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         LLM Customization
        </strong>
        &amp;
        <strong>
         Data Curation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        National University of Singapore
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.02832">
         FLIPATTACK: Jailbreak LLMs via Flipping
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        University of Wisconsin‚ÄìMadison, NVIDIA
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.05295">
         AUTODAN-TURBO: A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak
        </strong>
        &amp;
        <strong>
         Strategy Self-Exploration
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        University College London, Stanford University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.07283">
         Prompt Infection: LLM-to-LLM Prompt Injection within Multi-Agent Systems
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Infection
        </strong>
        &amp;
        <strong>
         Multi-Agent Systems
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        University of Wisconsin‚ÄìMadison
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.08660">
         RePD: Defending Jailbreak Attack through a Retrieval-based Prompt Decomposition Process
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak attack
        </strong>
        &amp;
        <strong>
         Prompt decomposition
        </strong>
        &amp;
        <strong>
         LLMs defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        UC Santa Cruz, Johns Hopkins University, University of Edinburgh, Peking University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.09040">
         AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention Manipulation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         LLMs vulnerability
        </strong>
        &amp;
        <strong>
         Optimization-based attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Independent Researcher
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.09097">
         Recent advancements in LLM Red-Teaming: Techniques, Defenses, and Ethical Considerations
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM red-teaming
        </strong>
        &amp;
        <strong>
         Jailbreaking defenses
        </strong>
        &amp;
        <strong>
         Prompt engineering
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Beihang University, Tsinghua University, Peking University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.09804">
         BLACKDAN: A Black-Box Multi-Objective Approach for Effective and Contextual Jailbreaking of Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak
        </strong>
        &amp;
        <strong>
         Multi-objective optimization
        </strong>
        &amp;
        <strong>
         Black-box attack
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Shanghai Jiao Tong University, Shanghai Artificial Intelligence Laboratory, Beihang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.10700">
         Derail Yourself: Multi-Turn LLM Jailbreak Attack Through Self-Discovered Clues
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multi-turn attacks
        </strong>
        &amp;
        <strong>
         Jailbreak
        </strong>
        &amp;
        <strong>
         Self-discovered clues
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Tsinghua University, Sea AI Lab, Peng Cheng Laboratory
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.10760">
         Denial-of-Service Poisoning Attacks on Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Denial-of-Service
        </strong>
        &amp;
        <strong>
         Poisoning attack
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        University of New Haven, Robust Intelligence
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.11272">
         COGNITIVE OVERLOAD ATTACK: PROMPT INJECTION FOR LONG CONTEXT
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Cognitive overload
        </strong>
        &amp;
        <strong>
         Prompt injection
        </strong>
        &amp;
        <strong>
         Jailbreak
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Harbin Institute of Technology, Tencent, University of Glasgow, Independent Researcher
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.11317">
         DECIPHERING THE CHAOS: ENHANCING JAILBREAK ATTACKS VIA ADVERSARIAL PROMPT TRANSLATION
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak attacks
        </strong>
        &amp;
        <strong>
         Adversarial prompt
        </strong>
        &amp;
        <strong>
         Gradient-based optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Monash University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.11459">
         JIGSAW PUZZLES: Splitting Harmful Questions to Jailbreak Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak
        </strong>
        &amp;
        <strong>
         Multi-turn attack
        </strong>
        &amp;
        <strong>
         Query splitting
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Wuhan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.11533">
         Multi-Round Jailbreak Attack on Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak
        </strong>
        &amp;
        <strong>
         Multi-round attack
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        The Hong Kong University of Science and Technology (Guangzhou), University of Birmingham, Baidu Inc.
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.12855">
         JAILJUDGE: A Comprehensive Jailbreak Judge Benchmark with Multi-Agent Enhanced Explanation Evaluation Framework
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak judge
        </strong>
        &amp;
        <strong>
         Multi-agent framework
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Theori Inc.
       </td>
       <td style="text-align: center;">
        ICLR 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.13334">
         DO LLMS HAVE POLITICAL CORRECTNESS? Analyzing Ethical Biases and Jailbreak Vulnerabilities in AI Systems
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Political correctness
        </strong>
        &amp;
        <strong>
         Jailbreak
        </strong>
        &amp;
        <strong>
         Ethical vulnerabilities
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        University of Manitoba
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.13901">
         SoK: Prompt Hacking of Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Hacking
        </strong>
        &amp;
        <strong>
         Jailbreak Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Thales DIS
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.14479">
         Backdoored Retrievers for Prompt Injection Attacks on Retrieval-Augmented Generation of Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Retrieval-Augmented Generation
        </strong>
        &amp;
        <strong>
         Prompt Injection
        </strong>
        &amp;
        <strong>
         Backdoor Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Duke University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.14827">
         Making LLMs Vulnerable to Prompt Injection via Poisoning Alignment
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection
        </strong>
        &amp;
        <strong>
         Poisoning Alignment
        </strong>
        &amp;
        <strong>
         LLM Vulnerabilities
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Tsinghua University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.15362">
         Faster-GCG: Efficient Discrete Optimization Jailbreak Attacks against Aligned Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Discrete Optimization
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        International Digital Economy Academy
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.15641">
         SMILES-Prompting: A Novel Approach to LLM Jailbreak Attacks in Chemical Synthesis
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         SMILES-Prompting
        </strong>
        &amp;
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Chemical Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Beijing University of Posts and Telecommunications
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.16327">
         FEINT AND ATTACK: Attention-Based Strategies for Jailbreaking and Protecting LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Attention Mechanisms
        </strong>
        &amp;
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Defense Strategies
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        IBM Research AI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.16950">
         Breaking ReAct Agents: Foot-in-the-Door Attack Will Get You In
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         ReAct Agents
        </strong>
        &amp;
        <strong>
         Prompt Injection
        </strong>
        &amp;
        <strong>
         Foot-in-the-Door Attack
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Google DeepMind
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.17175">
         Remote Timing Attacks on Efficient Language Model Inference
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Timing Attacks
        </strong>
        &amp;
        <strong>
         Efficient Inference
        </strong>
        &amp;
        <strong>
         Privacy
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Meta
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.18210">
         Towards Understanding the Fragility of Multilingual LLMs against Fine-Tuning Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Fine-Tuning Attacks
        </strong>
        &amp;
        <strong>
         Multilingual LLMs
        </strong>
        &amp;
        <strong>
         Safety Alignment
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        University of California San Diego
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.18469">
         Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         LLM Vulnerabilities
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Florida State University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.19160">
         Adversarial Attacks on Large Language Models Using Regularized Relaxation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         Continuous Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        The Pennsylvania State University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.19230">
         Humanizing the Machine: Proxy Attacks to Mislead LLM Detectors
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         Detection Evasion
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Nanyang Technological University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.20142">
         Mask-based Membership Inference Attacks for Retrieval-Augmented Generation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Retrieval-Augmented Generation
        </strong>
        &amp;
        <strong>
         Membership Inference Attacks
        </strong>
        &amp;
        <strong>
         Privacy
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        George Mason University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.20911">
         Hacking Back the AI-Hacker: Prompt Injection as a Defense Against LLM-driven Cyberattacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection Defense
        </strong>
        &amp;
        <strong>
         LLM Cybersecurity
        </strong>
        &amp;
        <strong>
         Adversarial Inputs
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Fudan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.20971">
         BlueSuffix: Reinforced Blue Teaming for Vision-Language Models Against Jailbreak Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Defense
        </strong>
        &amp;
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Reinforcement Learning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Harbin Institute of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.21083">
         Stealthy Jailbreak Attacks on Large Language Models via Benign Data Mirroring
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Adversarial Prompts
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        SRM Institute of Science and Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.21146">
         Palisade - Prompt Injection Detection Framework
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection
        </strong>
        &amp;
        <strong>
         Heuristic-based Detection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        The Ohio State University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.22143">
         AmpleGCG-Plus: A Strong Generative Model of Adversarial Suffixes to Jailbreak LLMs with Higher Success Rates in Fewer Attempts
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Adversarial Suffixes
        </strong>
        &amp;
        <strong>
         Generative Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Zhejiang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.22832">
         HIJACKRAG: Hijacking Attacks against Retrieval-Augmented Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Retrieval-Augmented Generation
        </strong>
        &amp;
        <strong>
         Prompt Injection Attacks
        </strong>
        &amp;
        <strong>
         Security Vulnerability
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        The Baldwin School
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.23308">
         Systematically Analyzing Prompt Injection Vulnerabilities in Diverse LLM Architectures
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection Vulnerabilities
        </strong>
        &amp;
        <strong>
         Model Susceptibility
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Competition for LLM and Agent Safety 2024
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.23558">
         Transferable Ensemble Black-box Jailbreak Attacks on Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Black-box Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Ensemble Methods
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        University of Electronic Science and Technology of China
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.23678">
         Pseudo-Conversation Injection for LLM Goal Hijacking
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Goal Hijacking
        </strong>
        &amp;
        <strong>
         Prompt Injection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Monash University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.23861">
         Audio Is the Achilles‚Äô Heel: Red Teaming Audio Large Multimodal Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Audio Multimodal Models
        </strong>
        &amp;
        <strong>
         Safety Vulnerabilities
        </strong>
        &amp;
        <strong>
         Jailbreak Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        Fudan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.00827">
         IDEATOR: Jailbreaking VLMs Using VLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Multimodal Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        International Computer Science Institute
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.01077">
         Emoji Attack: A Method for Misleading Judge LLMs in Safety Risk Detection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Emoji Attack
        </strong>
        &amp;
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Judge LLMs Bias
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        Peking University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.01222">
         B4: A Black-Box ScruBBing Attack on LLM Watermarks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Black-Box Attack
        </strong>
        &amp;
        <strong>
         Watermark Removal
        </strong>
        &amp;
        <strong>
         Adversarial Text Generation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        University of Science and Technology of China
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.01565">
         SQL Injection Jailbreak: a structural disaster of large language models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         SQL Injection
        </strong>
        &amp;
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         LLM Vulnerability
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        University of Illinois Urbana-Champaign
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.02785">
         Stochastic Monkeys at Play: Random Augmentations Cheaply Break LLM Safety Alignment
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Random Augmentations
        </strong>
        &amp;
        <strong>
         Safety Alignment
        </strong>
        &amp;
        <strong>
         LLM Jailbreak
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        Cambridge ERA: AI Fellowship
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.03343">
         What Features in Prompts Jailbreak LLMs? Investigating the Mechanisms Behind Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Prompts
        </strong>
        &amp;
        <strong>
         Nonlinear Probes
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        Alibaba Group
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.03814">
         MRJ-Agent: An Effective Jailbreak Agent for Multi-Round Dialogue
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multi-Round Dialogue
        </strong>
        &amp;
        <strong>
         Jailbreak Agent
        </strong>
        &amp;
        <strong>
         LLM Vulnerability
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        Columbia University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.04223">
         Diversity Helps Jailbreak Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Techniques
        </strong>
        &amp;
        <strong>
         LLM Safety
        </strong>
        &amp;
        <strong>
         Prompt Diversity
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        Bangladesh University of Engineering and Technology
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.06426">
         SequentialBreak: Large Language Models Can Be Fooled by Embedding Jailbreak Prompts into Sequential Prompt Chains
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Prompt Engineering
        </strong>
        &amp;
        <strong>
         LLM Vulnerabilities
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        Xi'an Jiaotong-Liverpool University
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.07268">
         Target-driven Attack for Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Black-box Attacks
        </strong>
        &amp;
        <strong>
         Optimization Methods
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        Georgia Institute of Technology
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.08862">
         LLM STINGER: Jailbreaking LLMs using RL Fine-tuned LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking Attacks
        </strong>
        &amp;
        <strong>
         Reinforcement Learning
        </strong>
        &amp;
        <strong>
         Adversarial Suffixes
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        Beijing University of Posts and Telecommunications
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.09259">
         Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Multimodal Generative Models
        </strong>
        &amp;
        <strong>
         Security Challenges
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        Arizona State University
       </td>
       <td style="text-align: center;">
        NeurIPS 2024 SafeGenAI Workshop
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.07559">
         Zer0-Jack: A Memory-efficient Gradient-based Jailbreaking Method for Black-box Multi-modal Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Black-box Jailbreaking
        </strong>
        &amp;
        <strong>
         Multi-modal Models
        </strong>
        &amp;
        <strong>
         Zeroth-order Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        Zhejiang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.11114">
         JailbreakLens: Interpreting Jailbreak Mechanism in the Lens of Representation and Circuit
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Large Language Models
        </strong>
        &amp;
        <strong>
         Mechanism Interpretability
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        University of Electronic Science and Technology of China
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.11496">
         Safe + Safe = Unsafe? Exploring How Safe Images Can Be Exploited to Jailbreak Large Vision-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Large Vision-Language Models
        </strong>
        &amp;
        <strong>
         Safety Snowball Effect
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        Tsinghua University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.12762">
         Playing Language Game with LLMs Leads to Jailbreaking
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Language Games
        </strong>
        &amp;
        <strong>
         LLM Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        University of Texas at Dallas
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.13757">
         AttentionBreaker: Adaptive Evolutionary Optimization for Unmasking Vulnerabilities in LLMs through Bit-Flip Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Bit-Flip Attacks
        </strong>
        &amp;
        <strong>
         Model Vulnerability Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        BITS Pilani
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.14133">
         GASP: Efficient Black-Box Generation of Adversarial Suffixes for Jailbreaking LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Latent Bayesian Optimization
        </strong>
        &amp;
        <strong>
         Adversarial Prompts
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        Nanyang Technological University, Wuhan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.18280">
         Neutralizing Backdoors through Information Conflicts for Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Defense
        </strong>
        &amp;
        <strong>
         Information Conflicts
        </strong>
        &amp;
        <strong>
         Model Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        Duke University, University of Louisville
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.16746">
         LoBAM: LoRA-Based Backdoor Attack on Model Merging
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Model Merging
        </strong>
        &amp;
        <strong>
         Backdoor Attack
        </strong>
        &amp;
        <strong>
         LoRA
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        Universit√© de Sherbrooke&amp;University of Kinshasa
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.16642">
         Preventing Jailbreak Prompts as Malicious Tools for Cybercriminals: A Cyber Defense Perspective
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Prompts
        </strong>
        &amp;
        <strong>
         Cyber Defense
        </strong>
        &amp;
        <strong>
         AI Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        Google DeepMind
       </td>
       <td style="text-align: center;">
        NeurIPS 2024 SafeGenAI Workshop
       </td>
       <td style="text-align: center;">
        <a href="https://openreview.net/forum?id=0YdEK1Dnbn">
         Does Safety Training of LLMs Generalize to Semantically Related Natural Prompts?
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Training
        </strong>
        &amp;
        <strong>
         LLM Generalization
        </strong>
        &amp;
        <strong>
         Toxic Prompts
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        Speechmatic, MATS
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.03556">
         Best-of-N Jailbreaking
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         AI Systems
        </strong>
        &amp;
        <strong>
         Defense Circumvention
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        Uppsala University, The University of Hong Kong, Tencent Inc., Research Institutes of Sweden
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.19335">
         PEFT-as-an-Attack! Jailbreaking Language Models during Federated Parameter-Efficient Fine-Tuning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         PEFT Attack
        </strong>
        &amp;
        <strong>
         Federated Learning
        </strong>
        &amp;
        <strong>
         Privacy-Preserving Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        University of Central Florida, University of Maryland, College Park, Princeton University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.05232">
         LIAR: Leveraging Alignment (Best-of-N) to Jailbreak LLMs in Seconds
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Alignment Techniques
        </strong>
        &amp;
        <strong>
         Reinforcement Learning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        Palisade Research
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.05346">
         BadGPT-4o: Stripping Safety Fine-Tuning from GPT Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Safety Fine-Tuning
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        Beijing Electronic Science and Technology Institute, AVIC Nanjing Engineering Institute of Aircraft Systems
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.05892">
         BAMBA: A Bimodal Adversarial Multi-Round Black-Box Jailbreak Attacker for LVLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Bimodal Adversarial Attack
        </strong>
        &amp;
        <strong>
         LVLMs
        </strong>
        &amp;
        <strong>
         Black-Box Jailbreaking
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        Sun Yat-Sen University, Nanyang Technological University, Alibaba Group, Zhejiang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.05934">
         Heuristic-Induced Multimodal Risk Distribution Jailbreak Attack for Multimodal Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal Large Language Models
        </strong>
        &amp;
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Risk Distribution
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        Oregon State University, University of British Columbia, Rutgers University, George Mason University, Pacific Northwest National Laboratories
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.07192">
         PrisonBreak: Jailbreaking Large Language Models with Fewer Than Twenty-Five Targeted Bit-Rips
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Bitwise Corruptions
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        Institute of Information Engineering, Chinese Academy of Sciences, School of Computer Science, Georgia Institute of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.08156">
         Antelope: Potent and Concealed Jailbreak Attack Strategy
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Generative Models
        </strong>
        &amp;
        <strong>
         Adversarial Prompts
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        Huazhong University of Science and Technology, Nanyang Technological University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.08201">
         Model-Editing-Based Jailbreak against Safety-aligned Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Model Editing
        </strong>
        &amp;
        <strong>
         Safety-aligned LLMs
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        University of Illinois at Urbana-Champaign
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.08608">
         ADVWAVE: Stealthy Adversarial Jailbreak Attack Against Large Audio-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Attack
        </strong>
        &amp;
        <strong>
         Audio-Language Models
        </strong>
        &amp;
        <strong>
         Gradient Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        University of West Florida
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.08755">
         Proactive Adversarial Defense: Harnessing Prompt Tuning in Vision-Language Models to Detect Unseen Backdoored Images
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         Backdoor Detection
        </strong>
        &amp;
        <strong>
         Vision-Language Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        Soochow University, University of Alberta, Tencent
       </td>
       <td style="text-align: center;">
        COLING 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.09173">
         Exploiting the Index Gradients for Optimization-Based Jailbreaking on Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Optimization-Based Attack
        </strong>
        &amp;
        <strong>
         Gradient Information
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        FAIR, Meta
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.10321">
         AdvPrefix: An Objective for Nuanced LLM Jailbreaks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Objective
        </strong>
        &amp;
        <strong>
         Prefix-Forcing
        </strong>
        &amp;
        <strong>
         LLM Alignment
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        Wuhan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.10807">
         Towards Action Hijacking of Large Language Model-based Agent
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Action Hijacking
        </strong>
        &amp;
        <strong>
         Memory Leakage
        </strong>
        &amp;
        <strong>
         LLM Agent Vulnerability
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        Sichuan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.11109">
         SpearBot: Leveraging Large Language Models in a Generative-Critique Framework for Spear-Phishing Email Generation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Spear-Phishing
        </strong>
        &amp;
        <strong>
         Jailbreak Techniques
        </strong>
        &amp;
        <strong>
         Critique-Based Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        Guangdong University of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.12621">
         Jailbreaking? One Step Is Enough!
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Reverse Embedded Defense Attack (REDA)
        </strong>
        &amp;
        <strong>
         In-Context Learning (ICL)
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        Beijing University of Posts and Telecommunications
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.13879">
         Crabs: Consuming Resource via Auto-generation for LLM-DoS Attack under Black-box Settings
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM-DoS Attack
        </strong>
        &amp;
        <strong>
         AutoDoS Algorithm
        </strong>
        &amp;
        <strong>
         Length Trojan
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        Cornell Tech
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.14113">
         Adversarial Hubness in Multi-Modal Retrieval
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Hubness
        </strong>
        &amp;
        <strong>
         Multi-Modal Retrieval
        </strong>
        &amp;
        <strong>
         Embedding-Based Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        National University of Singapore, Harbin Institute of Technology (Shenzhen)
       </td>
       <td style="text-align: center;">
        ICML TiFA Workshop 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.15614">
         Suffix Injection and Projected Gradient Descent Can Easily Fool An MLLM
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Suffix Injection
        </strong>
        &amp;
        <strong>
         Projected Gradient Descent (PGD)
        </strong>
        &amp;
        <strong>
         Multi-Modal Large Language Models (MLLM)
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        Hong Kong Baptist University, NVIDIA AI Technology Center
       </td>
       <td style="text-align: center;">
        AAAI 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.15503">
         Meme Trojan: Backdoor Attacks Against Hateful Meme Detection via Cross-Modal Triggers
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Attacks
        </strong>
        &amp;
        <strong>
         Hateful Meme Detection
        </strong>
        &amp;
        <strong>
         Cross-Modal Triggers
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        Fudan University
       </td>
       <td style="text-align: center;">
        AAAI 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.15623">
         JailPO: A Novel Black-box Jailbreak Framework via Preference Optimization Against Aligned LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Preference Optimization
        </strong>
        &amp;
        <strong>
         LLM Vulnerabilities
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        Zhejiang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.16633">
         POEX: Policy Executable Embodied AI Jailbreak Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Embodied AI
        </strong>
        &amp;
        <strong>
         Policy Executable Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        University of Maryland, Baltimore County
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.16359">
         Human-Readable Adversarial Prompts: An Investigation into LLM Vulnerabilities Using Situational Context
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Prompts
        </strong>
        &amp;
        <strong>
         Language Model Vulnerabilities
        </strong>
        &amp;
        <strong>
         Situational Context
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        Tsinghua University, Hefei University of Technology, Shanghai Qi Zhi Institute
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.15289">
         SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Assistive Task
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        OpenAI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.18693">
         Diverse and Effective Red Teaming with Auto-generated Rewards and Multi-step Reinforcement Learning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Red Teaming
        </strong>
        &amp;
        <strong>
         Reinforcement Learning
        </strong>
        &amp;
        <strong>
         Diversity Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        Safe AI for Humanity
       </td>
       <td style="text-align: center;">
        NeurIPS 2024 Safe GenAI Workshop
       </td>
       <td style="text-align: center;">
        Can Safety Fine-Tuning Be More Principled? Lessons Learned from Cybersecurity
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Safety Fine-Tuning
        </strong>
        &amp;
        <strong>
         Cybersecurity Lessons
        </strong>
        &amp;
        <strong>
         Jailbreak Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Squirrel Ai Learning
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.00055">
         LLM-Virus: Evolutionary Jailbreak Attack on Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Safety
        </strong>
        &amp;
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Evolutionary Algorithm
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Beihang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.00517">
         A Method for Enhancing the Safety of Large Model Generation Based on Multi-dimensional Attack and Defense
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Alignment
        </strong>
        &amp;
        <strong>
         Attack-Defense Strategy
        </strong>
        &amp;
        <strong>
         Large Language Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Tsinghua University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.01042">
         Image-based Multimodal Models as Intruders: Transferable Multimodal Attacks on Video-based MLLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Transferable Attacks
        </strong>
        &amp;
        <strong>
         Multimodal Models
        </strong>
        &amp;
        <strong>
         Adversarial Robustness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Technical University of Munich
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.07959">
         Self-Instruct Few-Shot Jailbreaking: Decompose the Attack into Pattern and Behavior Learning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Few-Shot Jailbreaking
        </strong>
        &amp;
        <strong>
         Adversarial Prompt Engineering
        </strong>
        &amp;
        <strong>
         Model Vulnerability
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Beihang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.04931">
         Jailbreaking Multimodal Large Language Models via Shuffle Inconsistency
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal Large Language Models
        </strong>
        &amp;
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Shuffle Inconsistency
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Institute of Software, Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.01830">
         AUTO-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Red-Teaming
        </strong>
        &amp;
        <strong>
         Reinforcement Learning
        </strong>
        &amp;
        <strong>
         Vulnerability Discovery
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        The University of Oxford
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.10800">
         Jailbreaking Large Language Models in Infinitely Many Ways
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Infinitely Many Meanings
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Nanyang Technological University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.13115">
         Dagger Behind Smile: Fool LLMs with a Happy Ending Story
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Positive Prompt Exploitation
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Beihang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.13563">
         Black-Box Adversarial Attack on Vision-Language Models for Autonomous Driving
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         Autonomous Driving
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        University of Illinois Urbana-Champaign
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.14073">
         LLMs are Vulnerable to Malicious Prompts Disguised as Scientific Language
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking LLMs
        </strong>
        &amp;
        <strong>
         Bias in AI
        </strong>
        &amp;
        <strong>
         Scientific Language Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        The Hong Kong Polytechnic University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.14250">
         Siren: A Learning-Based Multi-Turn Attack Framework for Simulating Real-World Human Jailbreak Behaviors
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking LLMs
        </strong>
        &amp;
        <strong>
         Multi-Turn Attacks
        </strong>
        &amp;
        <strong>
         Adversarial Machine Learning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        UC Berkeley
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.15145">
         PromptShield: Deployable Detection for Prompt Injection Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection Detection
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.16727">
         xJailbreak: Representation Space Guided Reinforcement Learning for Interpretable LLM Jailbreaking
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Jailbreaking
        </strong>
        &amp;
        <strong>
         Reinforcement Learning
        </strong>
        &amp;
        <strong>
         Representation Learning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Georgia Institute of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.17433v1">
         Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Harmful Fine-tuning Attack
        </strong>
        &amp;
        <strong>
         Guardrail Moderation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Tsinghua University, Beijing Institute of Mathematical Sciences and Applications
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.18280v1">
         Jailbreaking LLMs‚Äô Safeguard with Universal Magic Words for Text Embedding Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Text Embedding Models
        </strong>
        &amp;
        <strong>
         Magic Words
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Saint Louis University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.18617">
         DarkMind: Latent Chain-of-Thought Backdoor in Customized LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Chain-of-Thought
        </strong>
        &amp;
        <strong>
         Backdoor Attack
        </strong>
        &amp;
        <strong>
         Customized LLMs
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Institut Polytechnique de Paris
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.18626">
         The TIP of the Iceberg: Revealing a Hidden Class of Task-in-Prompt Adversarial Attacks on LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Task-in-Prompt Attack
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        University of Pittsburgh
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.18632">
         Towards Safe AI Clinicians: A Comprehensive Study on Large Language Model Jailbreaking in Healthcare
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Jailbreaking
        </strong>
        &amp;
        <strong>
         Healthcare AI Safety
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Amazon Bedrock Science
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.18638">
         Graph of Attacks with Pruning: Optimizing Stealthy Jailbreak Prompt Generation for Enhanced LLM Content Moderation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Prompt Generation
        </strong>
        &amp;
        <strong>
         Content Moderation
        </strong>
        &amp;
        <strong>
         Graph of Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        University of Sydney
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.00735">
         From Compliance to Exploitation: Jailbreak Prompt Attacks on Multimodal LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal LLMs
        </strong>
        &amp;
        <strong>
         Jailbreak Prompt Attacks
        </strong>
        &amp;
        <strong>
         Adversarial Robustness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Zhejiang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.00840">
         Activation Approximations Can Incur Safety Vulnerabilities Even in Aligned LLMs: Comprehensive Analysis and Defense
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Activation Approximation
        </strong>
        &amp;
        <strong>
         LLM Safety
        </strong>
        &amp;
        <strong>
         Security Alignment
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        CISPA Helmholtz Center for Information Security
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.01241">
         Peering Behind the Shield: Guardrail Identification in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Guardrails
        </strong>
        &amp;
        <strong>
         Adversarial Prompts
        </strong>
        &amp;
        <strong>
         Security Auditing
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        University of Pennsylvania
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.01633">
         Adversarial Reasoning at Jailbreaking Time
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Jailbreaking
        </strong>
        &amp;
        <strong>
         Adversarial Reasoning
        </strong>
        &amp;
        <strong>
         Security Vulnerabilities
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        University of Toronto
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.01925">
         PANDAS: Improving Many-shot Jailbreaking via Positive Affirmation, Negative Demonstration, and Adaptive Sampling
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Many-shot Jailbreaking
        </strong>
        &amp;
        <strong>
         Adaptive Sampling
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        University of Massachusetts Amherst
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.02542">
         OVERTHINK: Slowdown Attacks on Reasoning LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Reasoning LLMs
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         Computational Overhead
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        CSIRO‚Äôs Data61
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.02960">
         Large Language Model Adversarial Landscape Through the Lens of Attack Objectives
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Security
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         Threat Taxonomy
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Brown University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.04322">
         SPEAK EASY: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         LLM Vulnerabilities
        </strong>
        &amp;
        <strong>
         Multilingual Exploits
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        The University of Sydney
       </td>
       <td style="text-align: center;">
        ICLR 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.03052">
         Understanding and Enhancing the Transferability of Jailbreaking Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking Attacks
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
        &amp;
        <strong>
         Adversarial Transferability
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        National Taiwan University
       </td>
       <td style="text-align: center;">
        NAACL 2025 Findings
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.01154">
         Jailbreaking with Universal Multi-Prompts
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking Attacks
        </strong>
        &amp;
        <strong>
         Universal Multi-Prompts
        </strong>
        &amp;
        <strong>
         Adversarial Robustness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        City University of Hong Kong
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.05224">
         A Survey on Backdoor Threats in Large Language Models (LLMs): Attacks, Defenses, and Evaluations
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Attacks
        </strong>
        &amp;
        <strong>
         Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Tsinghua University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.05772">
         Effective Black-Box Multi-Faceted Attacks Breach Vision Large Language Model Guardrails
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision-Language Models (VLLMs)
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         AI Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        AIRI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.07987">
         Universal Adversarial Attack on Aligned Multimodal LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Universal Adversarial Attack
        </strong>
        &amp;
        <strong>
         Multimodal LLMs
        </strong>
        &amp;
        <strong>
         Security Vulnerability
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Columbia University, University of Maryland
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.08586">
         Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Agents Security
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         Jailbreaking
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Scale AI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.09638">
         LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         AI Safety
        </strong>
        &amp;
        <strong>
         Red Teaming
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Tsinghua Shenzhen International Graduate School
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.09723">
         Making Them a Malicious Database: Exploiting Query Code to Jailbreak Aligned Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Query Code Attacks
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Technion - Israel Institute of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.09755">
         Enhancing Jailbreak Attacks via Compliance-Refusal-Based Initialization
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Compliance-Refusal
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Beihang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.11054">
         Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multi-Turn Jailbreak
        </strong>
        &amp;
        <strong>
         Reasoning-Driven Attack
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Guangdong University of Foreign Studies
       </td>
       <td style="text-align: center;">
        ICLR 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.10438">
         Injecting Universal Jailbreak Backdoors into LLMs in Minutes
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Backdoor
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Zhejiang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.11084">
         Rewrite to Jailbreak: Discover Learnable and Transferable Implicit Harmfulness Instruction
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        East China Normal University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.11379">
         CCJA: Context-Coherent Jailbreak Attack for Aligned Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Security Vulnerabilities
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        The Chinese University of Hong Kong, Shenzhen
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.12202">
         BoT: Breaking Long Thought Processes of o1-like Large Language Models through Backdoor Attack
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Attack
        </strong>
        &amp;
        <strong>
         Chain-of-Thought Reasoning
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Beijing University of Posts and Telecommunications, Nanyang Technological University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.12575">
         DemonAgent: Dynamically Encrypted Multi-Backdoor Implantation Attack on LLM-based Agent
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Security
        </strong>
        &amp;
        <strong>
         Backdoor Attacks
        </strong>
        &amp;
        <strong>
         Agent Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        University of Zagreb
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.12630">
         Automating Prompt Leakage Attacks on Large Language Models Using Agentic Approach
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Leakage
        </strong>
        &amp;
        <strong>
         Agentic Approach
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Duke University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.12893">
         H-CoT: Hijacking the Chain-of-Thought Safety Reasoning Mechanism to Jailbreak Large Reasoning Models, Including OpenAI o1/o3, DeepSeek-R1, and Gemini 2.0 Flash Thinking
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Chain-of-Thought Reasoning
        </strong>
        &amp;
        <strong>
         Large Reasoning Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Peking University
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.13527">
         Exploiting Prefix-Tree in Structured Output Interfaces for Enhancing Jailbreak Attacking
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Structured Output
        </strong>
        &amp;
        <strong>
         Prefix-Tree
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        A*STAR, Singapore
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.14529">
         CORBA: Contagious Recursive Blocking Attacks on Multi-Agent Systems Based on Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multi-Agent Systems
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
        &amp;
        <strong>
         Blocking Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        University of Oxford
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.14828">
         Fundamental Limitations in Defending LLM Finetuning APIs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Fine-Tuning Attacks
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
        &amp;
        <strong>
         Pointwise Detection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Michigan State University
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.14847">
         Red-Teaming LLM Multi-Agent Systems via Communication Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multi-Agent Systems
        </strong>
        &amp;
        <strong>
         Communication Attacks
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        University of Illinois Urbana-Champaign
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.17832">
         MM-POISONRAG: Disrupting Multimodal RAG with Local and Global Poisoning Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal RAG
        </strong>
        &amp;
        <strong>
         Knowledge Poisoning
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Tsinghua University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.19883">
         Beyond the Tip of Efficiency: Uncovering the Submerged Threats of Jailbreak Attacks in Small Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Small Language Models
        </strong>
        &amp;
        <strong>
         Security Risks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Stanford University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.19537">
         No, of course I can! Refusal Mechanisms Can Be Exploited Using Harmless Fine-Tuning Data
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Refusal Mechanisms
        </strong>
        &amp;
        <strong>
         Fine-Tuning Attacks
        </strong>
        &amp;
        <strong>
         Jailbreaking LLMs
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Nanyang Technological University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.18101">
         Detecting Offensive Memes with Social Biases in Singapore Context Using Multimodal Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal LLM
        </strong>
        &amp;
        <strong>
         Content Moderation
        </strong>
        &amp;
        <strong>
         Online Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Indian Institute of Technology Gandhinagar
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.16901">
         Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in Multilingual LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Cross-lingual Backdoor Attacks
        </strong>
        &amp;
        <strong>
         Multilingual LLMs
        </strong>
        &amp;
        <strong>
         Security Vulnerabilities
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Peking University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.16109">
         Be a Multitude to Itself: A Prompt Evolution Framework for Red Teaming
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Red Teaming
        </strong>
        &amp;
        <strong>
         Prompt Evolution
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Shanghai Artificial Intelligence Laboratory
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.15806">
         A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of Iterative Chaos
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Large Reasoning Models
        </strong>
        &amp;
        <strong>
         Security Risks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Pennsylvania State University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.19820">
         Foot-In-The-Door: A Multi-turn Jailbreak for LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Multi-turn Exploitation
        </strong>
        &amp;
        <strong>
         LLM Vulnerabilities
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Amazon Web Services
       </td>
       <td style="text-align: center;">
        NAACL 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.18504">
         TURBOFUZZLLM: Turbocharging Mutation-based Fuzzing for Effectively Jailbreaking Large Language Models in Practice
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking LLMs
        </strong>
        &amp;
        <strong>
         Mutation-based Fuzzing
        </strong>
        &amp;
        <strong>
         Security Evaluation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        J.P. Morgan AI Research
       </td>
       <td style="text-align: center;">
        AAAI'25 Workshop on Preventing and Detecting LLM Generated Misinformation
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.18608">
         Toward Breaking Watermarks in Distortion-free Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Watermark Breaking
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
        &amp;
        <strong>
         Spoofing Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Zhejiang University
       </td>
       <td style="text-align: center;">
        USENIX Security 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.18943">
         Towards Label-Only Membership Inference Attack against Pre-trained Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Membership Inference Attack
        </strong>
        &amp;
        <strong>
         Label-Only Attack
        </strong>
        &amp;
        <strong>
         Privacy Risks in LLMs
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Peking University, Ant Group
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.20952">
         Efficient Jailbreaking of Large Models by Freeze Training: Lower Layers Exhibit Greater Sensitivity to Harmful Content
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Fine-Tuning
        </strong>
        &amp;
        <strong>
         Model Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Sogang University, KAIST
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.20995">
         The RAG Paradox: A Black-Box Attack Exploiting Unintentional Vulnerabilities in Retrieval-Augmented Generation Systems
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Retrieval-Augmented Generation
        </strong>
        &amp;
        <strong>
         Black-Box Attack
        </strong>
        &amp;
        <strong>
         Document Poisoning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        The Hong Kong University of Science and Technology (Guangzhou), The University of Hong Kong
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.21059">
         FC-Attack: Jailbreaking Large Vision-Language Models via Auto-Generated Flowcharts
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Institute of Computing Technology, Chinese Academy of Sciences, University of Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.00038">
         From Benign to Toxic: Jailbreaking the Language Model via Adversarial Metaphors
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Adversarial Metaphors
        </strong>
        &amp;
        <strong>
         Large Language Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Duke University, Johns Hopkins University
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.01839">
         Jailbreaking Safeguarded Text-to-Image Models via Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Text-to-Image Models
        </strong>
        &amp;
        <strong>
         Adversarial Prompting
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Tsinghua University
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.01865">
         Guiding not Forcing: Enhancing the Transferability of Jailbreaking Attacks on LLMs via Removing Superfluous Constraints
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Transferability
        </strong>
        &amp;
        <strong>
         Gradient-Based Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        University of Chicago, Meta
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.01908">
         UDora: A Unified Red Teaming Framework against LLM Agents by Dynamically Hijacking Their Own Reasoning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Red Teaming
        </strong>
        &amp;
        <strong>
         LLM Agents
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        University of Massachusetts Amherst
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.03039">
         LLM Misalignment via Adversarial RLHF Platforms
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         RLHF Security
        </strong>
        &amp;
        <strong>
         LLM Misalignment
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        University of California, Davis, University of Southern California
       </td>
       <td style="text-align: center;">
        ICLR 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.00596">
         BADJUDGE: Backdoor Vulnerabilities of LLM-as-a-Judge
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM-as-a-Judge
        </strong>
        &amp;
        <strong>
         Backdoor Attacks
        </strong>
        &amp;
        <strong>
         Evaluation Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        AIM Intelligence
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.04856">
         One-Shot is Enough: Consolidating Multi-Turn Attacks into Efficient Single-Turn Prompts for LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Prompt Engineering
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        McGill University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.04957">
         SAFEARENA: Evaluating the Safety of Autonomous Web Agents
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Autonomous Web Agents
        </strong>
        &amp;
        <strong>
         Safety Evaluation
        </strong>
        &amp;
        <strong>
         Jailbreak Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Microsoft
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.05264">
         Jailbreaking is (Mostly) Simpler Than You Think
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Context Injection
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Beijing University of Posts and Telecommunications
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.06950">
         CtrlRAG: Black-box Adversarial Attacks Based on Masked Language Models in Retrieval-Augmented Language Generation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Retrieval-Augmented Generation
        </strong>
        &amp;
        <strong>
         Black-box Attack
        </strong>
        &amp;
        <strong>
         Masked Language Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        360 AI Security Lab
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.06989">
         Utilizing Jailbreak Probability to Attack and Safeguard Multimodal LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal LLMs
        </strong>
        &amp;
        <strong>
         Jailbreak Probability
        </strong>
        &amp;
        <strong>
         Adversarial Robustness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Zhejiang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.08195">
         Dialogue Injection Attack: Jailbreaking LLMs through Context Manipulation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Black-box Attack
        </strong>
        &amp;
        <strong>
         Dialogue Injection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Independent
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.08990">
         JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Fuzzing
        </strong>
        &amp;
        <strong>
         Red-Teaming
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Home Team Science and Technology Agency
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.09066">
         Probing Latent Subspaces in LLM for AI Security: Identifying and Manipulating Adversarial States
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Security
        </strong>
        &amp;
        <strong>
         Latent Representation
        </strong>
        &amp;
        <strong>
         Jailbreak Intervention
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Intology AI
       </td>
       <td style="text-align: center;">
        TrustworhtyLLM @ ICLR 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.10619">
         SIEGE: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multi-Turn Jailbreaking
        </strong>
        &amp;
        <strong>
         Tree Search
        </strong>
        &amp;
        <strong>
         Partial Compliance Tracking
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        University of Maryland, College Park
       </td>
       <td style="text-align: center;">
        NAACL 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.07697">
         PoisonedParrot: Subtle Data Poisoning Attacks to Elicit Copyright-Infringing Content from Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Data Poisoning
        </strong>
        &amp;
        <strong>
         Copyright Evasion
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        University of Liverpool
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.10872">
         TAIJI: Textual Anchoring for Immunizing Jailbreak Images in Vision Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision Language Models
        </strong>
        &amp;
        <strong>
         Jailbreak Defense
        </strong>
        &amp;
        <strong>
         Textual Anchoring
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Southeast University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.11619">
         Tit-for-Tat: Safeguarding Large Vision-Language Models Against Jailbreak Attacks via Adversarial Defense
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Jailbreak Defense
        </strong>
        &amp;
        <strong>
         Adversarial Training
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Southeast University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.11750">
         Making Every Step Effective: Jailbreaking Large Vision-Language Models Through Hierarchical KV Equalization
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Adversarial Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        University of Sydney
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.13081">
         A Framework to Assess Multilingual Vulnerabilities of LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multilingual Evaluation
        </strong>
        &amp;
        <strong>
         Jailbreak Detection
        </strong>
        &amp;
        <strong>
         LLM Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        University of Illinois Urbana-Champaign
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.15754">
         AutoRedTeamer: Autonomous Red Teaming with Lifelong Attack Integration
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Automated Red Teaming
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
        &amp;
        <strong>
         Attack Strategy Discovery
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Huazhong University of Science and Technology
       </td>
       <td style="text-align: center;">
        CVPR 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.16023">
         BadToken: Token-level Backdoor Attacks to Multi-modal Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Attack
        </strong>
        &amp;
        <strong>
         Multi-modal LLMs
        </strong>
        &amp;
        <strong>
         Token Manipulation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Independent Research
       </td>
       <td style="text-align: center;">
        IEEE CAI
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.15560">
         Temporal Context Awareness: A Defense Framework Against Multi-turn Manipulation Attacks on Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Security
        </strong>
        &amp;
        <strong>
         Multi-turn Attacks
        </strong>
        &amp;
        <strong>
         Temporal Risk Modeling
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        The University of Sydney, CSIRO Data61, The University of New South Wales
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.17198">
         Jailbreaking the Non-Transferable Barrier via Test-Time Data Disguising
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Non-Transferable Learning
        </strong>
        &amp;
        <strong>
         Model Security
        </strong>
        &amp;
        <strong>
         Test-Time Attack
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Tianjin University, Huawei Technologies
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.17987">
         Metaphor-based Jailbreaking Attacks on Text-to-Image Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking attacks
        </strong>
        &amp;
        <strong>
         Text-to-Image Models
        </strong>
        &amp;
        <strong>
         Adversarial Prompt
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        University of Waterloo, National University of Singapore, University of California Merced, University of Alberta, University of Queensland
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.19134">
         MIRAGE: Multimodal Immersive Reasoning and Guided Exploration for Red-Team Jailbreak Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal Jailbreaking
        </strong>
        &amp;
        <strong>
         Visual Storytelling
        </strong>
        &amp;
        <strong>
         Role Immersion
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        National Central University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.20320">
         Iterative Prompting with Persuasion Skills in Jailbreaking Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Iterative Prompting
        </strong>
        &amp;
        <strong>
         Persuasion Skill
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        KTH Royal Institute of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.21598">
         Prompt, Divide, and Conquer: Bypassing Large Language Model Safety Filters via Segmented and Distributed Prompt Processing
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Distributed Prompting
        </strong>
        &amp;
        <strong>
         LLM Jury
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        NAVER Cloud, KAIST, Republic of Korea Naval Academy, AITRICS
       </td>
       <td style="text-align: center;">
        CVPR 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.20823">
         Playing the Fool: Jailbreaking LLMs and Multimodal LLMs with Out-of-Distribution Strategy
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Out-of-Distribution Attack
        </strong>
        &amp;
        <strong>
         Multimodal LLMs
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        ICT, Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.24191">
         Output Constraints as Attack Surface: Exploiting Structured Generation to Bypass LLM Safety Mechanisms
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Constrained Decoding
        </strong>
        &amp;
        <strong>
         LLM Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        University of North Carolina at Chapel Hill
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.00218">
         Agents Under Siege: Breaking Pragmatic Multi-Agent LLM Systems with Optimized Prompt Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multi-Agent LLMs
        </strong>
        &amp;
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Permutation-Invariant Adversarial Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        University of Massachusetts Amherst
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.01094">
         Multilingual and Multi-Accent Jailbreaking of Audio LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Audio LLMs
        </strong>
        &amp;
        <strong>
         Multilingual Jailbreak
        </strong>
        &amp;
        <strong>
         Adversarial Audio Perturbations
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Wuhan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.01444">
         PiCo: Jailbreaking Multimodal Large Language Models via Pictorial Code Contextualization
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         Multimodal LLMs
        </strong>
        &amp;
        <strong>
         Visual Prompt Jailbreak
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        ‚Äî
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.02080">
         Evolving Security in LLMs: A Study of Jailbreak Attacks and Defenses
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         LLM Defense
        </strong>
        &amp;
        <strong>
         Security Evaluation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        University of Granada
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.04976">
         A DOMAIN-BASED TAXONOMY OF JAILBREAK VULNERABILITIES IN LARGE LANGUAGE MODELS
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak
        </strong>
        &amp;
        <strong>
         LLMs
        </strong>
        &amp;
        <strong>
         Model alignment
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        CUHK
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.03759">
         Emerging Cyber Attack Risks of Medical AI Agents
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Medical AI Agents
        </strong>
        &amp;
        <strong>
         Cybersecurity
        </strong>
        &amp;
        <strong>
         Prompt Injection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        University of Southern California
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.03770">
         JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language Model
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Detection
        </strong>
        &amp;
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Test-Time Adaptation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Nankai University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.03957">
         Practical Poisoning Attacks against Retrieval-Augmented Generation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Retrieval-Augmented Generation
        </strong>
        &amp;
        <strong>
         Poisoning Attacks
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Xi‚Äôan Jiaotong University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.05689">
         Separator Injection Attack: Uncovering Dialogue Biases in Large Language Models Caused by Role Separators
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection
        </strong>
        &amp;
        <strong>
         Dialogue Bias
        </strong>
        &amp;
        <strong>
         Role Separators
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Shanghai University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.05605">
         ShadowCoT: Cognitive Hijacking for Stealthy Reasoning Backdoors in LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Reasoning Backdoors
        </strong>
        &amp;
        <strong>
         Chain-of-Thought
        </strong>
        &amp;
        <strong>
         LLM Robustness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Shanghai University of Engineering Science
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.05652">
         Sugar-Coated Poison: Benign Generation Unlocks LLM Jailbreaking
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak
        </strong>
        &amp;
        <strong>
         Defense Threshold Decay
        </strong>
        &amp;
        <strong>
         Adversarial Reasoning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Amazon
       </td>
       <td style="text-align: center;">
        TrustNLP Workshop @ NAACL 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.03174">
         Multi-lingual Multi-turn Automated Red Teaming for LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Automated Red Teaming
        </strong>
        &amp;
        <strong>
         Multi-lingual Safety
        </strong>
        &amp;
        <strong>
         Multi-turn Jailbreak
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Sun Yat-Sen University
       </td>
       <td style="text-align: center;">
        CVPR 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.05838">
         Mind the Trojan Horse: Image Prompt Adapter Enabling Scalable and Deceptive Jailbreaking
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Image Prompt Adapter
        </strong>
        &amp;
        <strong>
         Hijacking Attack
        </strong>
        &amp;
        <strong>
         T2I Diffusion Model Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Tongji University
       </td>
       <td style="text-align: center;">
        SIGIR 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.07717">
         PR-Attack: Coordinated Prompt-RAG Attacks on Retrieval-Augmented Generation in Large Language Models via Bilevel Optimization
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Retrieval-Augmented Generation
        </strong>
        &amp;
        <strong>
         Poisoning Attacks
        </strong>
        &amp;
        <strong>
         Bilevel Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        National University of Singapore
       </td>
       <td style="text-align: center;">
        ICLR 2025 Workshop
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.08104">
         GENESHIFT: IMPACT OF DIFFERENT SCENARIO SHIFT ON JAILBREAKING LLM
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Black-box Attack
        </strong>
        &amp;
        <strong>
         Scenario Shift
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Hefei University of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.09841">
         StruPhantom: Evolutionary Injection Attacks on Black-Box Tabular Agents Powered by Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Indirect Prompt Injection
        </strong>
        &amp;
        <strong>
         LLM Agents
        </strong>
        &amp;
        <strong>
         Tabular Data Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        GSA - FedRAMP
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.10603">
         Demo: ViolentUTF as An Accessible Platform for Generative AI Red Teaming
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Red Teaming
        </strong>
        &amp;
        <strong>
         LLM Evaluation
        </strong>
        &amp;
        <strong>
         AI Security Platform
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        ETH Zurich
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.10694">
         The Jailbreak Tax: How Useful are Your Jailbreak Outputs?
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Tax
        </strong>
        &amp;
        <strong>
         LLM Alignment
        </strong>
        &amp;
        <strong>
         Utility Evaluation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Xidian University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.11106">
         Token-Level Constraint Boundary Search for Jailbreaking Text-to-Image Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Text-to-Image model
        </strong>
        &amp;
        <strong>
         jailbreak attack
        </strong>
        &amp;
        <strong>
         constraint optimization problem
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Mindgard
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.11168">
         Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection
        </strong>
        &amp;
        <strong>
         Jailbreak Detection
        </strong>
        &amp;
        <strong>
         LLM Guardrails
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        The Hong Kong Polytechnic University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.11182">
         Exploring Backdoor Attack and Defense for LLM-empowered Recommendations
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Attack
        </strong>
        &amp;
        <strong>
         LLM-based RecSys
        </strong>
        &amp;
        <strong>
         Poison Scanner
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Case Western Reserve University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.13052">
         GraphAttack: Exploiting Representational Blindspots in LLM Safety Mechanisms
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Safety
        </strong>
        &amp;
        <strong>
         Jailbreak Prompt
        </strong>
        &amp;
        <strong>
         Semantic Graph Attack
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        University of California, Los Angeles
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.13203">
         X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Multi-Turn Red-Teaming
        </strong>
        &amp;
        <strong>
         LLM Safety Alignment
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        China Agricultural University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.13775">
         BADAPEX: Backdoor Attack Based on Adaptive Optimization Mechanism of Black-Box Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Attack
        </strong>
        &amp;
        <strong>
         Black-box LLMs
        </strong>
        &amp;
        <strong>
         Prompt Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Xi‚Äôan Jiaotong University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.16125">
         Breaking the Prompt Wall (I): A Real-World Case Study of Attacking ChatGPT via Lightweight Prompt Injection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection
        </strong>
        &amp;
        <strong>
         ChatGPT Security
        </strong>
        &amp;
        <strong>
         Adversarial Prompting
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        National University of Defense Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.16429">
         Give LLMs a Security Course: Securing Retrieval-Augmented Code Generation via Knowledge Injection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Retrieval-Augmented Code Generation
        </strong>
        &amp;
        <strong>
         Software Security
        </strong>
        &amp;
        <strong>
         Knowledge Injection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Shandong University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.16489">
         Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based Multi-Agent Debate
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multi-Agent Debate
        </strong>
        &amp;
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Prompt Rewriting
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        University of Virginia
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.17130">
         Steering the CensorShip: Uncovering Representation Vectors for LLM ‚ÄúThought‚Äù Control
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Censorship Steering
        </strong>
        &amp;
        <strong>
         Representation Engineering
        </strong>
        &amp;
        <strong>
         Refusal‚ÄìCompliance Vector
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        East China Normal University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.17480">
         Unified attacks to large language model watermarks: spoofing and scrubbing in unauthorized knowledge distillation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Watermark
        </strong>
        &amp;
        <strong>
         Knowledge Distillation
        </strong>
        &amp;
        <strong>
         Spoofing &amp; Scrubbing Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Independent Researcher
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.18333">
         Adversarial Attacks on LLM-as-a-Judge Systems: Insights from Prompt Injections
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         LLM-as-a-Judge
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Hong Kong University of Science and Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.18598">
         BadMoE: Backdooring Mixture-of-Experts LLMs via Optimizing Routing Triggers and Infecting Dormant Experts
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Attack
        </strong>
        &amp;
        <strong>
         Mixture-of-Experts LLMs
        </strong>
        &amp;
        <strong>
         AI Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Cardiff University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.19019">
         Graph of Attacks: Improved Black-Box and Interpretable Jailbreaks for LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Prompting
        </strong>
        &amp;
        <strong>
         Black-Box Jailbreaking
        </strong>
        &amp;
        <strong>
         Graph Reasoning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Huazhong University of Science and Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.19793">
         Prompt Injection Attack to Tool Selection in LLM Agents
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection
        </strong>
        &amp;
        <strong>
         Tool Selection
        </strong>
        &amp;
        <strong>
         LLM Agent Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Dreadnode
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.19855">
         The Automation Advantage in AI Red Teaming
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         AI Red Teaming
        </strong>
        &amp;
        <strong>
         Automation vs Manual Attacks
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        University of California, Merced
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.20493">
         Token-Efficient Prompt Injection Attack: Provoking Cessation in LLM Reasoning via Adaptive Token Compression
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection
        </strong>
        &amp;
        <strong>
         Reasoning Interruption
        </strong>
        &amp;
        <strong>
         Token Compression
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        No Institute Given
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.21038">
         Prefill-Based Jailbreak: A Novel Approach of Bypassing LLM Safety Boundary
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Prefill-based Attack
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Binjiang Institute of Zhejiang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.21053">
         NeuRel-Attack: Neuron Relearning for Safety Disalignment in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Neuron Relearning
        </strong>
        &amp;
        <strong>
         Safety Alignment
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Beijing University of Posts and Telecommunications
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.21680">
         Hoist with His Own Petard: Inducing Guardrails to Facilitate Denial-of-Service Attacks on Retrieval-Augmented Generation of LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         RAG
        </strong>
        &amp;
        <strong>
         Jailbreak Prompt
        </strong>
        &amp;
        <strong>
         Denial-of-Service
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        University of Pavia
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.21700">
         XBreaking: Explainable Artificial Intelligence for Jailbreaking LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Explainable AI
        </strong>
        &amp;
        <strong>
         White-box Manipulation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Nankai University
       </td>
       <td style="text-align: center;">
        WWW 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.21668">
         Traceback of Poisoning Attacks to Retrieval-Augmented Generation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         RAG
        </strong>
        &amp;
        <strong>
         Poisoning Attacks
        </strong>
        &amp;
        <strong>
         Traceback Forensics
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        University of Toronto
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.00467">
         Red Teaming Large Language Models for Healthcare
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Healthcare Red Teaming
        </strong>
        &amp;
        <strong>
         Clinical Safety
        </strong>
        &amp;
        <strong>
         LLM Vulnerability
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        University of Texas at San Antonio
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.01900">
         CAMOUFLAGE: Exploiting Misinformation Detection Systems Through LLM-driven Adversarial Claim Transformation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         Misinformation Detection
        </strong>
        &amp;
        <strong>
         Large Language Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        University of Electronic Science and Technology of China
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.03501">
         BadLingual: A Novel Lingual-Backdoor Attack against Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Attack
        </strong>
        &amp;
        <strong>
         Multilingual LLMs
        </strong>
        &amp;
        <strong>
         Adversarial Training
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Universidad de Concepci√≥n
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.04784">
         A Proposal for Evaluating the Operational Risk for ChatBots based on Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Operational Risk
        </strong>
        &amp;
        <strong>
         Chatbot Security
        </strong>
        &amp;
        <strong>
         LLM Evaluation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Independent Researcher
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.04806">
         Red Teaming the Mind of the Machine: A Systematic Evaluation of Prompt Injection and Jailbreak Vulnerabilities in LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection
        </strong>
        &amp;
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        University at Buffalo
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.06493v1">
         System Prompt Poisoning: Persistent Attacks on Large Language Models Beyond User Injection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         System Prompt Poisoning
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.06579v1">
         POISONCRAFT: Practical Poisoning of Retrieval-Augmented Generation for Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Retrieval-Augmented Generation
        </strong>
        &amp;
        <strong>
         Data Poisoning
        </strong>
        &amp;
        <strong>
         Language Model Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Beijing Institute of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.06643v1">
         Practical Reasoning Interruption Attacks on Reasoning Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Reasoning Interruption Attack
        </strong>
        &amp;
        <strong>
         Reasoning Token Overflow
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        University of Virginia
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.06843v1">
         Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely Breaks Safety
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Fine-tuning Vulnerabilities
        </strong>
        &amp;
        <strong>
         LLM Safety Alignment
        </strong>
        &amp;
        <strong>
         Outlier Detection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Xidian University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.07167v1">
         One Trigger Token Is Enough: A Defense Strategy for Balancing Safety and Usability in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Defense
        </strong>
        &amp;
        <strong>
         Shallow Safety Alignment
        </strong>
        &amp;
        <strong>
         Trigger Token
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Ben Gurion University of the Negev
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.10066v1">
         Dark LLMs: The Growing Threat of Unaligned AI Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak
        </strong>
        &amp;
        <strong>
         Dark LLMs
        </strong>
        &amp;
        <strong>
         AI Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Stony Brook University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.10846">
         AutoRAN: Weak-to-Strong Jailbreaking of Large Reasoning Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Large Reasoning Models
        </strong>
        &amp;
        <strong>
         Prompt Engineering
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        University of Electronic Science and Technology of China
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.11154">
         MPMA: Preference Manipulation Attack Against Model Context Protocol
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Model Context Protocol
        </strong>
        &amp;
        <strong>
         Preference Manipulation Attack
        </strong>
        &amp;
        <strong>
         Tool Selection Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Duke University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.11717">
         ENVINJECTION: Environmental Prompt Injection Attack to Multi-Modal Web Agents
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection
        </strong>
        &amp;
        <strong>
         Multi-modal Web Agent
        </strong>
        &amp;
        <strong>
         Adversarial Robustness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Wuhan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.11790v2">
         JULI: Jailbreak Large Language Models by Self-Introspection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Jailbreak
        </strong>
        &amp;
        <strong>
         Log Probability Manipulation
        </strong>
        &amp;
        <strong>
         BiasNet
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        University of Sydney
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.12287">
         The Tower of Babel Revisited: Multilingual Jailbreak Prompts on Closed-Source Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multilingual Jailbreak
        </strong>
        &amp;
        <strong>
         Closed-Source LLMs
        </strong>
        &amp;
        <strong>
         Prompt Injection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        The Hong Kong University of Science and Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.12442">
         IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Multi-Agent Systems
        </strong>
        &amp;
        <strong>
         IP Leakage
        </strong>
        &amp;
        <strong>
         Prompt Injection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        University of Minnesota
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.12567">
         A Survey of Attacks on Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Security
        </strong>
        &amp;
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Prompt Injection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        University of Edinburgh
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.13028">
         Evaluating the Efficacy of LLM Safety Solutions: The Palit Benchmark Dataset
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Security
        </strong>
        &amp;
        <strong>
         Prompt Injections
        </strong>
        &amp;
        <strong>
         Jailbreaks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        None
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.13348">
         Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM-as-a-Judge
        </strong>
        &amp;
        <strong>
         Prompt Injection
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Pengcheng Laboratory
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.14103">
         AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Audio Jailbreak
        </strong>
        &amp;
        <strong>
         End-to-End LALMs
        </strong>
        &amp;
        <strong>
         Over-the-Air Attack
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Universit√© Paris-Saclay
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.14226">
         ‚ÄúHaet Bhasha aur Diskrimineshun‚Äù: Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Red-Teaming
        </strong>
        &amp;
        <strong>
         Phonetic Perturbation
        </strong>
        &amp;
        <strong>
         Code-Mixing
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        University of Cambridge
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.14286">
         Universal Acoustic Adversarial Attacks for Flexible Control of Speech-LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Acoustic Adversarial Attack
        </strong>
        &amp;
        <strong>
         Speech-LLM
        </strong>
        &amp;
        <strong>
         Selective Control
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Henan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.14316">
         Exploring Jailbreak Attacks on LLMs through Intent Concealment and Diversion
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Prompt Obfuscation
        </strong>
        &amp;
        <strong>
         Security Benchmark
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        LMU Munich
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.14368">
         Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection
        </strong>
        &amp;
        <strong>
         Open-Source LLMs
        </strong>
        &amp;
        <strong>
         Hypnotism Attack
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Shanghai Jiao Tong University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.14418">
         Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Attack
        </strong>
        &amp;
        <strong>
         GUI Agents
        </strong>
        &amp;
        <strong>
         MLLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Indian Institute of Technology Kharagpur
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.14469">
         Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Code-Mixing
        </strong>
        &amp;
        <strong>
         Safety Alignment
        </strong>
        &amp;
        <strong>
         Attribution Analysis
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Google DeepMind
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.14534">
         Lessons from Defending Gemini Against Indirect Prompt Injections
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Indirect Prompt Injection
        </strong>
        &amp;
        <strong>
         Adversarial Evaluation
        </strong>
        &amp;
        <strong>
         Red-Teaming
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        National University of Singapore
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.15420v1">
         Silent Leaks: Implicit Knowledge Extraction Attack on RAG Systems through Benign Queries
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         RAG
        </strong>
        &amp;
        <strong>
         Knowledge Extraction
        </strong>
        &amp;
        <strong>
         Benign Queries
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Tsinghua University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.15656v1">
         Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data Could Be Secretly Stolen!
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Attack
        </strong>
        &amp;
        <strong>
         Data Extraction
        </strong>
        &amp;
        <strong>
         Fine-Tuning Vulnerability
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Imperial College London
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.15738v1">
         Alignment Under Pressure: The Case for Informed Adversaries When Evaluating LLM Defenses
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection
        </strong>
        &amp;
        <strong>
         LLM Alignment
        </strong>
        &amp;
        <strong>
         Adversarial Robustness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Nanyang Technological University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.16241v1">
         Three Minds, One Legend: Jailbreak Large Reasoning Model with Adaptive Stacked Ciphers
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Large Reasoning Models
        </strong>
        &amp;
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Stacked Encryption
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        ETH Zurich
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.16567v1">
         Finetuning-Activated Backdoors in LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Attack
        </strong>
        &amp;
        <strong>
         Meta-Learning
        </strong>
        &amp;
        <strong>
         LLM Fine-Tuning Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Huazhong University of Science and Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.16640v1">
         BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision-Language-Action Models
        </strong>
        &amp;
        <strong>
         Backdoor Attack
        </strong>
        &amp;
        <strong>
         Objective-Decoupled Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Nanyang Technological University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.16670v1">
         BitHydra: Towards Bit-flip Inference Cost Attack against Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Bit-Flip Attack
        </strong>
        &amp;
        <strong>
         Inference Cost
        </strong>
        &amp;
        <strong>
         LLM Robustness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Nankai University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.16765v1">
         When Safety Detectors Aren't Enough: A Stealthy and Effective Jailbreak Attack on LLMs via Steganographic Techniques
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Steganography
        </strong>
        &amp;
        <strong>
         LLM Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Indiana University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.16888v1">
         CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious System Prompt Generation and Refining Framework
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Prompt
        </strong>
        &amp;
        <strong>
         System Prompt Attack
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Harbin Institute of Technology (Shenzhen)
       </td>
       <td style="text-align: center;">
        ICLR 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.17598">
         ONE MODEL TRANSFER TO ALL: ON ROBUST JAILBREAK PROMPTS GENERATION AGAINST LLMS
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Robust Prompt Generation
        </strong>
        &amp;
        <strong>
         Transferability
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Google DeepMind
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.18773v1">
         Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Membership Inference Attack
        </strong>
        &amp;
        <strong>
         Large Language Models
        </strong>
        &amp;
        <strong>
         Privacy Risk
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        University of Missouri-Kansas City
       </td>
       <td style="text-align: center;">
        DSN 2025 Workshop
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.18864v1">
         Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box Framework
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Audio Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Multimodal Large Language Model
        </strong>
        &amp;
        <strong>
         SpeechGPT
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Xidian University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.19864v1">
         CPA-RAG: Covert Poisoning Attacks on Retrieval-Augmented Generation in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Poisoning Attack
        </strong>
        &amp;
        <strong>
         Retrieval-Augmented Generation
        </strong>
        &amp;
        <strong>
         Black-box Attack
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        University of G√∂ttingen
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.20118v2">
         TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Steganography
        </strong>
        &amp;
        <strong>
         Privacy Leakage
        </strong>
        &amp;
        <strong>
         Trojan LLM
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        ELLIS Institute T√ºbingen
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.20162v1">
         Capability-Based Scaling Laws for LLM Red-Teaming
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Red-Teaming
        </strong>
        &amp;
        <strong>
         Scaling Laws
        </strong>
        &amp;
        <strong>
         Jailbreak Attack
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Institute of Computing Technology, Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.21184">
         PoisonSwarm: Universal Harmful Information Synthesis via Model Crowdsourcing
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Harmful Information Detection
        </strong>
        &amp;
        <strong>
         Data Synthesis
        </strong>
        &amp;
        <strong>
         Model Crowdsourcing
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        State Key Laboratory of Intelligent Game, Institute of Software Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.21499v1">
         AdInject: Real-World Black-Box Attacks on Web Agents via Advertising Delivery
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Web Agent Security
        </strong>
        &amp;
        <strong>
         Adversarial Environment Injection
        </strong>
        &amp;
        <strong>
         Advertising Attack
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Korea Advanced Institute of Science and Technology (KAIST)
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.21556v1">
         Benign-to-Toxic Jailbreaking: Inducing Harmful Responses from Harmless Prompts
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Vision-Language Model
        </strong>
        &amp;
        <strong>
         Adversarial Attack
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        The Ohio State University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.21936">
         REDTEAMCUA: Realistic Adversarial Testing of Computer-Use Agents in Hybrid Web-OS Environments
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Testing
        </strong>
        &amp;
        <strong>
         Computer-Use Agent
        </strong>
        &amp;
        <strong>
         Prompt Injection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        University of Maryland
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.23001v1">
         DyePack: Provably Flagging Test Set Contamination in LLMs Using Backdoors
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Test Set Contamination
        </strong>
        &amp;
        <strong>
         Backdoor Detection
        </strong>
        &amp;
        <strong>
         Benchmark Integrity
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Beijing University of Posts and Telecommunications
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.23404v1">
         Adaptive Jailbreaking Strategies Based on the Semantic Understanding Capabilities of Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Jailbreaking
        </strong>
        &amp;
        <strong>
         AI Security
        </strong>
        &amp;
        <strong>
         Adaptive Jailbreaking Strategies
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Beihang University
       </td>
       <td style="text-align: center;">
        ACL 2025 Findings
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.21277v2">
         Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks through Expanding Strategy Space
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Strategy Space Expansion
        </strong>
        &amp;
        <strong>
         Genetic Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Huazhong University of Science and Technology; Lehigh University
       </td>
       <td style="text-align: center;">
        ACL 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.23561v1">
         Merge Hijacking: Backdoor Attacks to Model Merging of Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Model Merging
        </strong>
        &amp;
        <strong>
         Backdoor Attack
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
     </tbody>
    </table>
    <h2>
     üíªPresentations &amp; Talks
    </h2>
    <h2>
     üìñTutorials &amp; Workshops
    </h2>
    <table>
     <thead>
      <tr>
       <th style="text-align: center;">
        Date
       </th>
       <th style="text-align: center;">
        Type
       </th>
       <th style="text-align: center;">
        Title
       </th>
       <th style="text-align: center;">
        URL
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td style="text-align: center;">
        23.01
       </td>
       <td style="text-align: center;">
        Community
       </td>
       <td style="text-align: center;">
        Reddit/ChatGPTJailbrek
       </td>
       <td style="text-align: center;">
        <a href="https://www.reddit.com/r/ChatGPTJailbreak">
         link
        </a>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.02
       </td>
       <td style="text-align: center;">
        Resource&amp;Tutorials
       </td>
       <td style="text-align: center;">
        Jailbreak Chat
       </td>
       <td style="text-align: center;">
        <a href="https://www.jailbreakchat.com/">
         link
        </a>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.10
       </td>
       <td style="text-align: center;">
        Tutorials
       </td>
       <td style="text-align: center;">
        Awesome-LLM-Safety
       </td>
       <td style="text-align: center;">
        <a href="https://github.com/ydyjya/Awesome-LLM-Safety">
         link
        </a>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.10
       </td>
       <td style="text-align: center;">
        Article
       </td>
       <td style="text-align: center;">
        Adversarial Attacks on LLMs(Author: Lilian Weng)
       </td>
       <td style="text-align: center;">
        <a href="https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/">
         link
        </a>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Video
       </td>
       <td style="text-align: center;">
        [1hr Talk] Intro to Large Language Models
        <br/>
        From 45:45(Author: Andrej Karpathy)
       </td>
       <td style="text-align: center;">
        <a href="https://www.youtube.com/watch?v=zjkBMFhNj_g">
         link
        </a>
       </td>
      </tr>
     </tbody>
    </table>
    <h2>
     üì∞News &amp; Articles
    </h2>
    <table>
     <thead>
      <tr>
       <th style="text-align: center;">
        Date
       </th>
       <th style="text-align: center;">
        Type
       </th>
       <th style="text-align: center;">
        Title
       </th>
       <th style="text-align: center;">
        Author
       </th>
       <th style="text-align: center;">
        URL
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td style="text-align: center;">
        23.10
       </td>
       <td style="text-align: center;">
        Article
       </td>
       <td style="text-align: center;">
        Adversarial Attacks on LLMs
       </td>
       <td style="text-align: center;">
        Lilian Weng
       </td>
       <td style="text-align: center;">
        <a href="https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/">
         link
        </a>
       </td>
      </tr>
     </tbody>
    </table>
    <h2>
     üßë‚Äçüè´Scholars
    </h2>
   </div>
   <a class="back-to-home" href="../index_cn.html">
    <i class="fas fa-arrow-left">
    </i>
    ËøîÂõûÈ¶ñÈ°µ
   </a>
  </div>
  <footer>
   <div class="container">
    <div class="footer-content">
     <p>
      ‰ΩúËÄÖ:
      <a href="https://github.com/ydyjya">
       ydyjya
      </a>
     </p>
     <p>
      ËÅîÁ≥ªÊñπÂºè: zhouzhenhong@bupt.edu.cn
     </p>
    </div>
    <div class="footer-links">
     <a href="https://github.com/ydyjya/Awesome-LLM-Safety" target="_blank">
      <i class="fab fa-github">
      </i>
      GitHub ‰ªìÂ∫ì
     </a>
    </div>
   </div>
  </footer>
 </body>
</html>
