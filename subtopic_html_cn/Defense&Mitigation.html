<!DOCTYPE html>
<html lang="zh-CN">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   Defense - Awesome LLM-Safety
  </title>
  <link href="../style.css" rel="stylesheet"/>
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet"/>
  <style>
   .markdown-content {
            padding: 20px;
        }
        .markdown-content h1 {
            font-size: 2rem;
            color: var(--primary-color);
            margin-bottom: 1.5rem;
            padding-bottom: 0.5rem;
            border-bottom: 2px solid var(--border-color);
        }
        .markdown-content h2 {
            font-size: 1.6rem;
            color: var(--secondary-color);
            margin-top: 2rem;
            margin-bottom: 1rem;
        }
        .markdown-content table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            font-size: 0.9rem;
        }
        .markdown-content th, .markdown-content td {
            padding: 12px 15px;
            border: 1px solid var(--border-color);
        }
        .markdown-content th {
            background-color: var(--primary-color);
            color: white;
            font-weight: bold;
            text-align: left;
        }
        .markdown-content tr:nth-child(even) {
            background-color: var(--light-bg);
        }
        .markdown-content tr:hover {
            background-color: #ddd;
        }
        .back-to-home {
            display: inline-block;
            margin: 20px 0;
            padding: 10px 15px;
            background-color: var(--primary-color);
            color: white;
            border-radius: 4px;
            text-decoration: none;
            font-weight: bold;
        }
        .back-to-home:hover {
            background-color: var(--secondary-color);
            color: white;
        }
  </style>
 </head>
 <body>
  <header>
   <div class="container">
    <h1>
     üõ°Ô∏è Awesome LLM-Safety üõ°Ô∏è
    </h1>
    <div class="language-switch">
     <a href="../index.html">
      English
     </a>
     |
     <a class="active" href="../index_cn.html">
      ‰∏≠Êñá
     </a>
    </div>
   </div>
  </header>
  <div class="container">
   <a class="back-to-home" href="../index_cn.html">
    <i class="fas fa-arrow-left">
    </i>
    ËøîÂõûÈ¶ñÈ°µ
   </a>
   <div class="markdown-content">
    <h1>
     Defense
    </h1>
    <h2>
     Different from the main READMEüïµÔ∏è
    </h2>
    <ul>
     <li>
      Within this subtopic, we will be updating with the latest articles. This will help researchers in this area to quickly understand recent trends.
     </li>
     <li>
      In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
     </li>
     <li>
      Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"
     </li>
    </ul>
    <h2>
     üìëPapers
    </h2>
    <table>
     <thead>
      <tr>
       <th style="text-align: center;">
        Date
       </th>
       <th style="text-align: center;">
        Institute
       </th>
       <th style="text-align: center;">
        Publication
       </th>
       <th style="text-align: center;">
        Paper
       </th>
       <th style="text-align: center;">
        Keywords
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td style="text-align: center;">
        21.07
       </td>
       <td style="text-align: center;">
        Google Research
       </td>
       <td style="text-align: center;">
        ACL2022
       </td>
       <td style="text-align: center;">
        <a href="https://aclanthology.org/2022.acl-long.577/">
         Deduplicating Training Data Makes Language Models Better
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Privacy Protected
        </strong>
        &amp;
        <strong>
         Deduplication
        </strong>
        &amp;
        <strong>
         Memorization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.05
       </td>
       <td style="text-align: center;">
        UC Davis, USC, UW-Madison
       </td>
       <td style="text-align: center;">
        NAACL2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2305.14910">
         From Shortcuts to Triggers: Backdoor Defense with Denoised PoE
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Attacks
        </strong>
        &amp;
        <strong>
         Defense Methods
        </strong>
        &amp;
        <strong>
         Natural Language Processing
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.08
       </td>
       <td style="text-align: center;">
        Georgia Tech, Intel Labs
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2308.07308">
         LLM Self Defense: By Self Examination LLMs Know They Are Being Tricked
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         Self Defense
        </strong>
        &amp;
        <strong>
         Harmful Content Detection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.08
       </td>
       <td style="text-align: center;">
        University of Michigan
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2308.14132v3">
         DETECTING LANGUAGE MODEL ATTACKS WITH PERPLEXITY
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Suffixes
        </strong>
        &amp;
        <strong>
         Perplexity
        </strong>
        &amp;
        <strong>
         Attack Detection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.09
       </td>
       <td style="text-align: center;">
        University of Maryland
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2309.02705">
         Certifying LLM Safety against Adversarial Prompting
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Filter
        </strong>
        &amp;
        <strong>
         Adversarial Prompts
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.09
       </td>
       <td style="text-align: center;">
        University of Maryland
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2309.00614">
         BASELINE DEFENSES FOR ADVERSARIAL ATTACKS AGAINST ALIGNED LANGUAGE MODELS
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Perplexity
        </strong>
        &amp;
        <strong>
         Input Preprocessing
        </strong>
        &amp;
        <strong>
         Adversarial Training
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.09
       </td>
       <td style="text-align: center;">
        The Pennsylvania State University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2309.14348">
         DEFENDING AGAINST ALIGNMENT-BREAKING ATTACKS VIA ROBUSTLY ALIGNED LLM
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Alignment-Breaking Attacks
        </strong>
        &amp;
        <strong>
         Adversarial Prompts
        </strong>
        &amp;
        <strong>
         Jailbreaking Prompts
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.10
       </td>
       <td style="text-align: center;">
        University of Pennsylvania
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2310.03684">
         SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak
        </strong>
        &amp;
        <strong>
         Adversarial Attack
        </strong>
        &amp;
        <strong>
         Perturbation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.10
       </td>
       <td style="text-align: center;">
        Michigan State University
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2310.02417">
         Jailbreaker in Jail: Moving Target Defense for Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Dialogue System
        </strong>
        &amp;
        <strong>
         Trustworthy Machine Learning
        </strong>
        &amp;
        <strong>
         Moving Target Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.10
       </td>
       <td style="text-align: center;">
        Peking University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2310.06387">
         Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         In-Context Learning
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         In-Context Demonstrations
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.10
       </td>
       <td style="text-align: center;">
        The Chinese University of Hong Kong&amp;Microsoft
       </td>
       <td style="text-align: center;">
        NAACL2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2310.15851">
         SELF-GUARD: Empower the LLM to Safeguard Itself
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Large Language Models
        </strong>
        &amp;
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Safety Mechanisms
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        University of California Irvine
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.00172">
         Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Prompt Shield
        </strong>
        &amp;
        <strong>
         Safety Classifier
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Child Health Evaluative Sciences
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.02748">
         Pyclipse, a library for deidentification of free-text clinical notes
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Clinical Text Data
        </strong>
        &amp;
        <strong>
         Deidentification
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Tsinghua University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.09096">
         Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking Attacks
        </strong>
        &amp;
        <strong>
         Goal Prioritization
        </strong>
        &amp;
        <strong>
         Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        University of Southern California, Harvard University, University of California Davis, University of Wisconsin-Madison
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.09763">
         Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Attacks
        </strong>
        &amp;
        <strong>
         Defensive Demonstrations
        </strong>
        &amp;
        <strong>
         Test-Time Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        University of Maryland College Park
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.11509">
         Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Prompt Detection
        </strong>
        &amp;
        <strong>
         Perplexity Measures
        </strong>
        &amp;
        <strong>
         Token-level Analysis
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        Rensselaer Polytechnic Institute, Northeastern University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.00029">
         Combating Adversarial Attacks through a Conscience-Based Alignment Framework
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         Conscience-Based Alignment
        </strong>
        &amp;
        <strong>
         Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        Azure Research, Microsoft Security Response Center
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.11513">
         Maatphor: Automated Variant Analysis for Prompt Injection Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection Attacks
        </strong>
        &amp;
        <strong>
         Automated Variant Analysis
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        University of Massachusetts Amherst, Columbia University, Google, Stanford University, New York University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.12736">
         Learning and Forgetting Unsafe Examples in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Issues
        </strong>
        &amp;
        <strong>
         ForgetFilter Algorithm
        </strong>
        &amp;
        <strong>
         Unsafe Content
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        UC Berkeley, King Abdulaziz City for Science and Technology
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.17673">
         Jatmo: Prompt Injection Defense by Task-Specific Finetuning
        </a>
       </td>
       <td style="text-align: center;">
        Prompt Injection&amp;LLM Security
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        Arizona State University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.00287">
         The Art of Defending: A Systematic Evaluation and Analysis of LLM Defense Strategies on Safety and Over-Defensiveness
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety
        </strong>
        &amp;
        <strong>
         Over-Defensiveness
        </strong>
        &amp;
        <strong>
         Defense Strategies
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        Logistic and Supply Chain MultiTech R&amp;D Centre (LSCM)
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.00994">
         Detection and Defense Against Prominent Attacks on Preconditioned LLM-Integrated Virtual Assistants
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Preconditioning
        </strong>
        &amp;
        <strong>
         Cyber Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        The Hong Kong University of Science and Technology, University of Illinois at Urbana-Champaign, The Hong Kong Polytechnic University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.02906">
         MLLM-Protector: Ensuring MLLM‚Äôs Safety without Hurting Performance
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal Large Language Models (MLLMs)
        </strong>
        &amp;
        <strong>
         Safety
        </strong>
        &amp;
        <strong>
         Malicious Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        Carnegie Mellon University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.06121">
         TOFU: A Task of Fictitious Unlearning for LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Data Privacy
        </strong>
        &amp;
        <strong>
         Ethical Concerns
        </strong>
        &amp;
        <strong>
         Unlearning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        Wuhan University, The University of Sydney
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.06561">
         Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Intention Analysis
        </strong>
        &amp;
        <strong>
         Jailbreak Defense
        </strong>
        &amp;
        <strong>
         Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        The Hong Kong Polytechnic University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.07612">
         Signed-Prompt: A New Approach to Prevent Prompt Injection Attacks Against LLM-Integrated Applications
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         AI Security
        </strong>
        &amp;
        <strong>
         Prompt Injection Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        University of Illinois at Urbana-Champaign, University of Chicago
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.17263">
         Robust Prompt Optimization for Defending Large Language Models Against Jailbreaking Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         AI Alignment
        </strong>
        &amp;
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Robust Prompt Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Arizona State University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.06655">
         Adversarial Text Purification: A Large Language Model Approach for Defense
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Textual Adversarial Defenses
        </strong>
        &amp;
        <strong>
         Adversarial Purification
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Peking University, Wuhan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.06255">
         Fight Back Against Jailbreaking via Prompt Adversarial Tuning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking Attacks
        </strong>
        &amp;
        <strong>
         Prompt Adversarial Tuning
        </strong>
        &amp;
        <strong>
         Defense Mechanisms
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        University of Washington, The Pennsylvania State University, Allen Institute for AI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.08983">
         SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Safety-Aware Decoding
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Shanghai Artificial Intelligence Laboratory
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.09283">
         Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Conversation Safety
        </strong>
        &amp;
        <strong>
         Attacks
        </strong>
        &amp;
        <strong>
         Defenses
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        University of Notre Dame, INRIA&amp;King Abdullah University of Science and Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.13148">
         Defending Jailbreak Prompts via In-Context Adversarial Game
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Training
        </strong>
        &amp;
        <strong>
         Jailbreak Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        University of New South Wales Australia, Delft University of Technology The Netherlands&amp;Nanyang Technological University Singapore
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.13457">
         LLM Jailbreak Attack versus Defense Techniques - A Comprehensive Study
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Defense Techniques
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        The Hong Kong University of Science and Technology, Duke University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.13494">
         GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety-Critical Gradient Analysis
        </strong>
        &amp;
        <strong>
         Unsafe Prompt Detection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        The University of Melbourne
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.13517">
         Round Trip Translation Defence against Large Language Model Jailbreaking Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Social-Engineered Attacks
        </strong>
        &amp;
        <strong>
         Round Trip Translation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Nanyang Technological University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.15727">
         LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A Vision Paper
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking Attacks
        </strong>
        &amp;
        <strong>
         Self-Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Ajou University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.15180">
         Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Self-Refinement
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        UCLA
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.16459">
         Defending LLMs against Jailbreaking Attacks via Backtranslation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backtranslation
        </strong>
        &amp;
        <strong>
         Jailbreaking Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        University of California Santa Barbara
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.16192">
         Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Semantic Smoothing
        </strong>
        &amp;
        <strong>
         Jailbreak Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        University of Wisconsin-Madison
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.14968">
         Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Fine-tuning Attack
        </strong>
        &amp;
        <strong>
         Backdoor Alignment
        </strong>
        &amp;
        <strong>
         Safety Examples
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        University of Exeter
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.14857">
         Is the System Message Really Important to Jailbreaks in Large Language Models?
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak
        </strong>
        &amp;
        <strong>
         System Messages
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        The Chinese University of Hong Kong, IBM Research
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.00867">
         Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Refusal Loss
        </strong>
        &amp;
        <strong>
         Gradient Cuff
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        Oregon State University, Pennsylvania State University, CISPA Helmholtz Center for Information Security
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.04783">
         AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Defense Mechanisms
        </strong>
        &amp;
        <strong>
         Jailbreak Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        Peking University, University of Wisconsin‚ÄìMadison, International Digital Economy Academy, University of California Davis
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.09513">
         AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal Large Language Models Safety
        </strong>
        &amp;
        <strong>
         Defense Strategy
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        Southern University of Science and Technology, Hong Kong University of Science and Technology, Huawei Noah‚Äôs Ark Lab, Peng Cheng Laboratory
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.09572">
         Eyes Closed Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal LLMs
        </strong>
        &amp;
        <strong>
         Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        UIUC, Virginia Tech, Salesforce Research, University of California Berkeley, UChicago
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.13031">
         RIGORLLM: RESILIENT GUARDRAILS FOR LARGE LANGUAGE MODELS AGAINST UNDESIRED CONTENT
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Biases
        </strong>
        &amp;
        <strong>
         Harmful Content
        </strong>
        &amp;
        <strong>
         Resilient Guardrails
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        Microsoft
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.14720">
         Defending Against Indirect Prompt Injection Attacks With Spotlighting
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Indirect Prompt Injection
        </strong>
        &amp;
        <strong>
         Spotlighting
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        XiaMen University, Yanshan University, IDEA Research, Inner Mongolia University, Microsoft, Microsoft Research Asia
       </td>
       <td style="text-align: center;">
        NAACL2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.11838">
         Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Language Models
        </strong>
        &amp;
        <strong>
         Safety Guidelines
        </strong>
        &amp;
        <strong>
         Model Alignment
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        IIT Delhi
       </td>
       <td style="text-align: center;">
        NAACL2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.10088">
         Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task Instruction Tuning with RLAIF
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Counterspeech Generation
        </strong>
        &amp;
        <strong>
         Multi-Task Instruction Tuning
        </strong>
        &amp;
        <strong>
         Reinforcement Learning from AI Feedback (RLAIF)
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        Stony Brook University
       </td>
       <td style="text-align: center;">
        NAACL2024(findings)
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.17155">
         Task-Agnostic Detector for Insertion-Based Backdoor Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Detection
        </strong>
        &amp;
        <strong>
         Logit Features
        </strong>
        &amp;
        <strong>
         NLP Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        Chung-Ang University
       </td>
       <td style="text-align: center;">
        NAACL2024(findings)
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.15467">
         Don‚Äôt be a Fool: Pooling Strategies in Offensive Language Detection from User-Intended Adversarial Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Offensive Language Detection
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         Pooling Strategies
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        South China University of Technology&amp;Pazhou Laboratory
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.05880">
         Eraser: Jailbreaking Defense in Large Language Models via Unlearning Harmful Knowledge
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Unlearning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        Zhejiang University, Johns Hopkins University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.06666">
         SAFEGEN: Mitigating Unsafe Content Generation in Text-to-Image Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Text-to-Image Models
        </strong>
        &amp;
        <strong>
         Unsafe Content
        </strong>
        &amp;
        <strong>
         Content Mitigation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        Hong Kong University of Science and Technology, University of Oxford
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.08031">
         Latent Guard: A Safety Framework for Text-to-Image Generation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Text-to-Image Models
        </strong>
        &amp;
        <strong>
         Safety Framework
        </strong>
        &amp;
        <strong>
         Latent Guard
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        Nanjing University, Microsoft Research Asia, Tsinghua University, Queen Mary University of London, Pennsylvania State University,  NEC Laboratories America
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.13968">
         Protecting Your LLMs with Information Bottleneck
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Information Bottleneck
        </strong>
        &amp;
        <strong>
         Adversarial Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        Centre for Software Excellence Huawei, University of Manitoba, Queen‚Äôs University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.19048">
         A Framework for Real-time Safeguarding the Text Generation of Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Text Generation Safety
        </strong>
        &amp;
        <strong>
         Real-time Safeguarding
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        Princeton University&amp;UC Davis&amp;USC
       </td>
       <td style="text-align: center;">
        NAACL2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.02356">
         Two Heads are Better than One: Nested PoE for Robust Defense Against Multi-Backdoors
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Nested Product of Experts
        </strong>
        &amp;
        <strong>
         Data Poisoning
        </strong>
        &amp;
        <strong>
         Backdoor Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        Tsinghua University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.01509">
         Learnable Linguistic Watermarks for Tracing Model Extraction Attacks on Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Model Extraction Attacks
        </strong>
        &amp;
        <strong>
         Watermarking
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        Tsinghua University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.02365">
         Adaptive and Robust Watermark against Model Extraction Attack
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Model Extraction Attacks
        </strong>
        &amp;
        <strong>
         Watermarking
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        Johns Hopkins University
       </td>
       <td style="text-align: center;">
        ICML 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.07932">
         PARDEN: Can You Repeat That? Defending against Jailbreaks via Repetition
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaks
        </strong>
        &amp;
        <strong>
         Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        University of Edinburgh
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.09719">
         Spectral Editing of Activations for Large Language Model Alignment
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Bias Mitigation
        </strong>
        &amp;
        <strong>
         Truthfulness Enhancement
        </strong>
        &amp;
        <strong>
         Spectral Editing
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        East China Normal University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.09055">
         A Safety Realignment Framework via Subspace-Oriented Model Fusion for Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Model Fusion
        </strong>
        &amp;
        <strong>
         Safeguard Strategy
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        The Hong Kong University of Science and Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.07667">
         Backdoor Removal for Generative Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Attacks
        </strong>
        &amp;
        <strong>
         Generative Models
        </strong>
        &amp;
        <strong>
         Safety Training
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        Stony Brook University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.19358">
         Robustifying Safety-Aligned Large Language Models through Clean Data Curation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking Attacks
        </strong>
        &amp;
        <strong>
         Clean Data Curation
        </strong>
        &amp;
        <strong>
         LLM Safety Alignment
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        University of Pennsylvania
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.19544">
         One-Shot Safety Alignment for Large Language Models via Optimal Dualization
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Alignment
        </strong>
        &amp;
        <strong>
         Constrained RLHF
        </strong>
        &amp;
        <strong>
         Optimal Dualization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        Naver
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.19795">
         SLM as Guardian: Pioneering AI Safety with Small Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         AI Safety
        </strong>
        &amp;
        <strong>
         Small Language Models
        </strong>
        &amp;
        <strong>
         Harmful Query Detection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        The Chinese University of Hong Kong
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.20099">
         Defensive Prompt Patch: A Robust and Interpretable Defense of LLMs against Jailbreak Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Defensive Prompt Patch
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        Tokyo University of Agriculture and Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.20770">
         Large Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Robustness
        </strong>
        &amp;
        <strong>
         LLM Agent
        </strong>
        &amp;
        <strong>
         Textual Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        University of California, Riverside
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.02575">
         Cross-Modal Safety Alignment: Is Textual Unlearning All You Need?
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Cross-Modality Safety Alignment
        </strong>
        &amp;
        <strong>
         Textual Unlearning
        </strong>
        &amp;
        <strong>
         Vision-Language Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        University of Liverpool
       </td>
       <td style="text-align: center;">
        IEEE Transactions on Pattern Analysis and Machine Intelligence
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.02622">
         Safeguarding Large Language Models: A Survey
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Survey
        </strong>
        &amp;
        <strong>
         Safeguards
        </strong>
        &amp;
        <strong>
         Trustworthy AI
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Oregon State University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.03230">
         Defending Large Language Models Against Attacks With Residual Stream Activation Analysis
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Machine Learning
        </strong>
        &amp;
        <strong>
         Machine Learning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Huazhong University of Science and Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.03805">
         AutoJailbreak: Exploring Jailbreak Attacks and Defenses through a Dependency Lens
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Dependency Analysis
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        The Hong Kong University of Science and Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.05498">
         SELFDEFEND: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking Defense
        </strong>
        &amp;
        <strong>
         SELFDEFEND
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Princeton University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.05946">
         Safety Alignment Should Be Made More Than Just a Few Tokens Deep
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Alignment
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        The University of Alabama at Birmingham
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.05948">
         Chain-of-Scrutiny: Detecting Backdoor Attacks for Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Attacks
        </strong>
        &amp;
        <strong>
         Chain-of-Scrutiny
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        The Hong Kong University of Science and Technology (Guangzhou)
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.06622">
         Adversarial Tuning: Defending Against Jailbreak Attacks for LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Adversarial Tuning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Komorebi AI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.07188">
         Merging Improves Self-Critique Against Jailbreak Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Self-Critique
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Tsinghua Shenzhen International Graduate School
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.07594">
         MLLMGUARD: A Multi-dimensional Safety Evaluation Suite for Multimodal Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Evaluation
        </strong>
        &amp;
        <strong>
         MLLMs
        </strong>
        &amp;
        <strong>
         Multi-dimensional
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Purdue University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.08725">
         RL-JACK: Reinforcement Learning-powered Black-box Jailbreaking Attack against LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking Attacks
        </strong>
        &amp;
        <strong>
         Reinforcement Learning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Mohamed bin Zayed University of Artificial Intelligence
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.09250">
         MirrorCheck: Efficient Adversarial Defense for Vision-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Defense
        </strong>
        &amp;
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         MirrorCheck
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Teesside University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.11007">
         Threat Modelling and Risk Analysis for Large Language Model (LLM)-Powered Applications
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Threat Modelling
        </strong>
        &amp;
        <strong>
         Risk Analysis
        </strong>
        &amp;
        <strong>
         LLM-Powered Applications
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        NVIDIA Corporation
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.11036">
         garak: A Framework for Security Probing Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         garak
        </strong>
        &amp;
        <strong>
         Security Probing
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        University of Science and Technology of China
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.11285">
         Self and Cross-Model Distillation for LLMs: Effective Methods for Refusal Pattern Alignment
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Self Distillation
        </strong>
        &amp;
        <strong>
         Cross-Model Distillation
        </strong>
        &amp;
        <strong>
         Refusal Pattern Alignment
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        University of Washington
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.12257">
         CLEANGEN: Mitigating Backdoor Attacks for Generation Tasks in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         CLEANGEN
        </strong>
        &amp;
        <strong>
         Backdoor Attacks
        </strong>
        &amp;
        <strong>
         Generation Tasks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Columbia University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.12263">
         Defending Against Social Engineering Attacks in the Age of LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Social Engineering
        </strong>
        &amp;
        <strong>
         CSE Detection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Indian Institute of Technology Kharagpur
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.12274">
         SafeInfer: Context Adaptive Decoding Time Safety Alignment for Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         SafeInfer
        </strong>
        &amp;
        <strong>
         Context Adaptive Decoding
        </strong>
        &amp;
        <strong>
         Safety Alignment
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.16743">
         Adversarial Contrastive Decoding: Boosting Safety Alignment of Large Language Models via Opposite Prompt Optimization
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Alignment
        </strong>
        &amp;
        <strong>
         Contrastive Decoding
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        University of Toronto
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.02551">
         A False Sense of Safety: Unsafe Information Leakage in ‚ÄòSafe‚Äô AI Responses
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Information Leakage
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Tsinghua University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.02855">
         Safe Unlearning: A Surprisingly Effective and Generalizable Solution to Defend Against Jailbreak Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Unlearning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        National University of Singapore
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.03234">
         Self-Evaluation as a Defense Against Adversarial Attacks on LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         Self-Evaluation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Tianjin University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.03876">
         DART: Deep Adversarial Automated Red Teaming for LLM Safety
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Automated Red Teaming
        </strong>
        &amp;
        <strong>
         Adversarial Training
        </strong>
        &amp;
        <strong>
         LLM Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Seoul National University
       </td>
       <td style="text-align: center;">
        ACL 2024 Workshop
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.06851">
         Safe-Embed: Unveiling the Safety-Critical Knowledge of Sentence Encoders
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Sentence Encoders
        </strong>
        &amp;
        <strong>
         Safety-Critical Knowledge
        </strong>
        &amp;
        <strong>
         Unsafe Prompts
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Duke University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.09050">
         Refusing Safe Prompts for Multi-modal Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal LLMs
        </strong>
        &amp;
        <strong>
         Safe Prompt Refusal
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        The Chinese University of Hong Kong
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.09121">
         Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Training
        </strong>
        &amp;
        <strong>
         Refusal Position Bias
        </strong>
        &amp;
        <strong>
         Decoupled Refusal Training
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        GovTech Singapore
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.10995">
         LionGuard: Building a Contextualized Moderation Classifier to Tackle Localized Unsafe Content
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Moderation Classifier
        </strong>
        &amp;
        <strong>
         Localized Content
        </strong>
        &amp;
        <strong>
         Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Georgia Institute of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.15549">
         Targeted Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Latent Adversarial Training
        </strong>
        &amp;
        <strong>
         Robustness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Georgia Institute of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.16994">
         A Voter-Based Stochastic Rejection-Method Framework for Asymptotically Safe Language Model Outputs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Stochastic Rejection-Method
        </strong>
        &amp;
        <strong>
         Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Shanghai Jiao Tong University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.17075">
         SAFETY-J: Evaluating Safety with Critique
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Evaluation
        </strong>
        &amp;
        <strong>
         Critique-based Judgment
        </strong>
        &amp;
        <strong>
         Bilingual Evaluator
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Dynamo AI
       </td>
       <td style="text-align: center;">
        ICML 2024 Workshop
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.16318">
         PrimeGuard: Safe and Helpful LLMs through Tuning-Free Routing
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Inference-Time Guardrails
        </strong>
        &amp;
        <strong>
         Safety
        </strong>
        &amp;
        <strong>
         Helpfulness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        ShanghaiTech University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.21659">
         Defending Jailbreak Attack in VLMs via Cross-modality Information Detector
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Vision Language Models (VLMs)
        </strong>
        &amp;
        <strong>
         Cross-modality Information
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        University of Illinois Urbana-Champaign
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.00761">
         Tamper-Resistant Safeguards for Open-Weight LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Tamper Resistance
        </strong>
        &amp;
        <strong>
         Security
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Beijing University of Posts and Telecommunications
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.02632">
         SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Self-Evolving Framework
        </strong>
        &amp;
        <strong>
         Adversarial Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        University of Texas at San Antonio
       </td>
       <td style="text-align: center;">
        KDD 2024 AI4Cyber
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.02651">
         Can Reinforcement Learning Unlock the Hidden Dangers in Aligned Large Language Models?
        </a>
       </td>
       <td style="text-align: center;">
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         Alignment
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        University of Texas at Arlington
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.05667">
         Utilizing Large Language Models to Optimize the Detection and Explainability of Phishing Websites
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Phishing Detection
        </strong>
        &amp;
        <strong>
         Explainability
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        New York University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.06621">
         Towards Robust and Cost-Efficient Knowledge Unlearning for Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Knowledge Unlearning
        </strong>
        &amp;
        <strong>
         Cost-Efficiency
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Beijing University of Posts and Telecommunications
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.07663">
         Alignment-Enhanced Decoding: Defending via Token-Level Adaptive Refining of Probability Distributions
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Alignment-Enhanced Decoding
        </strong>
        &amp;
        <strong>
         Jailbreak Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        University of Science and Technology of China
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.08924">
         Prefix Guidance: A Steering Wheel for Large Language Models to Defend Against Jailbreak Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Defense
        </strong>
        &amp;
        <strong>
         Prefix Guidance
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        University of Liverpool
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.08959">
         Adaptive Guardrails for Large Language Models via Trust Modeling and In-Context Learning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adaptive Guardrails
        </strong>
        &amp;
        <strong>
         Trust Modeling
        </strong>
        &amp;
        <strong>
         In-Context Learning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        National University of Singapore
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.09093">
         BaThe: Defense against the Jailbreak Attack in Multimodal Large Language Models by Treating Harmful Instruction as Backdoor Trigger
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Defense
        </strong>
        &amp;
        <strong>
         Multimodal Large Language Models
        </strong>
        &amp;
        <strong>
         Backdoor Trigger
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Georgia Institute of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.09600">
         Antidote: Post-fine-tuning Safety Alignment for Large Language Models against Harmful Fine-tuning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Alignment
        </strong>
        &amp;
        <strong>
         Harmful Fine-tuning
        </strong>
        &amp;
        <strong>
         Post-fine-tuning Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        LG Electronics
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.11021">
         ATHENA: Safe Autonomous Agents with Verbal Contrastive Learning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Autonomous Agents
        </strong>
        &amp;
        <strong>
         Verbal Contrastive Learning
        </strong>
        &amp;
        <strong>
         Safety Evaluation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Duke Kunshan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.11308">
         EEG-Defender: Defending against Jailbreak through Early Exit Generation of Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Defense
        </strong>
        &amp;
        <strong>
         Early Exit Generation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Hong Kong University of Science and Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.14972">
         AgentMonitor: A Plug-and-Play Framework for Predictive and Secure Multi-Agent Systems
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multi-Agent Systems
        </strong>
        &amp;
        <strong>
         Predictive Modeling
        </strong>
        &amp;
        <strong>
         AgentMonitor
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        University of Maryland, College Park
       </td>
       <td style="text-align: center;">
        CoLM24
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.00598">
         Automatic Pseudo-Harmful Prompt Generation for Evaluating False Refusals in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         False Refusals
        </strong>
        &amp;
        <strong>
         Pseudo-Harmful Prompts
        </strong>
        &amp;
        <strong>
         Safety Alignment
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Georgia Institute of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.01586">
         BOOSTER: Tackling Harmful Fine-Tuning for Large Language Models via Attenuating Harmful Perturbation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Harmful Fine-Tuning
        </strong>
        &amp;
        <strong>
         LLM Alignment
        </strong>
        &amp;
        <strong>
         Perturbation Attenuation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        University of Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.03274">
         Recent Advances in Attack and Defense Approaches of Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Attack Approaches
        </strong>
        &amp;
        <strong>
         Defense Mechanisms
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing, Institute of Artificial Intelligence
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.03788">
         HSF: Defending against Jailbreak Attacks with Hidden State Filtering
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hidden State Filtering
        </strong>
        &amp;
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Defense Mechanism
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Mahindra International School, SuperAGI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.03793">
         Safeguarding AI Agents: Developing and Analyzing Safety Architectures
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         AI Agents
        </strong>
        &amp;
        <strong>
         Safety Architectures
        </strong>
        &amp;
        <strong>
         Risk Mitigation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Southern Illinois University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.07353">
         Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Adversarial Fine-Tuning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Noah's Ark Lab
       </td>
       <td style="text-align: center;">
        COLM 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.11365">
         CoCA: Regaining Safety-awareness of Multimodal Large Language Models with Constitutional Calibration
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal Large Language Models (MLLMs)
        </strong>
        &amp;
        <strong>
         Safety Awareness
        </strong>
        &amp;
        <strong>
         Constitutional Calibration
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Rutgers University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.14200">
         Data-centric NLP Backdoor Defense from the Lens of Memorization
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         NLP Backdoor Defense
        </strong>
        &amp;
        <strong>
         Memorization
        </strong>
        &amp;
        <strong>
         Data-centric Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Northwestern University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.14729">
         PROMPTFUZZ: Harnessing Fuzzing Techniques for Robust Testing of Prompt Injection in LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection
        </strong>
        &amp;
        <strong>
         Fuzzing
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        University of Minnesota
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.17275">
         On the Vulnerability of Applying Retrieval-Augmented Generation within Knowledge-Intensive Application Domains
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Retrieval-Augmented Generation
        </strong>
        &amp;
        <strong>
         Poisoning Attacks
        </strong>
        &amp;
        <strong>
         Knowledge-Intensive Applications
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        IBM Research Europe
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.17699">
         MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard for Prompt Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Mixture of Experts
        </strong>
        &amp;
        <strong>
         Tabular Classifiers
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Institute of Information Engineering, Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        EMNLP 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.16783">
         Holistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Red Teaming
        </strong>
        &amp;
        <strong>
         Multi-turn Interaction
        </strong>
        &amp;
        <strong>
         Automated Red Teaming
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Chegg Inc.
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.19476">
         Overriding Safety Protections of Open-Source Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Harmfulness
        </strong>
        &amp;
        <strong>
         Knowledge Drift
        </strong>
        &amp;
        <strong>
         Model Uncertainty
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        University of Toronto
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.20089">
         ROBUST LLM SAFEGUARDING VIA REFUSAL FEATURE ADVERSARIAL TRAINING
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Training
        </strong>
        &amp;
        <strong>
         LLM Safeguarding
        </strong>
        &amp;
        <strong>
         Refusal Feature
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        University of A Coru√±a
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.00775">
         Decoding Hate: Exploring Language Models' Reactions to Hate Speech
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hate Speech
        </strong>
        &amp;
        <strong>
         LLMs
        </strong>
        &amp;
        <strong>
         Mitigation Strategies
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Tel Aviv University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.01288">
         MITIGATING COPY BIAS IN IN-CONTEXT LEARNING THROUGH NEURON PRUNING
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Copy Bias
        </strong>
        &amp;
        <strong>
         In-Context Learning
        </strong>
        &amp;
        <strong>
         Neuron Pruning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.02298">
         JAILBREAK ANTIDOTE: RUNTIME SAFETY-UTILITY BALANCE VIA SPARSE REPRESENTATION ADJUSTMENT IN LARGE LANGUAGE MODELS
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         LLM Safety
        </strong>
        &amp;
        <strong>
         Sparse Representation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        CAS Key Laboratory of AI Safety
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.02684">
         HIDDENGUARD: FINE-GRAINED SAFE GENERATION WITH SPECIALIZED REPRESENTATION ROUTER
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safe Generation
        </strong>
        &amp;
        <strong>
         Representation Router
        </strong>
        &amp;
        <strong>
         Token-level Moderation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        HydroX AI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.03772">
         Precision Knowledge Editing: Enhancing Safety in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Knowledge Editing
        </strong>
        &amp;
        <strong>
         Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        IBM Research, MIT
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.03818">
         Large Language Models Can Be Strong Self-Detoxifiers
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Toxicity Reduction
        </strong>
        &amp;
        <strong>
         Self-Detoxification
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        UC Berkeley, Meta, FAIR
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.05451">
         Aligning LLMs to Be Robust Against Prompt Injection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection
        </strong>
        &amp;
        <strong>
         Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Purdue University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.06625">
         ETA: Evaluating Then Aligning Safety of Vision Language Models at Inference Time
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision Language Models
        </strong>
        &amp;
        <strong>
         Safety Alignment
        </strong>
        &amp;
        <strong>
         Inference Time
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Aerospace Information Research Institute, Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.06809">
         Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Decoding-Level Defense
        </strong>
        &amp;
        <strong>
         Jailbreak Prevention
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Rensselaer Polytechnic Institute, IBM Research
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.07471">
         SEAL: Safety-enhanced Aligned LLM Fine-tuning via Bilevel Data Selection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety-enhanced Aligned LLM
        </strong>
        &amp;
        <strong>
         Fine-tuning
        </strong>
        &amp;
        <strong>
         Bilevel Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        KAIST AI, Naver Cloud AI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.07571">
         HOW DOES VISION-LANGUAGE ADAPTATION IMPACT THE SAFETY OF VISION LANGUAGE MODELS?
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision-Language Adaptation
        </strong>
        &amp;
        <strong>
         Safety
        </strong>
        &amp;
        <strong>
         LVLMs
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Johns Hopkins University, Microsoft Responsible AI Research
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.08968">
         Controllable Safety Alignment: Inference-Time Adaptation to Diverse Safety Requirements
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety alignment
        </strong>
        &amp;
        <strong>
         Inference-time adaptation
        </strong>
        &amp;
        <strong>
         LLMs controllability
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Princeton University, Zoom Video Communications
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.09102">
         Instructional Segment Embedding: Improving LLM Safety with Instruction Hierarchy
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Instruction hierarchy
        </strong>
        &amp;
        <strong>
         LLM safety
        </strong>
        &amp;
        <strong>
         Instructional segment embedding
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Zhejiang University, Westlake University, Beihang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.10343">
         LOCKING DOWN THE FINETUNED LLMS SAFETY
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety alignment
        </strong>
        &amp;
        <strong>
         Fine-tuned LLMs
        </strong>
        &amp;
        <strong>
         Meta-SafetyLock
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Carnegie Mellon University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.13138">
         Data Defenses Against Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Data defenses
        </strong>
        &amp;
        <strong>
         Adversarial prompt injections
        </strong>
        &amp;
        <strong>
         Privacy protection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        University of Wisconsin-Madison
       </td>
       <td style="text-align: center;">
        NeurIPS 2024 Safe Generative AI Workshop
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.10014">
         Safety-Aware Fine-Tuning of Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety-aware fine-tuning
        </strong>
        &amp;
        <strong>
         Harmful data detection
        </strong>
        &amp;
        <strong>
         Data contamination
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Zhejiang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.13903">
         CoreGuard: Safeguarding Foundational Capabilities of LLMs Against Model Stealing in Edge Deployment
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Model Stealing
        </strong>
        &amp;
        <strong>
         Edge Deployment
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Nanyang Technological University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.14425">
         Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge Distillation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Defense
        </strong>
        &amp;
        <strong>
         Knowledge Distillation
        </strong>
        &amp;
        <strong>
         Parameter-Efficient Fine-Tuning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Stony Brook University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.19937">
         RobustKV: Defending Large Language Models Against Jailbreak Attacks via KV Eviction
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         LLM Defense
        </strong>
        &amp;
        <strong>
         KV Cache Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        UW-Madison
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.21492">
         FATH: Authentication-based Test-time Defense against Indirect Prompt Injection Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection Defense
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
        &amp;
        <strong>
         Authentication System
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Vanderbilt University Medical Center
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.22284">
         Embedding-based Classifiers Can Detect Prompt Injection Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection Detection
        </strong>
        &amp;
        <strong>
         Embedding-based Classification
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        University of Wisconsin-Madison
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.22770">
         InjecGuard: Benchmarking and Mitigating Over-defense in Prompt Injection Guardrail Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection Defense
        </strong>
        &amp;
        <strong>
         Over-defense Detection
        </strong>
        &amp;
        <strong>
         Guardrail Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        National Taiwan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.00348">
         Attention Tracker: Detecting Prompt Injection Attacks in LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection
        </strong>
        &amp;
        <strong>
         Attention Mechanism
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        National University of Singapore
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.00459">
         Defense Against Prompt Injection Attack by Leveraging Attack Techniques
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection Defense
        </strong>
        &amp;
        <strong>
         Attack Techniques
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        Georgia Institute of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.01703">
         UNIGUARD: Towards Universal Safety Guardrails for Jailbreak Attacks on Multimodal Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Multimodal Safety Guardrails
        </strong>
        &amp;
        <strong>
         Adversarial Robustness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        Mila ‚Äì Quebec AI Institute
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.06824">
         Combining Domain and Alignment Vectors to Achieve Better Knowledge-Safety Trade-offs in LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Domain Expertise
        </strong>
        &amp;
        <strong>
         Safety Alignment
        </strong>
        &amp;
        <strong>
         Model Merging
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        Anthropic
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.07494">
         Rapid Response: Mitigating LLM Jailbreaks with a Few Examples
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Defense
        </strong>
        &amp;
        <strong>
         Rapid Response
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        National University of Singapore
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.08410">
         The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Jailbreak Defense
        </strong>
        &amp;
        <strong>
         Model Alignment
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        Beijing University of Posts and Telecommunications
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.09259">
         Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Multimodal Generative Models
        </strong>
        &amp;
        <strong>
         Security Challenges
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        Singapore Management University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.12768">
         CROW: Eliminating Backdoors from Large Language Models via Internal Consistency Regularization
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Defense
        </strong>
        &amp;
        <strong>
         Consistency Regularization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        UC Berkeley
       </td>
       <td style="text-align: center;">
        COLING 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.14398">
         Lightweight Safety Guardrails Using Fine-tuned BERT Embeddings
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Guardrails
        </strong>
        &amp;
        <strong>
         Fine-tuned BERT
        </strong>
        &amp;
        <strong>
         Prompt Filtering
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        The Hong Kong University of Science and Technology (Guangzhou), Tsinghua University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.17453">
         PEFTGuard: Detecting Backdoor Attacks Against Parameter-Efficient Fine-Tuning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Detection
        </strong>
        &amp;
        <strong>
         PEFT
        </strong>
        &amp;
        <strong>
         LoRA
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        MIT,Speechmatics,MATS
       </td>
       <td style="text-align: center;">
        NeurIPS 2024 SoLaR workshops
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.02159">
         Jailbreak Defense in a Narrow Domain: Limitations of Existing Methods and a New Transcript-Classifier Approach
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Defense
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
        &amp;
        <strong>
         Transcript Classifier
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        Shanghai Jiao Tong University
       </td>
       <td style="text-align: center;">
        COLING 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.02454">
         Gracefully Filtering Backdoor Samples for Generative Large Language Models without Retraining
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Attacks
        </strong>
        &amp;
        <strong>
         Generative LLMs
        </strong>
        &amp;
        <strong>
         Frequency Space
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        NVIDIA Corporation
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.01547">
         Improved Large Language Model Jailbreak Detection via Pretrained Embeddings
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Detection
        </strong>
        &amp;
        <strong>
         Pretrained Embeddings
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        Fudan University, Worcester Polytechnic Institute
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.18948">
         Knowledge Database or Poison Base? Detecting RAG Poisoning Attack through LLM Activations
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         RAG Poisoning Attack
        </strong>
        &amp;
        <strong>
         LLM Activations
        </strong>
        &amp;
        <strong>
         Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        University of Maryland-College Park, Indian Institute of Technology Bombay, Princeton University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.18688">
         Immune: Improving Safety Against Jailbreaks in Multi-modal LLMs via Inference-Time Alignment
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Alignment
        </strong>
        &amp;
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Inference-Time Alignment
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        Not specified
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.06181">
         Enhancing Adversarial Resistance in LLMs with Recursion
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Resistance
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
        &amp;
        <strong>
         Prompt Simplification
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        University of Maryland, Capital One, New York University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.06748">
         Refusal Tokens: A Simple Way to Calibrate Refusals in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Refusal Tokens
        </strong>
        &amp;
        <strong>
         Model Calibration
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        Dalhousie University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.06846">
         Classifier-free Guidance in LLMs Safety
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Unlearning
        </strong>
        &amp;
        <strong>
         Reinforcement Learning
        </strong>
        &amp;
        <strong>
         Classifier-free Guidance
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        Princeton University, Google
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.07097">
         Evaluating the Durability of Safeguards for Open-Weight LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Safeguards
        </strong>
        &amp;
        <strong>
         Open-Weight Models
        </strong>
        &amp;
        <strong>
         Adversarial Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        Michigan State University, University of Hawaii at Manoa
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.07672">
         FlexLLM: Exploring LLM Customization for Moving Target Defense on Black-Box LLMs Against Jailbreak Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Customization
        </strong>
        &amp;
        <strong>
         Moving Target Defense
        </strong>
        &amp;
        <strong>
         Jailbreak Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        Neudesic
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.13435">
         Lightweight Safety Classification Using Pruned Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Model Pruning
        </strong>
        &amp;
        <strong>
         Content Safety
        </strong>
        &amp;
        <strong>
         Prompt Injection Detection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        Asan Medical Center
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.13705">
         Mitigating Adversarial Attacks in LLMs through Defensive Suffix Generation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Defense
        </strong>
        &amp;
        <strong>
         Gradient-Based Optimization
        </strong>
        &amp;
        <strong>
         Defensive Suffix
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        The Chinese University of Hong Kong, IBM Research
       </td>
       <td style="text-align: center;">
        AAAI 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.18171">
         Token Highlighter: Inspecting and Mitigating Jailbreak Prompts for Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Affirmation Loss
        </strong>
        &amp;
        <strong>
         Token-Level Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        MBZUAI, Huazhong University of Science and Technology, University of Notre Dame
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.17034">
         Shaping the Safety Boundaries: Understanding and Defending Against Jailbreaks in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Activation Boundary Defense (ABD)
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        The Pennsylvania State University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.16682">
         The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt Injection in LLM Agents
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Indirect Prompt Injection
        </strong>
        &amp;
        <strong>
         Task Alignment
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        OpenAI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.16339">
         Deliberative Alignment: Reasoning Enables Safer Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Alignment
        </strong>
        &amp;
        <strong>
         Reasoning-Based Training
        </strong>
        &amp;
        <strong>
         Chain-of-Thought
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        MMLab, The Chinese University of Hong Kong
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.18826">
         RapGuard: Safeguarding Multimodal Large Language Models via Rationale-aware Defensive Prompting
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal Large Language Models
        </strong>
        &amp;
        <strong>
         Defensive Prompting
        </strong>
        &amp;
        <strong>
         Safety Mechanisms
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        National Taiwan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.19512">
         Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Alignment
        </strong>
        &amp;
        <strong>
         Model Merging
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        MBZUAI, Huazhong University of Science and Technology, University of Notre Dame
       </td>
       <td style="text-align: center;">
        ACL 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.17034">
         Shaping the Safety Boundaries: Understanding and Defending Against Jailbreaks in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak
        </strong>
        &amp;
        <strong>
         LLMs
        </strong>
        &amp;
        <strong>
         Safety Boundary
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Lakera
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.07927">
         Gandalf the Red: Adaptive Security for LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adaptive Security
        </strong>
        &amp;
        <strong>
         Prompt Attacks
        </strong>
        &amp;
        <strong>
         Red-Teaming
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        IIIT Hyderabad
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.06208">
         Enhancing AI Safety Through the Fusion of Low-Rank Adapters
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Low-Rank Adapters
        </strong>
        &amp;
        <strong>
         Safety Alignment
        </strong>
        &amp;
        <strong>
         Jailbreak Mitigation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Xidian University
       </td>
       <td style="text-align: center;">
        AAAI 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.03272">
         Backdoor Token Unlearning: Exposing and Defending Backdoors in Pretrained Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Defense
        </strong>
        &amp;
        <strong>
         Token Unlearning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Case Western Reserve University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.04323">
         Navigating the Designs of Privacy-Preserving Fine-tuning for Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Privacy-Preserving Fine-tuning
        </strong>
        &amp;
        <strong>
         Data Reconstruction Attack Defense
        </strong>
        &amp;
        <strong>
         GuardedTuning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        North Carolina State University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.02629">
         Layer-Level Self-Exposure and Patch: Affirmative Token Mitigation for Jailbreak Attack Defense
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Layer-Specific Editing
        </strong>
        &amp;
        <strong>
         Machine Unlearning
        </strong>
        &amp;
        <strong>
         Jailbreak Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Xi‚Äôan Jiaotong University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.02029">
         Spot Risks Before Speaking! Unraveling Safety Attention Heads in Large Vision-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Safety Mechanisms
        </strong>
        &amp;
        <strong>
         Attention Heads
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        New York University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.02018">
         Safeguarding Large Language Models in Real-time with Tunable Safety-Performance Trade-offs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Large Language Model Safeguards
        </strong>
        &amp;
        <strong>
         Controlled Text Generation
        </strong>
        &amp;
        <strong>
         Jailbreak Mitigation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        East China Normal University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.10639">
         Latent-space adversarial training with post-aware calibration for defending large language models against jailbreak attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Adversarial Training
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Capital One
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.13080">
         Refining Input Guardrails: Enhancing LLM-as-a-Judge Efficiency Through Chain-of-Thought Fine-Tuning and Alignment
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Input Guardrails
        </strong>
        &amp;
        <strong>
         Chain-of-Thought Fine-Tuning
        </strong>
        &amp;
        <strong>
         LLM Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Xidian University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.13677">
         HumorReject: Decoupling LLM Safety from Refusal Prefix via A Little Humor
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Safety
        </strong>
        &amp;
        <strong>
         Prefix Injection Defense
        </strong>
        &amp;
        <strong>
         Humor-based Alignment
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Hainan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.16029">
         FDLLM: A Text Fingerprint Detection Method for LLMs in Multi-Language, Multi-Domain Black-Box Environments
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Fingerprinting
        </strong>
        &amp;
        <strong>
         Black-Box Detection
        </strong>
        &amp;
        <strong>
         Multi-Language AI Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        University of Wisconsin-Madison
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.16534">
         Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Alignment
        </strong>
        &amp;
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Safety Classifiers
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        AIShield
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.17030v1">
         CHALLENGES IN ENSURING AI SAFETY IN DEEPSEEK-R1 MODELS: THE SHORTCOMINGS OF REINFORCEMENT LEARNING STRATEGIES
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         AI Safety
        </strong>
        &amp;
        <strong>
         Supervised Fine-Tuning
        </strong>
        &amp;
        <strong>
         Harmlessness Reduction
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Mondragon University, University of Seville
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.17132v1">
         ASTRAL: Automated Safety Testing of Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Safety Testing
        </strong>
        &amp;
        <strong>
         Automated Evaluation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        UK AI Safety Institute, Redwood Research
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.17315v1">
         A sketch of an AI control safety case
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Case
        </strong>
        &amp;
        <strong>
         Data Exfiltration
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Tsinghua University, Shenzhen Campus of Sun Yat-sen University, Didichuxing, Nanyang Technological University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.18100v1">
         Panacea: Mitigating Harmful Fine-tuning for Large Language Models via Post-fine-tuning Perturbation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Harmful Fine-tuning Attack
        </strong>
        &amp;
        <strong>
         Defensive Alignment
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        National University of Singapore, University of Chinese Academy of Sciences, Peking University, Westlake University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.18492v1">
         GuardReasoner: Towards Reasoning-based LLM Safeguards
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Safeguards
        </strong>
        &amp;
        <strong>
         Reasoning-based Moderation
        </strong>
        &amp;
        <strong>
         Direct Preference Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Anthropic
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.18837">
         Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Universal Jailbreaks
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
        &amp;
        <strong>
         Classifier-based Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        National University of Singapore
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.19180">
         Enhancing Model Defense Against Jailbreaks with Proactive Safety Reasoning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Defense
        </strong>
        &amp;
        <strong>
         Safety Chain-of-Thought
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Aligned AI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.00580">
         Defense Against the Dark Prompts: Mitigating Best-of-N Jailbreaking with Prompt Evaluation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking Defense
        </strong>
        &amp;
        <strong>
         Prompt Evaluation
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        The Pennsylvania State University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.00653">
         Towards Robust Multimodal Large Language Models Against Jailbreak Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal LLMs
        </strong>
        &amp;
        <strong>
         Jailbreak Defense
        </strong>
        &amp;
        <strong>
         Adversarial Training
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Purdue University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.00657">
         LLM Safety Alignment is Divergence Estimation in Disguise
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Alignment
        </strong>
        &amp;
        <strong>
         Divergence Estimation
        </strong>
        &amp;
        <strong>
         Safety Training
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        University of Oxford
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.00757">
         AgentBreeder: Mitigating the AI Safety Impact of Multi-Agent Scaffolds
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multi-Agent Systems
        </strong>
        &amp;
        <strong>
         LLM Safety
        </strong>
        &amp;
        <strong>
         Evolutionary Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Huawei Noah‚Äôs Ark Lab
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.01208">
         Almost Surely Safe Alignment of Large Language Models at Inference-Time
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Safety Alignment
        </strong>
        &amp;
        <strong>
         Inference-Time Optimization
        </strong>
        &amp;
        <strong>
         Constrained MDPs
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Tsinghua University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.02384">
         STAIR: Improving Safety Alignment with Introspective Reasoning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Safety Alignment
        </strong>
        &amp;
        <strong>
         Introspective Reasoning
        </strong>
        &amp;
        <strong>
         Monte Carlo Tree Search
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        The Hong Kong Polytechnic University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.03964">
         "It Warned Me Just at the Right Moment": Exploring LLM-based Real-time Detection of Phone Scams
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Phone Scam Detection
        </strong>
        &amp;
        <strong>
         LLM-based Security
        </strong>
        &amp;
        <strong>
         Real-time Intervention
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Tsinghua University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.04040">
         Leveraging Reasoning with Guidelines to Elicit and Utilize Knowledge for Enhancing Safety Alignment
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Safety Alignment
        </strong>
        &amp;
        <strong>
         Reasoning-based Training
        </strong>
        &amp;
        <strong>
         Out-of-Distribution Generalization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        King Abdullah University of Science and Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.04204">
         "Short-length" Adversarial Training Helps LLMs Defend "Long-length" Jailbreak Attacks: Theoretical and Empirical Evidence
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Training
        </strong>
        &amp;
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        University of California, Los Angeles
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.05163">
         DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multilingual LLM Safety
        </strong>
        &amp;
        <strong>
         Reinforcement Learning
        </strong>
        &amp;
        <strong>
         Guardrail Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        University of California, Santa Barbara
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.05174">
         MELON: Indirect Prompt Injection Defense via Masked Re-execution and Tool Comparison
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Indirect Prompt Injection Defense
        </strong>
        &amp;
        <strong>
         Large Language Models (LLMs)
        </strong>
        &amp;
        <strong>
         Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        University of California, Irvine
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.08142">
         Bridging the Safety Gap: A Guardrail Pipeline for Trustworthy LLM Inferences
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Safety
        </strong>
        &amp;
        <strong>
         Guardrail Pipeline
        </strong>
        &amp;
        <strong>
         Hallucination Detection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Beijing University of Posts and Telecommunications
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.08657">
         Refining Positive and Toxic Samples for Dual Safety Self-Alignment of LLMs with Minimal Human Interventions
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Alignment
        </strong>
        &amp;
        <strong>
         Self-Supervised Learning
        </strong>
        &amp;
        <strong>
         LLM Robustness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Carnegie Mellon University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.08966">
         RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection
        </strong>
        &amp;
        <strong>
         Privacy Leakage
        </strong>
        &amp;
        <strong>
         Tool-Based Agent Systems
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Georgia Institute of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.09809">
         AGENTGUARD: Repurposing Agentic Orchestrator for Safety Evaluation of Tool Orchestration
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Agents
        </strong>
        &amp;
        <strong>
         Tool Orchestration
        </strong>
        &amp;
        <strong>
         AI Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Shanghai Jiao Tong University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.09990">
         X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from Multi-Turn Jailbreaks without Compromising Usability
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking Defense
        </strong>
        &amp;
        <strong>
         Multi-Turn Attacks
        </strong>
        &amp;
        <strong>
         LLM Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        KuaiShou
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.10391">
         MM-RLHF: The Next Step Forward in Multimodal LLM Alignment
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal RLHF
        </strong>
        &amp;
        <strong>
         LLM Alignment
        </strong>
        &amp;
        <strong>
         Reward Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Indian Institute of Technology Kharagpur
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.11244">
         Soteria: Language-Specific Functional Parameter Steering for Multilingual Safety Alignment
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multilingual Safety
        </strong>
        &amp;
        <strong>
         Functional Parameter Steering
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        The Ohio State University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.11448">
         AGrail: A Lifelong Agent Guardrail with Effective and Adaptive Safety Detection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Agent Safety
        </strong>
        &amp;
        <strong>
         Risk Detection
        </strong>
        &amp;
        <strong>
         Adaptive Guardrails
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Alibaba Group
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.11555">
         Equilibrate RLHF: Towards Balancing Helpfulness-Safety Trade-off in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         RLHF
        </strong>
        &amp;
        <strong>
         Helpfulness-Safety Trade-off
        </strong>
        &amp;
        <strong>
         Large Language Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        ShanghaiTech University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.11647">
         DELMAN: Dynamic Defense Against Large Language Model Jailbreaking with Model Editing
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Defense
        </strong>
        &amp;
        <strong>
         Model Editing
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        KAIST
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.12464">
         SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Safety
        </strong>
        &amp;
        <strong>
         Model Selection
        </strong>
        &amp;
        <strong>
         Guardrails
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        GovTech Singapore
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.12485">
         Safe at the Margins: A General Approach to Safety Alignment in Low-Resource English Languages ‚Äì A Singlish Case Study
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Alignment
        </strong>
        &amp;
        <strong>
         Low-Resource Languages
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Beihang University, Baidu Inc.
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.12970">
         Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking Defense
        </strong>
        &amp;
        <strong>
         LLM Safety
        </strong>
        &amp;
        <strong>
         Reasoning-based Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        The Pennsylvania State University
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.13095">
         Understanding and Rectifying Safety Perception Distortion in VLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Safety Alignment
        </strong>
        &amp;
        <strong>
         Activation Shift
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Rochester Institute of Technology
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.13141">
         UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor Attacks and Adversarial Attacks in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection
        </strong>
        &amp;
        <strong>
         Backdoor Attacks
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Institute of Automation, Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.13162">
         ShieldLearner: A New Paradigm for Jailbreak Attack Defense in LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Adversarial Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        The Hong Kong Polytechnic University
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.13946">
         Why Safeguarded Ships Run Aground? Aligned Large Language Models‚Äô Safety Mechanisms Tend to Be Anchored in The Template Region
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Alignment
        </strong>
        &amp;
        <strong>
         Jailbreaking
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Fudan University
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.14486">
         How Jailbreak Defenses Work and Ensemble? A Mechanistic Investigation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Defenses
        </strong>
        &amp;
        <strong>
         Large Vision-Language Models
        </strong>
        &amp;
        <strong>
         Ensemble Strategies
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        The Chinese University of Hong Kong
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.14744">
         HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Detection
        </strong>
        &amp;
        <strong>
         Large Vision-Language Models
        </strong>
        &amp;
        <strong>
         Hidden States Monitoring
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Wuhan University
       </td>
       <td style="text-align: center;">
        USENIX Security 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.07557">
         JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept Analysis and Manipulation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Defense
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
        &amp;
        <strong>
         Concept Analysis
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        North South University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.16750">
         Guardians of the Agentic System: Preventing Many-Shots Jailbreak with Agentic System
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         AI Agents
        </strong>
        &amp;
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Warsaw University of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.16174">
         MAYBE I SHOULD NOT ANSWER THAT, BUT... DO LLMs UNDERSTAND THE SAFETY OF THEIR INPUTS?
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Safety
        </strong>
        &amp;
        <strong>
         Prompt Classification
        </strong>
        &amp;
        <strong>
         Adversarial Detection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Sichuan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.19041">
         Beyond Surface-Level Patterns: An Essence-Driven Defense Framework Against Jailbreak Attacks in LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
        &amp;
        <strong>
         Defense Framework
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Samsung Electronics
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.16691">
         Toward Responsible Federated Large Language Models: Leveraging a Safety Filter and Constitutional AI
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Federated Learning
        </strong>
        &amp;
        <strong>
         LLM Safety
        </strong>
        &amp;
        <strong>
         Constitutional AI
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Mila, Universit√© de Montr√©al
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.16366">
         A Generative Approach to LLM Harmfulness Detection with Special Red Flag Tokens
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Harmfulness Detection
        </strong>
        &amp;
        <strong>
         Safety Fine-Tuning
        </strong>
        &amp;
        <strong>
         Red Flag Token
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        LMU Munich
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.15836">
         Soft Token Attacks Cannot Reliably Audit Unlearning in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Unlearning in LLMs
        </strong>
        &amp;
        <strong>
         Soft Token Attacks
        </strong>
        &amp;
        <strong>
         Model Auditing
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        PRISM Eval
       </td>
       <td style="text-align: center;">
        AAAI 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.19145">
         Multi-Agent Security Tax: Trading Off Security and Collaboration Capabilities in Multi-Agent Systems
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multi-Agent Systems
        </strong>
        &amp;
        <strong>
         Security Trade-Offs
        </strong>
        &amp;
        <strong>
         Jailbreak Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        NEC Laboratories America
       </td>
       <td style="text-align: center;">
        GenAI4Health Workshop AAAI 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.15040">
         Reducing Hallucinations of Medical Multimodal Large Language Models with Visual Retrieval-Augmented Generation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Reduction
        </strong>
        &amp;
        <strong>
         Medical Multimodal LLMs
        </strong>
        &amp;
        <strong>
         Visual Retrieval-Augmented Generation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        √âcole Polytechnique F√©d√©rale de Lausanne
       </td>
       <td style="text-align: center;">
        TMLR 2025
       </td>
       <td style="text-align: center;">
        <a href="https://openreview.net/forum?id=42v6I5Ut9a">
         Single-pass Detection of Jailbreaking Input in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Detection
        </strong>
        &amp;
        <strong>
         Single-pass Defense
        </strong>
        &amp;
        <strong>
         Logit-based Classification
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Harbin Institute of Technology, Shenzhen, Baidu Inc.
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.20757">
         The Rise of Darkness: Safety-Utility Trade-Offs in Role-Playing Dialogue Agents
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Role-Playing Dialogue Agents
        </strong>
        &amp;
        <strong>
         Safety-Utility Trade-Off
        </strong>
        &amp;
        <strong>
         Adaptive Preference Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Harbin Institute of Technology &amp; Singapore Management University
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.20968">
         Beware of Your Po! Measuring and Mitigating AI Safety Risks in Role-Play Fine-Tuning of LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Role-Playing LLMs
        </strong>
        &amp;
        <strong>
         AI Safety
        </strong>
        &amp;
        <strong>
         Fine-Tuning Risks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Singapore Management University
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.00037">
         Zero-Shot Defense Against Toxic Images via Inherent Multimodal Alignment in LVLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Toxic Image Detection
        </strong>
        &amp;
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Multimodal Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        ShanghaiTech University, Quantstamp
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.00416">
         Breaking the Loop: Detecting and Mitigating Denial-of-Service Vulnerabilities in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Denial-of-Service
        </strong>
        &amp;
        <strong>
         Large Language Models
        </strong>
        &amp;
        <strong>
         Recurrent Generation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Georgia Institute of Technology
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.00555">
         Safety Tax: Safety Alignment Makes Your Large Reasoning Models Less Reasonable
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Large Reasoning Models
        </strong>
        &amp;
        <strong>
         Safety Alignment
        </strong>
        &amp;
        <strong>
         Reasoning Trade-off
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Appier AI Research, National Taiwan University
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.01332">
         Answer, Refuse, or Guess? Investigating Risk-Aware Decision Making in Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Risk-Aware Decision Making
        </strong>
        &amp;
        <strong>
         Language Models
        </strong>
        &amp;
        <strong>
         Uncertainty Calibration
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Technical University of Munich, Mila, Universit√© de Montr√©al
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.02574">
         LLM-Safety Evaluations Lack Robustness
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Safety Evaluation
        </strong>
        &amp;
        <strong>
         Robustness
        </strong>
        &amp;
        <strong>
         Bias Mitigation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Peking University
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.03480">
         SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision-Language-Action Models
        </strong>
        &amp;
        <strong>
         Safe Reinforcement Learning
        </strong>
        &amp;
        <strong>
         Robot Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        University of California, Berkeley
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.03710">
         Improving LLM Safety Alignment with Dual-Objective Optimization
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Safety Alignment
        </strong>
        &amp;
        <strong>
         Dual-Objective Optimization
        </strong>
        &amp;
        <strong>
         Jailbreak Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        University of Science and Technology of China, Nanyang Technological University
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.04392">
         AgentSafe: Safeguarding Large Language Model-based Multi-agent Systems via Hierarchical Data Management
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multi-agent Systems Security
        </strong>
        &amp;
        <strong>
         Hierarchical Data Management
        </strong>
        &amp;
        <strong>
         Memory Protection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Nanjing University of Science and Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.04833">
         Adversarial Training for Multimodal Large Language Models against Jailbreak Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal Large Language Models
        </strong>
        &amp;
        <strong>
         Adversarial Training
        </strong>
        &amp;
        <strong>
         Jailbreak Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Carnegie Mellon University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.05021">
         Safety is Not Only About Refusal: Reasoning-Enhanced Fine-tuning for Interpretable LLM Safety
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Safety
        </strong>
        &amp;
        <strong>
         Reasoning Fine-Tuning
        </strong>
        &amp;
        <strong>
         Jailbreak Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Google
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.08919">
         BSAFE: (B)acktracking for (SAFE)ty
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Safety
        </strong>
        &amp;
        <strong>
         Backtracking Alignment
        </strong>
        &amp;
        <strong>
         Jailbreak Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        National University of Singapore
       </td>
       <td style="text-align: center;">
        S&amp;P 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.09022">
         Prompt Inversion Attack against Collaborative Inference of Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Inversion
        </strong>
        &amp;
        <strong>
         Collaborative Inference
        </strong>
        &amp;
        <strong>
         LLM Privacy Attack
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Institute of Information Engineering, Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.11185">
         Align in Depth: Defending Jailbreak Attacks via Progressive Answer Detoxification
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Defense
        </strong>
        &amp;
        <strong>
         Safety Fine-tuning
        </strong>
        &amp;
        <strong>
         LLM Robustness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        XCALLY
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.11517">
         Prompt Injection Detection and Mitigation via AI Multi-Agent NLP Frameworks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection
        </strong>
        &amp;
        <strong>
         Multi-Agent Systems
        </strong>
        &amp;
        <strong>
         AI Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Michigan State University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.11832">
         Safety Mirage: How Spurious Correlations Undermine VLM Safety Fine-tuning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Safety Fine-tuning
        </strong>
        &amp;
        <strong>
         Spurious Correlation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Beijing University of Posts and Telecommunications
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.12931">
         MirrorGuard: Adaptive Defense Against Jailbreaks via Entropy-Guided Mirror Crafting
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Defense
        </strong>
        &amp;
        <strong>
         Entropy Analysis
        </strong>
        &amp;
        <strong>
         Prompt Calibration
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        The Hong Kong Polytechnic University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.14189">
         Towards Harmless Multimodal Assistants with Blind Preference Optimization
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal LLMs
        </strong>
        &amp;
        <strong>
         Safety Alignment
        </strong>
        &amp;
        <strong>
         Preference Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        University of Modena and Reggio Emilia
       </td>
       <td style="text-align: center;">
        CVPR 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.12127">
         Hyperbolic Safety-Aware Vision-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety-Aware Retrieval
        </strong>
        &amp;
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Hyperbolic Embedding
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        OpenAI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.16431">
         OpenAI‚Äôs Approach to External Red Teaming for AI Models and Systems
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Red Teaming
        </strong>
        &amp;
        <strong>
         AI Risk Assessment
        </strong>
        &amp;
        <strong>
         Safety Evaluation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Unicom Data Intelligence, China Unicom
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.16529">
         Safety Evaluation and Enhancement of DeepSeek Models in Chinese Contexts
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Evaluation
        </strong>
        &amp;
        <strong>
         Distillation
        </strong>
        &amp;
        <strong>
         Chinese LLMs
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Zhejiang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.16851">
         Towards LLM Guardrails via Sparse Representation Steering
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Representation Engineering
        </strong>
        &amp;
        <strong>
         Sparse Autoencoder
        </strong>
        &amp;
        <strong>
         LLM Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Technical University of Munich, Ludwig Maximilian University of Munich
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.17882">
         THINK BEFORE REFUSAL: Triggering Safety Reflection in LLMs to Mitigate False Refusal Behavior
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         False Refusal
        </strong>
        &amp;
        <strong>
         Safety Alignment
        </strong>
        &amp;
        <strong>
         LLM Reasoning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        The Hong Kong University of Science and Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.17932">
         STShield: Single-Token Sentinel for Real-Time Jailbreak Detection in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Detection
        </strong>
        &amp;
        <strong>
         LLM Safety
        </strong>
        &amp;
        <strong>
         Adversarial Training
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Zhejiang University, Ant Group
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.19041">
         LookAhead Tuning: Safer Language Models via Partial Answer Previews
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safe Fine-tuning
        </strong>
        &amp;
        <strong>
         Answer Prefix
        </strong>
        &amp;
        <strong>
         LLM Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        IBM Research, Rensselaer Polytechnic Institute
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.20807">
         Fundamental Safety-Capability Trade-offs in Fine-tuning Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety-Capability Trade-off
        </strong>
        &amp;
        <strong>
         Fine-tuning Strategy
        </strong>
        &amp;
        <strong>
         Theoretical Analysis
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Technical University Munich, IBM Research
       </td>
       <td style="text-align: center;">
        ICLR 2025 Workshop on BTLMA
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.17239">
         SAFEMERGE: Preserving Safety Alignment in Fine-Tuned Large Language Models via Selective Layer-Wise Model Merging
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Alignment
        </strong>
        &amp;
        <strong>
         Model Merging
        </strong>
        &amp;
        <strong>
         Post-Fine-Tuning Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        HydroX AI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.21819">
         Optimizing Safe and Aligned Language Generation: A Multi-Objective GRPO Approach
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Language Model Alignment
        </strong>
        &amp;
        <strong>
         Multi-Objective Optimization
        </strong>
        &amp;
        <strong>
         Group Relative Policy Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        University of Science and Technology of China
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.01533">
         LightDefense: A Lightweight Uncertainty-Driven Defense against Jailbreaks via Shifted Token Distribution
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Safety
        </strong>
        &amp;
        <strong>
         Jailbreak Defense
        </strong>
        &amp;
        <strong>
         Token Distribution Adjustment
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Seoul National University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.01550">
         Representation Bending for Large Language Model Safety
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Safety
        </strong>
        &amp;
        <strong>
         Representation Engineering
        </strong>
        &amp;
        <strong>
         Adversarial Robustness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        LG Toronto AI Research Lab
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.02708">
         The Hidden Space of Safety: Understanding Preference-Tuned LLMs in Multilingual Context
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multilingual Alignment
        </strong>
        &amp;
        <strong>
         LLM Safety
        </strong>
        &amp;
        <strong>
         Representation Analysis
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Zhejiang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.02725">
         ERPO: Advancing Safety Alignment via Ex-Ante Reasoning Preference Optimization
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Safety Alignment
        </strong>
        &amp;
        <strong>
         Ex-Ante Reasoning
        </strong>
        &amp;
        <strong>
         Preference Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Carleton University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.03040">
         Safety Modulation: Enhancing Safety in Reinforcement Learning through Cost-Modulated Rewards
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safe Reinforcement Learning
        </strong>
        &amp;
        <strong>
         Constrained MDP
        </strong>
        &amp;
        <strong>
         Reward Modulation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        VMware Research
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.06577">
         Bypassing Safety Guardrails in LLMs Using Humor
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Jailbreaking
        </strong>
        &amp;
        <strong>
         Prompt Engineering
        </strong>
        &amp;
        <strong>
         Humor Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        University of Warwick
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.03726">
         Detecting Malicious AI Agents Through Simulated Interactions
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Malicious intent detection
        </strong>
        &amp;
        <strong>
         Human-AI interaction
        </strong>
        &amp;
        <strong>
         Manipulation techniques
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.07135">
         SINCon: Mitigate LLM-Generated Malicious Message Injection Attack for Rumor Detection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Rumor Detection
        </strong>
        &amp;
        <strong>
         Message Injection Attack
        </strong>
        &amp;
        <strong>
         Contrastive Learning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        University of South Carolina
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.07995">
         SafeChat: A Framework for Building Trustworthy Collaborative Assistants and a Case Study of its Usefulness
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Collaborative Assistant
        </strong>
        &amp;
        <strong>
         Trustworthy Chatbot
        </strong>
        &amp;
        <strong>
         SafeChat Framework
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Carnegie Mellon University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.08192">
         SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Machine Unlearning
        </strong>
        &amp;
        <strong>
         Sparse Autoencoder
        </strong>
        &amp;
        <strong>
         LLM Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Harbin Institute of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.09466">
         AdaSteer: Your Aligned LLM is Inherently an Adaptive Jailbreak Defender
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Defense
        </strong>
        &amp;
        <strong>
         Activation Steering
        </strong>
        &amp;
        <strong>
         Adaptive Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        City University of Hong Kong
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.09593">
         ControlNet: A Firewall for RAG-based LLM System
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         RAG Security
        </strong>
        &amp;
        <strong>
         AI Firewall
        </strong>
        &amp;
        <strong>
         Activation Shift
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Independent Researchers
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.09604">
         Mitigating Many-Shot Jailbreaking
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Many-Shot Jailbreaking
        </strong>
        &amp;
        <strong>
         Fine-Tuning Defense
        </strong>
        &amp;
        <strong>
         Input Sanitization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Georgia Tech
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.09712">
         The Structural Safety Generalization Problem
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Generalization
        </strong>
        &amp;
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Guardrails
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        University of Utah
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.09757">
         Alleviating the Fear of Losing Alignment in LLM Fine-tuning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Alignment
        </strong>
        &amp;
        <strong>
         Fine-tuning Recovery
        </strong>
        &amp;
        <strong>
         Harmful Direction Restoration
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        UC Berkeley
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.11703">
         Progent: Programmable Privilege Control for LLM Agents
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Agents
        </strong>
        &amp;
        <strong>
         Privilege Control
        </strong>
        &amp;
        <strong>
         Policy Programming
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Microsoft
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.12321">
         AttentionDefense: Leveraging System Prompt Attention for Explainable Defense Against Novel Jailbreaks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Detection
        </strong>
        &amp;
        <strong>
         Explainable AI
        </strong>
        &amp;
        <strong>
         System Prompt Attention
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Shanghai Jiao Tong University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.12661">
         VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven Prompt Optimization
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision-Language Model
        </strong>
        &amp;
        <strong>
         Safety Alignment
        </strong>
        &amp;
        <strong>
         Multimodal Reasoning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Sporo Health
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.12757">
         MCP Guardian: A Security-First Layer for Safeguarding MCP-Based AI System
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Model Context Protocol
        </strong>
        &amp;
        <strong>
         Agentic AI
        </strong>
        &amp;
        <strong>
         AI Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        University of Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        CVPR 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.13052">
         Do We Really Need Curated Malicious Data for Safety Alignment in Multi-modal Large Language Models?
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         MLLM Safety
        </strong>
        &amp;
        <strong>
         Rejection Tuning
        </strong>
        &amp;
        <strong>
         Compliance Bias
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Fudan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.13201">
         Concept Enhancement Engineering: A Lightweight and Efficient Robust Defense Against Jailbreak Attacks in Embodied AI
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Embodied Intelligence
        </strong>
        &amp;
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Representation Engineering
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Tongji University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.13562">
         DETAM: Defending LLMs Against Jailbreak Attacks via Targeted Attention Modification
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Jailbreak Defense
        </strong>
        &amp;
        <strong>
         Attention Modification
        </strong>
        &amp;
        <strong>
         Inference-Time Strategy
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Quebec University at Chicoutimi
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.16120">
         A DATA-CENTRIC APPROACH FOR SAFE AND SECURE LARGE LANGUAGE MODELS AGAINST THREATENING AND TOXIC CONTENT
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Toxicity Mitigation
        </strong>
        &amp;
        <strong>
         Post-generation Correction
        </strong>
        &amp;
        <strong>
         LLM Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Intuit
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.16902">
         Building A Secure Agentic AI Application Leveraging Google‚Äôs A2A Protocol
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Agentic AI
        </strong>
        &amp;
        <strong>
         A2A Protocol
        </strong>
        &amp;
        <strong>
         MAESTRO Threat Modeling
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Zhejiang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.18564">
         DUALBREACH: Efficient Dual-Jailbreaking via Target-Driven Initialization and Multi-Target Optimization
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Guardrails
        </strong>
        &amp;
        <strong>
         Adversarial Attack
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Microsoft India
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.19674">
         SAGE: A Generic Framework for LLM Safety Evaluation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Safety Evaluation
        </strong>
        &amp;
        <strong>
         Adversarial Testing
        </strong>
        &amp;
        <strong>
         Multi-turn Dialogue
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Chongqing University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.19730">
         Evaluate-and-Purify: Fortifying Code Language Models Against Adversarial Attacks Using LLM-as-a-Judge
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Attack
        </strong>
        &amp;
        <strong>
         Code Language Model
        </strong>
        &amp;
        <strong>
         LLM-as-a-Judge
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        National University of Singapore
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.20472">
         Robustness via Referencing: Defending against Prompt Injection Attacks by Referencing the Executed Instruction
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection
        </strong>
        &amp;
        <strong>
         Instruction Referencing
        </strong>
        &amp;
        <strong>
         LLM Robustness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Beijing Institute of Technology
       </td>
       <td style="text-align: center;">
        ACM MM‚Äô25
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.21044">
         AGATE: Stealthy Black-box Watermarking for Multimodal Model Copyright Protection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Black-box Watermarking
        </strong>
        &amp;
        <strong>
         Model Copyright Protection
        </strong>
        &amp;
        <strong>
         Watermarking Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        University of Cincinnati
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.00010">
         Jailbreak Detection in Clinical Training LLMs Using Feature-Based Predictive Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Detection
        </strong>
        &amp;
        <strong>
         Clinical Simulation
        </strong>
        &amp;
        <strong>
         Fuzzy Decision Tree
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        University of California, Santa Cruz
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.04843">
         Large Language Models are Autonomous Cyber Defenders
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Autonomous Cyber Defense
        </strong>
        &amp;
        <strong>
         LLM Agents
        </strong>
        &amp;
        <strong>
         Multi-Agent Systems
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Nanjing University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.06311v1">
         Defending against Indirect Prompt Injection by Instruction Detection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Indirect Prompt Injection
        </strong>
        &amp;
        <strong>
         Instruction Detection
        </strong>
        &amp;
        <strong>
         Behavioral State Analysis
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        University of Cambridge
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.09602v1">
         Adversarial Suffix Filtering: a Defense Pipeline for LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Suffix
        </strong>
        &amp;
        <strong>
         Jailbreak Defense
        </strong>
        &amp;
        <strong>
         Model-Agnostic Filtering
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Saarland University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.11459">
         ProxyPrompt: Securing System Prompts against Prompt Extraction Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Extraction
        </strong>
        &amp;
        <strong>
         System Prompt Protection
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Fudan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.11926">
         SAFEVID: Toward Safety Aligned Video Large Multimodal Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Video Multimodal Models
        </strong>
        &amp;
        <strong>
         Safety Alignment
        </strong>
        &amp;
        <strong>
         Preference Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        George Mason University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.12655">
         Web IP at Risk: Prevent Unauthorized Real-Time Retrieval by Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Web IP Protection
        </strong>
        &amp;
        <strong>
         LLM Anti-Retrieval
        </strong>
        &amp;
        <strong>
         Semantic Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        ARIMLABS.AI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.13076">
         The Hidden Dangers of Browsing AI Agents
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Browsing Agents
        </strong>
        &amp;
        <strong>
         Prompt Injection
        </strong>
        &amp;
        <strong>
         Multi-Layer Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Institute of Information Engineering, Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.13506">
         EcoSafeRAG: Efficient Security through Context Analysis in Retrieval-Augmented Generation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         RAG Security
        </strong>
        &amp;
        <strong>
         Contextual Diversity Detection
        </strong>
        &amp;
        <strong>
         Corpus Poisoning Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Beijing Institute of AI Safety and Governance
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.13862">
         PANDAGUARD: Systematic Evaluation of LLM Safety against Jailbreaking Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Evaluation
        </strong>
        &amp;
        <strong>
         Multi-Agent System
        </strong>
        &amp;
        <strong>
         LLM Safety Benchmark
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        University of Oxford
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.14300">
         SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Harmful Output Detection
        </strong>
        &amp;
        <strong>
         Backdoor Behavior
        </strong>
        &amp;
        <strong>
         Unsupervised Monitoring
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        HKUST
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.14590">
         MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         MCP
        </strong>
        &amp;
        <strong>
         Contextual Integrity
        </strong>
        &amp;
        <strong>
         Safety Benchmark
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Yonsei University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.14667">
         SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Alignment
        </strong>
        &amp;
        <strong>
         Chain-of-Thought Reasoning
        </strong>
        &amp;
        <strong>
         Early Intervention
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Tsinghua University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.15404v1">
         How Should We Enhance the Safety of Large Reasoning Models: An Empirical Study
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Large Reasoning Models
        </strong>
        &amp;
        <strong>
         Safety Fine-Tuning
        </strong>
        &amp;
        <strong>
         Chain-of-Thought
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Peking University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.15710v1">
         Advancing LLM Safe Alignment with Safety Representation Ranking
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Alignment
        </strong>
        &amp;
        <strong>
         Representation Learning
        </strong>
        &amp;
        <strong>
         Response Ranking
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Peking University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.15753v1">
         Scalable Defense against In-the-wild Jailbreaking Attacks with Safety Context Retrieval
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreaking Defense
        </strong>
        &amp;
        <strong>
         Safety Context Retrieval
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        UC Santa Cruz
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.16186v1">
         SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Alignment
        </strong>
        &amp;
        <strong>
         Jailbreak Defense
        </strong>
        &amp;
        <strong>
         Reasoning Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Nankai University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.16559v1">
         CTRAP: Embedding Collapse Trap to Safeguard Large Language Models from Harmful Fine-Tuning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Fine-tuning Security
        </strong>
        &amp;
        <strong>
         Collapse Trap
        </strong>
        &amp;
        <strong>
         LLM Alignment Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Peking University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.16737v1">
         Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Fine-tuning Risk
        </strong>
        &amp;
        <strong>
         Safety Optimization
        </strong>
        &amp;
        <strong>
         Gradient Probing
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Wuhan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.16916v1">
         Backdoor Cleaning without External Guidance in MLLM Fine-tuning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Defense
        </strong>
        &amp;
        <strong>
         Multimodal LLMs
        </strong>
        &amp;
        <strong>
         Attention Entropy
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        East China Normal University
       </td>
       <td style="text-align: center;">
        ACL 2025 Findings
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.16104v1">
         Hierarchical Safety Realignment: Lightweight Restoration of Safety in Pruned Large Vision-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Model Pruning
        </strong>
        &amp;
        <strong>
         LVLM Safety
        </strong>
        &amp;
        <strong>
         Neuron Restoration
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Nanjing University
       </td>
       <td style="text-align: center;">
        ACL 2025 Findings
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.12060v1">
         Why Not Act on What You Know? Unleashing Safety Potential of LLMs via Self-Aware Guard Enhancement
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Safety
        </strong>
        &amp;
        <strong>
         Jailbreak Defense
        </strong>
        &amp;
        <strong>
         Discrimination-Generation Gap
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Harbin Institute of Technology
       </td>
       <td style="text-align: center;">
        ACL 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.16869v1">
         MPO: Multilingual Safety Alignment via Reward Gap Optimization
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multilingual Safety
        </strong>
        &amp;
        <strong>
         Reward Gap Optimization
        </strong>
        &amp;
        <strong>
         LLM Alignment
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Southern University of Science and Technology
       </td>
       <td style="text-align: center;">
        ICML 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.12038v1">
         Safe Delta: Consistently Preserving Safety when Fine-Tuning LLMs on Diverse Datasets
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Fine-Tuning
        </strong>
        &amp;
        <strong>
         Delta Optimization
        </strong>
        &amp;
        <strong>
         LLM Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        North Carolina State University
       </td>
       <td style="text-align: center;">
        ICML 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.17072">
         Safety Alignment Can Be Not Superficial With Explicit Safety Signals
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Alignment
        </strong>
        &amp;
        <strong>
         LLMs
        </strong>
        &amp;
        <strong>
         Binaray Classification
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Impel
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.17066v1">
         Improving LLM Outputs Against Jailbreak Attacks with Expert Model Integration
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Expert Model Integration
        </strong>
        &amp;
        <strong>
         Prompt Injection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Beijing Institute of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.17106v1">
         RRTL: Red Teaming Reasoning Large Language Models in Tool Learning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Red Teaming
        </strong>
        &amp;
        <strong>
         Reasoning LLMs
        </strong>
        &amp;
        <strong>
         Tool Learning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Tsinghua University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.17650v1">
         Does Chain-of-Thought Reasoning Really Reduce Harmfulness from Jailbreaking?
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Chain-of-Thought Reasoning
        </strong>
        &amp;
        <strong>
         Jailbreaking
        </strong>
        &amp;
        <strong>
         Harmfulness Modeling
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Duke University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.18333v1">
         A Critical Evaluation of Defenses against Prompt Injection Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Injection
        </strong>
        &amp;
        <strong>
         Defense Evaluation
        </strong>
        &amp;
        <strong>
         Adaptive Attack
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Harbin Institute of Technology (Shenzhen)
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.18588">
         Safety Alignment via Constrained Knowledge Unlearning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety Alignment
        </strong>
        &amp;
        <strong>
         Knowledge Unlearning
        </strong>
        &amp;
        <strong>
         Jailbreak Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Beijing University of Posts and Telecommunications
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.18680">
         PD3F: A Pluggable and Dynamic DoS-Defense Framework Against Resource Consumption Attacks Targeting Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         DoS Defense
        </strong>
        &amp;
        <strong>
         Resource Consumption Attack
        </strong>
        &amp;
        <strong>
         Large Language Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        King Abdullah University of Science and Technology (KAUST)
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.19056v1">
         An Embarrassingly Simple Defense Against LLM Abliteration Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Abliteration Attack
        </strong>
        &amp;
        <strong>
         Extended Refusal
        </strong>
        &amp;
        <strong>
         LLM Alignment
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        King‚Äôs College London
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.19234v1">
         GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multi-Agent Collaboration
        </strong>
        &amp;
        <strong>
         Safety Detection
        </strong>
        &amp;
        <strong>
         Temporal Graph Modeling
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Sichuan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.19260">
         ALRPHFS: Adversarially Learned Risk Patterns with Hierarchical Fast &amp; Slow Reasoning for Robust Agent Defense
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Agent Defense
        </strong>
        &amp;
        <strong>
         Adversarial Learning
        </strong>
        &amp;
        <strong>
         Hierarchical Reasoning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        University of Maryland, College Park
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.19405">
         CoTGuard: Using Chain-of-Thought Triggering for Copyright Protection in Multi-Agent LLM Systems
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Copyright Protection
        </strong>
        &amp;
        <strong>
         Chain-of-Thought
        </strong>
        &amp;
        <strong>
         Multi-Agent LLM
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        NVIDIA
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.20087v1">
         Safety Through Reasoning: An Empirical Study of Reasoning Guardrail Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Reasoning Guardrails
        </strong>
        &amp;
        <strong>
         LLM Safety
        </strong>
        &amp;
        <strong>
         Data Efficiency
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Sea AI Lab
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.20259">
         Lifelong Safety Alignment for Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Lifelong Safety Alignment
        </strong>
        &amp;
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Meta-Attacker
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        University of Melbourne
       </td>
       <td style="text-align: center;">
        ICLR 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.20621">
         MULTI-LEVEL CERTIFIED DEFENSE AGAINST POISONING ATTACKS IN OFFLINE REINFORCEMENT LEARNING
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Certified Defense
        </strong>
        &amp;
        <strong>
         Poisoning Attack
        </strong>
        &amp;
        <strong>
         Offline Reinforcement Learning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        NLPR &amp; MAIS, Institute of Automation, Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.22271v1">
         Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Defense
        </strong>
        &amp;
        <strong>
         Test-Time Learning
        </strong>
        &amp;
        <strong>
         Multimodal LLM
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        SentinelAI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.22852v1">
         Operationalizing CaMeL: Strengthening LLM Defenses for Enterprise Deployment
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Defense
        </strong>
        &amp;
        <strong>
         Enterprise Security
        </strong>
        &amp;
        <strong>
         Prompt Injection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Beihang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.23015v1">
         Detecting Stealthy Backdoor Samples based on Intra-class Distance for Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Backdoor Detection
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
        &amp;
        <strong>
         TF-IDF Clustering
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Leidos
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.23634v1">
         MCP Safety Training: Learning to Refuse Falsely Benign MCP Exploits using Improved Preference Alignment
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         MCP Safety
        </strong>
        &amp;
        <strong>
         Falsely Benign Attack
        </strong>
        &amp;
        <strong>
         Preference Alignment
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Microsoft
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.23643v1">
         Securing AI Agents with Information-Flow Control
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Information-Flow Control
        </strong>
        &amp;
        <strong>
         Agent Security
        </strong>
        &amp;
        <strong>
         Prompt Injection Defense
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Institute of Information Engineering, Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        ACL 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.23020v1">
         AgentAlign: Navigating Safety Alignment in the Shift from Informative to Agentic Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Agent Alignment
        </strong>
        &amp;
        <strong>
         Safety Alignment
        </strong>
        &amp;
        <strong>
         Agentic LLM
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Harbin Institute of Technology, Shenzhen
       </td>
       <td style="text-align: center;">
        ACL 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.17147v1">
         MTSA: Multi-turn Safety Alignment for LLMs through Multi-round Red-teaming
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multi-turn Alignment
        </strong>
        &amp;
        <strong>
         Red-teaming
        </strong>
        &amp;
        <strong>
         LLM Safety
        </strong>
       </td>
      </tr>
     </tbody>
    </table>
    <h2>
     üíªPresentations &amp; Talk
    </h2>
    <h2>
     üìñTutorials &amp; Workshops
    </h2>
    <table>
     <thead>
      <tr>
       <th style="text-align: center;">
        Date
       </th>
       <th style="text-align: center;">
        Type
       </th>
       <th style="text-align: center;">
        Title
       </th>
       <th style="text-align: center;">
        URL
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td style="text-align: center;">
        23.10
       </td>
       <td style="text-align: center;">
        Tutorials
       </td>
       <td style="text-align: center;">
        Awesome-LLM-Safety
       </td>
       <td style="text-align: center;">
        <a href="https://github.com/ydyjya/Awesome-LLM-Safety">
         link
        </a>
       </td>
      </tr>
     </tbody>
    </table>
    <h2>
     üì∞News &amp; Articles
    </h2>
    <h2>
     üßë‚Äçüè´Scholars
    </h2>
   </div>
   <a class="back-to-home" href="../index_cn.html">
    <i class="fas fa-arrow-left">
    </i>
    ËøîÂõûÈ¶ñÈ°µ
   </a>
  </div>
  <footer>
   <div class="container">
    <div class="footer-content">
     <p>
      ‰ΩúËÄÖ:
      <a href="https://github.com/ydyjya">
       ydyjya
      </a>
     </p>
     <p>
      ËÅîÁ≥ªÊñπÂºè: zhouzhenhong@bupt.edu.cn
     </p>
    </div>
    <div class="footer-links">
     <a href="https://github.com/ydyjya/Awesome-LLM-Safety" target="_blank">
      <i class="fab fa-github">
      </i>
      GitHub ‰ªìÂ∫ì
     </a>
    </div>
   </div>
  </footer>
 </body>
</html>
