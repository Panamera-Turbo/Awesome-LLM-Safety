<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   Truthfulness&amp;Misinformation - Awesome LLM-Safety
  </title>
  <link href="../style.css" rel="stylesheet"/>
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet"/>
  <style>
   .markdown-content {
            padding: 20px;
        }
        .markdown-content h1 {
            font-size: 2rem;
            color: var(--primary-color);
            margin-bottom: 1.5rem;
            padding-bottom: 0.5rem;
            border-bottom: 2px solid var(--border-color);
        }
        .markdown-content h2 {
            font-size: 1.6rem;
            color: var(--secondary-color);
            margin-top: 2rem;
            margin-bottom: 1rem;
        }
        .markdown-content table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            font-size: 0.9rem;
        }
        .markdown-content th, .markdown-content td {
            padding: 12px 15px;
            border: 1px solid var(--border-color);
        }
        .markdown-content th {
            background-color: var(--primary-color);
            color: white;
            font-weight: bold;
            text-align: left;
        }
        .markdown-content tr:nth-child(even) {
            background-color: var(--light-bg);
        }
        .markdown-content tr:hover {
            background-color: #ddd;
        }
        .back-to-home {
            display: inline-block;
            margin: 20px 0;
            padding: 10px 15px;
            background-color: var(--primary-color);
            color: white;
            border-radius: 4px;
            text-decoration: none;
            font-weight: bold;
        }
        .back-to-home:hover {
            background-color: var(--secondary-color);
            color: white;
        }
  </style>
 </head>
 <body>
  <header>
   <div class="container">
    <h1>
     üõ°Ô∏è Awesome LLM-Safety üõ°Ô∏è
    </h1>
    <div class="language-switch">
     <a href="../index.html">
      English
     </a>
     |
     <a href="../index_cn.html">
      ‰∏≠Êñá
     </a>
    </div>
   </div>
  </header>
  <div class="container">
   <a class="back-to-home" href="../index.html">
    <i class="fas fa-arrow-left">
    </i>
    Back to Home
   </a>
   <div class="markdown-content">
    <h1>
     Truthfulness&amp;Misinformation
    </h1>
    <h2>
     Different from the main READMEüïµÔ∏è
    </h2>
    <ul>
     <li>
      Within this subtopic, we will be updating with the latest articles. This will help researchers in this area to quickly understand recent trends.
     </li>
     <li>
      In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
     </li>
     <li>
      Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"
     </li>
    </ul>
    <h2>
     üìëPapers
    </h2>
    <table>
     <thead>
      <tr>
       <th style="text-align: center;">
        Date
       </th>
       <th style="text-align: center;">
        Institute
       </th>
       <th style="text-align: center;">
        Publication
       </th>
       <th style="text-align: center;">
        Paper
       </th>
       <th style="text-align: center;">
        Keywords
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td style="text-align: center;">
        21.09
       </td>
       <td style="text-align: center;">
        University of Oxford
       </td>
       <td style="text-align: center;">
        ACL2022
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2109.07958">
         TruthfulQA: Measuring How Models Mimic Human Falsehoods
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Benchmark
        </strong>
        &amp;
        <strong>
         Truthfulness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.05
       </td>
       <td style="text-align: center;">
        KAIST
       </td>
       <td style="text-align: center;">
        NAACL2024(findings)
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2305.01579">
         Why So Gullible? Enhancing the Robustness of Retrieval-Augmented Models against Counterfactual Noise
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Retrieval-Augmented Models
        </strong>
        &amp;
        <strong>
         Counterfactual Noise
        </strong>
        &amp;
        <strong>
         Open-Domain Question Answering
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.07
       </td>
       <td style="text-align: center;">
        Microsoft Research Asia, Hong Kong University of Science and Technology, University of Science and Technology of China, Tsinghua University, Sony AI
       </td>
       <td style="text-align: center;">
        ResearchSquare
       </td>
       <td style="text-align: center;">
        <a href="https://www.researchsquare.com/article/rs-2873090/v1">
         Defending ChatGPT against Jailbreak Attack via Self-Reminder
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Jailbreak Attack
        </strong>
        &amp;
        <strong>
         Self-Reminder
        </strong>
        &amp;
        <strong>
         AI Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.10
       </td>
       <td style="text-align: center;">
        University of Zurich
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2310.18089">
         Lost in Translation -- Multilingual Misinformation and its Evolution
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Misinformation
        </strong>
        &amp;
        <strong>
         Multilingual
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.10
       </td>
       <td style="text-align: center;">
        New York University&amp;Javier Rando
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2310.18168">
         Personas as a Way to Model Truthfulness in Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Truthfulness
        </strong>
        &amp;
        <strong>
         Truthful Persona
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.10
       </td>
       <td style="text-align: center;">
        Tsinghua University, Allen Institute for AI, University of Illinois Urbana-Champaign
       </td>
       <td style="text-align: center;">
        NAACL2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2310.14564">
         Language Models Hallucinate, but May Excel at Fact Verification
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Fact Verification
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.10
       </td>
       <td style="text-align: center;">
        Stanford University&amp;University of Maryland&amp;Carnegie Mellon University&amp;NYU Shanghai&amp;New York University&amp;Microsoft Research
       </td>
       <td style="text-align: center;">
        NAACL2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2310.12558">
         Large Language Models Help Humans Verify Truthfulness‚ÄîExcept When They Are Convincingly Wrong
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Fact-Checking
        </strong>
        &amp;
        <strong>
         Truthfulness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.10
       </td>
       <td style="text-align: center;">
        Shandong University
       </td>
       <td style="text-align: center;">
        NAACL2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2310.17918">
         Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Self-Detection
        </strong>
        &amp;
        <strong>
         Non-Factuality Detection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.10
       </td>
       <td style="text-align: center;">
        Fudan University
       </td>
       <td style="text-align: center;">
        CIKM 2023
       </td>
       <td style="text-align: center;">
        <a href="https://doi.org/10.1145/3583780.3614905">
         Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Reliable Answers
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Dialpad Canada Inc
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.00681">
         Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Factuality Assessment
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        The University of Manchester
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.00671">
         Emotion Detection for Misinformation: A Review
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Survey
        </strong>
        &amp;
        <strong>
         Misinformation
        </strong>
        &amp;
        <strong>
         Emotions
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        University of Virginia
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.01386">
         Can Language Models Be Tricked by Language Illusions? Easier with Syntax, Harder with Semantics
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Language Illusions
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        University of Illinois Urbana-Champaign
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.01041">
         Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucinations
        </strong>
        &amp;
        <strong>
         Refusal Mechanism
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        University of Washington Bothell
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.01463">
         Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Healthcare
        </strong>
        &amp;
        <strong>
         Trustworthiness
        </strong>
        &amp;
        <strong>
         Hallucinations
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Intuit AI Research
       </td>
       <td style="text-align: center;">
        EMNLP2023
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.01740">
         SAC3: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Trustworthiness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Shanghai Jiao Tong University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.01766">
         Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Misinformation
        </strong>
        &amp;
        <strong>
         Disinformation
        </strong>
        &amp;
        <strong>
         Out-of-Context
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Hamad Bin Khalifa University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.03179">
         ArAIEval Shared Task: Persuasion Techniques and Disinformation Detection in Arabic Text
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Disinformation
        </strong>
        &amp;
        <strong>
         Arabic Text
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        UNC-Chapel Hill
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.03287">
         Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Benchmark
        </strong>
        &amp;
        <strong>
         Multimodal
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Cornell University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.04917">
         Adapting Fake News Detection to the Era of Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Fake news detection
        </strong>
        &amp;
        <strong>
         Generated News
        </strong>
        &amp;
        <strong>
         Misinformation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Harbin Institute of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.05232">
         A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Factual Consistency
        </strong>
        &amp;
        <strong>
         Trustworthiness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Korea University, KAIST AI,LG AI Research
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.07362">
         VOLCANO: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal Models
        </strong>
        &amp;
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Self-Feedback
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Beijing Jiaotong University, Alibaba Group, Peng Cheng Lab
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.07397">
         AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation
        </a>
       </td>
       <td style="text-align: center;">
        Multi-modal Large Language Models&amp;Hallucination&amp;Benchmark
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        LMU Munich; Munich Center of Machine Learning; Google Research
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.07424">
         Hallucination Augmented Recitations for Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Counterfactual Datasets
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Stanford University, UNC Chapel Hill
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.08401">
         Fine-tuning Language Models for Factuality
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Factuality
        </strong>
        &amp;
        <strong>
         Reference-Free Truthfulness
        </strong>
        &amp;
        <strong>
         Direct Preference Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Corporate Data and Analytics Office (CDAO)
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.07592">
         Hallucination-minimized Data-to-answer Framework for Financial Decision-makers
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Financial Decision Making
        </strong>
        &amp;
        <strong>
         Hallucination Minimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Arizona State University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.07914">
         Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Knowledge Graphs
        </strong>
        &amp;
        <strong>
         Hallucinations
        </strong>
        &amp;
        <strong>
         Survey
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Kempelen Institute of Intelligent Technologies; Brno University of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.08838">
         Disinformation Capabilities of Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Disinformation Generation
        </strong>
        &amp;
        <strong>
         Safety Filters
        </strong>
        &amp;
        <strong>
         Automated Evaluation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        UNC-Chapel Hill, University of Washington
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.09114">
         EVER: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Real-Time Verification
        </strong>
        &amp;
        <strong>
         Rectification
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Peking University, WeChat AI, Tencent Inc.
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.08147">
         RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         External Counterfactual Knowledge
        </strong>
        &amp;
        <strong>
         Benchmarking
        </strong>
        &amp;
        <strong>
         Robustness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        PolyAI Limited
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.09800">
         Dial BEINFO for Faithfulness: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Factuality
        </strong>
        &amp;
        <strong>
         Behavioural Fine-Tuning
        </strong>
        &amp;
        <strong>
         Hallucination
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        The Hong Kong University of Science and Technology, University of Illinois Urbana-Champaign
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.09677">
         R-Tuning: Teaching Large Language Models to Refuse Unknown Questions
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Refusal-Aware Instruction Tuning
        </strong>
        &amp;
        <strong>
         Knowledge Gap
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        University of Southern California, University of Pennsylvania, University of California Davis
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.09702">
         Deceiving Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucinations
        </strong>
        &amp;
        <strong>
         Semantic Associations
        </strong>
        &amp;
        <strong>
         Benchmark
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        The Ohio State University, University of California Davis
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.09447">
         How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Trustworthiness
        </strong>
        &amp;
        <strong>
         Malicious Demonstrations
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        University of Sheffield
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.09335">
         Lighter yet More Faithful: Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucinations
        </strong>
        &amp;&amp;
        <strong>
         Language Model Reliability
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Institute of Information Engineering Chinese Academy of Sciences, University of Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.12699">
         Can Large Language Models Understand Content and Propagation for Misinformation Detection: An Empirical Study
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Misinformation Detection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Shanghai Jiaotong University, Amazon AWS AI, Westlake University, IGSNRR Chinese Academy of Sciences, China
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.13230">
         Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Uncertainty-Based Methods
        </strong>
        &amp;
        <strong>
         Factuality Checking
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Institute of Software Chinese Academy of Sciences, University of Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.13314">
         Mitigating Large Language Model Hallucinations via Autonomous Knowledge Graph-based Retrofitting
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucinations
        </strong>
        &amp;
        <strong>
         Knowledge Graphs
        </strong>
        &amp;
        <strong>
         Retrofitting
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Applied Research Quantiphi
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.13878">
         Minimizing Factual Inconsistency and Hallucination in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Factual Inconsistency
        </strong>
        &amp;
        <strong>
         Hallucination
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Microsoft Research, Georgia Tech
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.14648">
         Calibrated Language Models Must Hallucinate
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination&amp;Calibration
        </strong>
        &amp;
        <strong>
         Statistical Analysis
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        School of Information Renmin University of China
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.15296">
         UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Evaluation Benchmark
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        DAMO Academy Alibaba Group, Nanyang Technological University, Hupan Lab
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.16922">
         Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Object Hallucinations
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Shanghai AI Laboratory
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.16839">
         Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal Language Models
        </strong>
        &amp;
        <strong>
         Hallucination Problem
        </strong>
        &amp;
        <strong>
         Direct Preference Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Arizona State University
       </td>
       <td style="text-align: center;">
        NAACL2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.07914">
         Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Knowledge Graphs
        </strong>
        &amp;
        <strong>
         Hallucination Reduction
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Mohamed bin Zayed University of Artificial Intelligence
       </td>
       <td style="text-align: center;">
        NAACL2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.08298">
         A Survey of Confidence Estimation and Calibration in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Confidence Estimation
        </strong>
        &amp;
        <strong>
         Calibration
        </strong>
        &amp;
        <strong>
         Large Language Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        University of California, Davis
       </td>
       <td style="text-align: center;">
        NAACL2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.09702">
         Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Semantic Shortcuts
        </strong>
        &amp;
        <strong>
         Reasoning Chains
        </strong>
        &amp;
        <strong>
         Hallucination
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        University of Utah
       </td>
       <td style="text-align: center;">
        NAACL2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.07092">
         To Tell The Truth: Language of Deception and Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Deception Detection
        </strong>
        &amp;
        <strong>
         Language Models
        </strong>
        &amp;
        <strong>
         Conversational Analysis
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.11
       </td>
       <td style="text-align: center;">
        Cornell University
       </td>
       <td style="text-align: center;">
        NAACL2024(findings)
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2311.04917">
         Adapting Fake News Detection to the Era of Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Fake News Detection
        </strong>
        &amp;
        <strong>
         Machine-Generated Content
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        Singapore Management University, Beijing Forestry University, University of Electronic Science and Technology of China
       </td>
       <td style="text-align: center;">
        MMM 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.01701">
         Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision-language Models
        </strong>
        &amp;
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Fine-grained Evaluation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        Mila, McGill University
       </td>
       <td style="text-align: center;">
        EMNLP2023(findings)
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.01858">
         Evaluating Dependencies in Fact Editing for Language Models: Specificity and Implication Awareness
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Knowledge Bases
        </strong>
        &amp;
        <strong>
         Dataset
        </strong>
        &amp;
        <strong>
         Evaluation Protocol
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        MIT CSAIL
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.03729">
         Cognitive Dissonance: Why Do Language Model Outputs Disagree with Internal Representations of Truthfulness?
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Truthfulness
        </strong>
        &amp;
        <strong>
         Internal Representations
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        University of Illinois Chicago, Bosch Research North America &amp; Bosch Center for Artificial Intelligence (BCAI), UNC Chapel-Hill
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.05200">
         DELUCIONQA: Detecting Hallucinations in Domain-specific Question Answering
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Domain-specific QA
        </strong>
        &amp;
        <strong>
         Retrieval-augmented LLMs
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        The University of Hong Kong, Beihang University
       </td>
       <td style="text-align: center;">
        AAAI2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.07049">
         Improving Factual Error Correction by Learning to Inject Factual Errors
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Factual Error Correction
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        Allen Institute for AI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.07527">
         BARDA: A Belief and Reasoning Dataset that Separates Factual Accuracy and Reasoning Ability
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Dataset
        </strong>
        &amp;
        <strong>
         Factual Accuracy
        </strong>
        &amp;
        <strong>
         Reasoning Ability
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        Tsinghua University, Shanghai Jiao Tong University, Stanford University, Nanyang Technological University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.09085">
         The Earth is Flat because...: Investigating LLMs‚Äô Belief towards Misinformation via Persuasive Conversation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Misinformation
        </strong>
        &amp;
        <strong>
         Persuasive Conversation
        </strong>
        &amp;
        <strong>
         Factual Questions
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        University of California Davis
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.11870">
         A Revisit of Fake News Dataset with Augmented Fact-checking by ChatGPT
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Fake News
        </strong>
        &amp;
        <strong>
         Fact-checking
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        Amazon Web Services
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.14183">
         On Early Detection of Hallucinations in Factual Question Answering
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucinations
        </strong>
        &amp;
        <strong>
         Factual Question Answering
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        University of California Santa Cruz
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.14346">
         Don‚Äôt Believe Everything You Read: Enhancing Summarization Interpretability through Automatic Identification of Hallucinations in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucinations
        </strong>
        &amp;
        <strong>
         Faithfulness
        </strong>
        &amp;
        <strong>
         Token-level
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        Department of Radiology, The University of Tokyo Hospital
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.14504">
         Theory of Hallucinations based on Equivariance
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucinations
        </strong>
        &amp;
        <strong>
         Equivariance
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        Georgia Institute of Technology
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.15576">
         REDUCING LLM HALLUCINATIONS USING EPISTEMIC NEURAL NETWORKS
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucinations
        </strong>
        &amp;
        <strong>
         Uncertainty Estimation
        </strong>
        &amp;
        <strong>
         TruthfulQA
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, Tencent AI Lab
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.15710">
         Alleviating Hallucinations of Large Language Models through Induced Hallucinations
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucinations
        </strong>
        &amp;
        <strong>
         Induce-then-Contrast Decoding
        </strong>
        &amp;
        <strong>
         Factuality
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        23.12
       </td>
       <td style="text-align: center;">
        SKLOIS Institute of Information Engineering Chinese Academy of Sciences, School of Cyber Security University of Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2312.16374">
         LLM Factoscope: Uncovering LLMs‚Äô Factual Discernment through Inner States Analysis
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Factual Detection
        </strong>
        &amp;
        <strong>
         Inner States
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        The Chinese University of Hong Kong, Tencent AI Lab
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.00761">
         The Earth is Flat? Unveiling Factual Errors in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Factual Errors
        </strong>
        &amp;
        <strong>
         Knowledge Graph
        </strong>
        &amp;
        <strong>
         Answer Assessment
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        NewsBreak, University of Illinois Urbana-Champaign
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.00396">
         RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Retrieval-Augmented Generation
        </strong>
        &amp;
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Dataset
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        University of California Berkeley, Universit√© de Montr√©al, McGill UniversityÔºå Mila
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.01197">
         Uncertainty Resolution in Misinformation Detection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Misinformation
        </strong>
        &amp;
        <strong>
         Uncertainty Resolution
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        Yale University, Stanford University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.01301">
         Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Legal Hallucinations
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        Islamic University of Technology, AI Institute University of South Carolina, Stanford University, Amazon AI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.01313">
         A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        √ü
        <strong>
         Hallucination Mitigation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        Renmin University of China, Renmin University of China, DIRO, Universit√© de Montr√©al
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.03205">
         The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Detection and Mitigation
        </strong>
        &amp;
        <strong>
         Empirical Study
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        IIT Hyderabad India, Parmonic USA, University of Glasgow UK, LDRP Institute of Technology and Research India
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.04481">
         Fighting Fire with Fire: Adversarial Prompting to Generate a Misinformation Detection Dataset
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Misinformation Detection
        </strong>
        &amp;
        <strong>
         LLM-generated Synthetic Data
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        University College London
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.05827">
         Hallucination Benchmark in Medical Visual Question Answering
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Medical Visual Question Answering
        </strong>
        &amp;
        <strong>
         Hallucination Benchmark
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        Soochow University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.06792">
         LightHouse: A Survey of AGI Hallucination
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         AGI Hallucination
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        University of Washington, Carnegie Mellon University, Allen Institute for AI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.06855">
         Fine-grained Hallucination Detection and Editing for Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         FAVA
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        Dartmouth College, Universit√© de Montr√©al, McGill UniversityÔºåMila
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.06920">
         Comparing GPT-4 and Open-Source Language Models in Misinformation Mitigation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         GPT-4
        </strong>
        &amp;
        <strong>
         Misinformation Detection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        Utrecht University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.07897">
         The Pitfalls of Defining Hallucination
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        Samsung AI Center
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.08358">
         Hallucination Detection and Hallucination Mitigation: An Investigation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Hallucination Mitigation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        McGill UniversityÔºå Mila, Universit√© de Montr√©al
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.08694">
         Combining Confidence Elicitation and Sample-based Methods for Uncertainty Quantification in Misinformation Mitigation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Misinformation Mitigation
        </strong>
        &amp;
        <strong>
         Uncertainty Quantification
        </strong>
        &amp;
        <strong>
         Sample-based Consistency
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        LY Corporation
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.09774">
         On the Audio Hallucinations in Large Audio-Video Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Audio Hallucinations
        </strong>
        &amp;
        <strong>
         Audio-visual Learning
        </strong>
        &amp;
        <strong>
         Audio-video language Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        Sun Yat-sen University Tencent AI Lab
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.10768">
         Mitigating Hallucinations of Large Language Models via Knowledge Consistent Alignment
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Knowledge Consistent Alignment
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        National University of Singapore
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.11817">
         Hallucination is Inevitable: An Innate Limitation of Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Real World LLMs
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        X2Robot&amp;International Digital Economy Academy
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.15449">
         Learning to Trust Your Feelings: Leveraging Self-awareness in LLMs for Hallucination Mitigation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Knowledge Probing
        </strong>
        &amp;
        <strong>
         Reinforcement Learning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        University of Texas at Austin, Northeastern University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.16558">
         Diverse but Divisive: LLMs Can Exaggerate Gender Differences in Opinion Related to Harms of Misinformation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Misinformation Detection
        </strong>
        &amp;
        <strong>
         Socio-Technical Systems
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.01
       </td>
       <td style="text-align: center;">
        National University of Defense Technology, National University of Singapore
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2401.17809">
         SWEA: Changing Factual Knowledge in Large Language Models via Subject Word Embedding Altering
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Factual Knowledge Editing
        </strong>
        &amp;
        <strong>
         Word Embeddings
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        University of Washington, University of California Berkeley, The Hong Kong University of Science and Technology, Carnegie Mellon University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.00367">
         Don‚Äôt Hallucinate Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Knowledge Gaps
        </strong>
        &amp;
        <strong>
         Multi-LLM Collaboration
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        IT Innovation and Research Center, Huawei Technologies
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.00253">
         A Survey on Hallucination in Large Vision-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Large Vision-Language Models
        </strong>
        &amp;
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Mitigation Strategies
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Tianjin University, National University of Singapore, A*STAR
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.01345">
         SKIP \N: A SIMPLE METHOD TO REDUCE HALLUCINATION IN LARGE VISION-LANGUAGE MODELS
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Semantic Shift Bias
        </strong>
        &amp;
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Vision-Language Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        University of Marburg, University of Mannheim
       </td>
       <td style="text-align: center;">
        EACL Findings 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.01453">
         The Queen of England is not England‚Äôs Queen: On the Lack of Factual Coherency in PLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Factual Coherency
        </strong>
        &amp;
        <strong>
         Knowledge Bases
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        MBZUAI, Monash University, LibrAI, Sofia University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.02420">
         Factuality of Large Language Models in the Year 2024
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Factuality
        </strong>
        &amp;
        <strong>
         Evaluation
        </strong>
        &amp;
        <strong>
         Multimodal LLMs
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Institute of Information Engineering, Chinese Academy of Sciences, University of Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.02549">
         Are Large Language Models Table-based Fact-Checkers?
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Table-based Fact Verification
        </strong>
        &amp;
        <strong>
         In-context Learning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Zhejiang University, Ant Group
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.03190">
         Unified Hallucination Detection for Multimodal Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal Large Language Models
        </strong>
        &amp;
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Benchmark
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Alibaba Cloud, Zhejiang University
       </td>
       <td style="text-align: center;">
        ICLR2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.03744">
         INSIDE: LLMS‚Äô INTERNAL STATES RETAIN THE POWER OF HALLUCINATION DETECTION
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         EigenScore
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        The Hong Kong University of Science and Technology, University of Illinois at Urbana-Champaign, The Hong Kong Polytechnic University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.03757">
         The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal Large Language Models
        </strong>
        &amp;
        <strong>
         Hallucination
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Institute of Automation Chinese Academy of Sciences, University of Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.03916">
         Can Large Language Models Detect Rumors on Social Media?
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Rumor Detection
        </strong>
        &amp;
        <strong>
         Social Media
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        CAS Key Laboratory of AI Safety, School of Computer Science and Technology University of Chinese Academy of Science, International Digital Economy Academy IDEA Research
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.06647">
         A Survey on Large Language Model Hallucination via a Creativity Perspective
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Creativity
        </strong>
        &amp;
        <strong>
         Hallucination
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        University College London, Speechmatics, MATS, Anthropic, FAR AI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.06782">
         Debating with More Persuasive LLMs Leads to More Truthful Answers
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Debate
        </strong>
        &amp;
        <strong>
         Truthfulness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        University of Illinois Urbana-Champaign, DAMO Academy Alibaba Group, Northwestern University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.07401">
         Towards Faithful Explainable Fact-Checking via Multi-Agent Debate
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Fact-checking
        </strong>
        &amp;
        <strong>
         Explainability
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Rice Universitym, Texas A&amp;M University, Wake Forest University, New Jersey Institute of Technology, Meta Platforms Inc.
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.04678">
         Large Language Models As Faithful Explainers
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Explainability
        </strong>
        &amp;
        <strong>
         Fidelity
        </strong>
        &amp;
        <strong>
         Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        The Hong Kong University of Science and Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.09733">
         Do LLMs Know about Hallucination? An Empirical Investigation of LLM‚Äôs Hidden States
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Hidden States
        </strong>
        &amp;
        <strong>
         Model Interpretation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        UC Santa Cruz, ByteDance Research, Northwestern University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.10412">
         MEASURING AND REDUCING LLM HALLUCINATION WITHOUT GOLD-STANDARD ANSWERS VIA EXPERTISE-WEIGHTING
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Large Language Models (LLMs)
        </strong>
        &amp;
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Factualness Evaluations
        </strong>
        &amp;
        <strong>
         FEWL
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Paul G. Allen School of Computer Science &amp; Engineering, University of Washington
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.10496">
         Comparing Hallucination Detection Metrics for Multilingual Generation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Multilingual Generation
        </strong>
        &amp;
        <strong>
         Lexical Metrics
        </strong>
        &amp;
        <strong>
         Natural Language Inference (NLI)
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Institute of Computing Technology, Chinese Academy of Sciences; University of Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.10612">
         Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Large Language Models (LLMs)
        </strong>
        &amp;
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Retrieval Augmentation
        </strong>
        &amp;
        <strong>
         Rowen
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Institute of Automation, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Nanjing University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.11622">
         Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Object Hallucination
        </strong>
        &amp;
        <strong>
         Vision-Language Models (LVLMs)
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Institute of Mathematics and Statistics University of S√£o Paulo, Artificial Intelligence Specialist in the Banking Sector
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.14002">
         Hallucinations or Attention Misdirection? The Path to Strategic Value Extraction in Business Using Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucinations
        </strong>
        &amp;
        <strong>
         Generative Artificial Intelligence
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Stevens Institute of Technology, Peraton Labs
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.14268">
         Can Large Language Models Detect Misinformation in Scientific News Reporting?
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Scientific Reporting
        </strong>
        &amp;
        <strong>
         Misinformation
        </strong>
        &amp;
        <strong>
         Explainability
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Middle East Technical University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.16211">
         HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination Tendency of LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Benchmarking Dataset
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        National University of Singapore
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.15300">
         Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         CLIP-Guided Decoding
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        University of California Los Angeles, Cisco Research
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.18048">
         Characterizing Truthfulness in Large Language Model Generations with Local Intrinsic Dimension
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Truthfulness
        </strong>
        &amp;
        <strong>
         Local Intrinsic Dimension
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Institute of Automation Chinese Academy of Sciences, School of Artificial Intelligence University of Chinese Academy of Sciences, Hunan Normal University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.19103">
         Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         False Premise Hallucinations
        </strong>
        &amp;
        <strong>
         Attention Mechanism
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        Shanghai Artificial Intelligence Laboratory, Renmin University of China, University of Chinese Academy of Sciences, Shanghai Jiao Tong University, The University of Sydney
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.19465">
         Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Trustworthiness Dynamics
        </strong>
        &amp;
        <strong>
         Pre-training
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.02
       </td>
       <td style="text-align: center;">
        AWS AI Labs&amp;Korea Advanced Institute of Science &amp; Technology&amp;The University of Texas at Austin
       </td>
       <td style="text-align: center;">
        NAACL2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2402.13249">
         TOFUEVAL: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Evaluation
        </strong>
        &amp;
        <strong>
         Dialogue Summarization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        √âcole polytechnique f√©d√©rale de Lausanne, Carnegie Mellon University, University of Maryland College Park
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.00180">
         "Flex Tape Can‚Äôt Fix That": Bias and Misinformation in Edited Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Model Editing
        </strong>
        &amp;
        <strong>
         Demographic Bias
        </strong>
        &amp;
        <strong>
         Misinformation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        East China Normal University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.00896">
         DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Dialogue-level Hallucination
        </strong>
        &amp;
        <strong>
         Benchmarking
        </strong>
        &amp;
        <strong>
         Human-machine Interaction
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        Peking University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.01373">
         Evaluating and Mitigating Number Hallucinations in Large Vision-Language Models: A Consistency Perspective
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Number Hallucination
        </strong>
        &amp;
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Consistency Training
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        City University of Hong Kong, National University of Singapore, Shanghai Jiao Tong University, Stanford University, Penn State University, Hong Kong University of Science and Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.01548">
         IN-CONTEXT SHARPNESS AS ALERTS: AN INNER REPRESENTATION PERSPECTIVE FOR HALLUCINATION MITIGATION
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Inner Representation
        </strong>
        &amp;
        <strong>
         Entropy
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        Microsoft
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.02889">
         In Search of Truth: An Interrogation Approach to Hallucination Detection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Interrogation Technique
        </strong>
        &amp;
        <strong>
         Balanced Accuracy
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        Mohamed bin Zayed University of Artificial Intelligence
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.03627">
         Multimodal Large Language Models to Support Real-World Fact-Checking
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal Large Language Models
        </strong>
        &amp;
        <strong>
         Fact-Checking
        </strong>
        &amp;
        <strong>
         Misinformation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        KAIST, Microsoft Research Asia
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.05266">
         ERBENCH: AN ENTITY-RELATIONSHIP BASED AUTOMATICALLY VERIFIABLE HALLUCINATION BENCHMARK FOR LARGE LANGUAGE MODELS
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Entity-Relationship Model
        </strong>
        &amp;
        <strong>
         Benchmarking
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        University of Alberta, Platform and Content Group, Tencent
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.07557">
         SIFiD: Reassess Summary Factual Inconsistency Detection with LLM
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Factual Consistency
        </strong>
        &amp;
        <strong>
         Summarization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences; University of Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.07556">
         Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Truth Detection
        </strong>
        &amp;
        <strong>
         Context Selection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        UC Berkeley, Google DeepMind
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.05612">
         Unfamiliar Finetuning Examples Control How Language Models Hallucinate
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Finetuning
        </strong>
        &amp;
        <strong>
         Hallucination Control
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        University of Alberta, Platform and Content Group, Tencent
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.07557">
         SIFiD: Reassess Summary Factual Inconsistency Detection with LLM
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Factual Consistency
        </strong>
        &amp;
        <strong>
         Summarization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences; University of Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.07556">
         Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Truth Detection
        </strong>
        &amp;
        <strong>
         Context Selection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        UC Berkeley, Google DeepMind
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.05612">
         Unfamiliar Finetuning Examples Control How Language Models Hallucinate
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Finetuning
        </strong>
        &amp;
        <strong>
         Hallucination Control
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        Google Research, UC San Diego
       </td>
       <td style="text-align: center;">
        COLING 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.08904">
         Detecting Hallucination and Coverage Errors in Retrieval Augmented Generation for Controversial Topics
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Conversational Systems
        </strong>
        &amp;
        <strong>
         Evaluation Methodologies
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        University of Maryland, University of Antwerp, New York University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.09148">
         Evaluating LLMs for Gender Disparities in Notable Persons
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Bias
        </strong>
        &amp;
        <strong>
         Fairness
        </strong>
        &amp;
        <strong>
         Hallucinations
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        University of Duisburg-Essen
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.09743">
         The Human Factor in Detecting Errors of Large Language Models: A Systematic Literature Review and Future Research Directions
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        Wuhan University, Beihang University, The University of Sydney, Nanyang Technological University
       </td>
       <td style="text-align: center;">
        COLING 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.09963">
         Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias in Factual Knowledge Extraction
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Factual Knowledge Extraction
        </strong>
        &amp;
        <strong>
         Prompt Bias
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        Carnegie Mellon University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.10446">
         Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Retrieval Augmented Generation (RAG)
        </strong>
        &amp;
        <strong>
         Private Knowledge-Bases
        </strong>
        &amp;
        <strong>
         Hallucinations
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        Integrated Vision and Language Lab KAIST South Korea
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.13513">
         What if...?: Counterfactual Inception to Mitigate Hallucination Effects in Large Multimodal Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Large Multimodal Models
        </strong>
        &amp;
        <strong>
         Hallucination
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        UCAS
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.14171">
         MMIDR: Teaching Large Language Model to Interpret Multimodal Misinformation via Knowledge Distillation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal Misinformation Detection
        </strong>
        &amp;
        <strong>
         Knowledge Distillation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        Seoul National University, Sogang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.16167">
         Exploiting Semantic Reconstruction to Mitigate Hallucinations in Vision-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Semantic Reconstruction
        </strong>
        &amp;
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Hallucination Mitigation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        University of Illinois Urbana-Champaign
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.16527">
         Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Foundation Models
        </strong>
        &amp;
        <strong>
         Decision-Making
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        Shanghai Jiao Tong University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.18349">
         Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Knowledge Feedback
        </strong>
        &amp;
        <strong>
         Reliable Reward Model
        </strong>
        &amp;
        <strong>
         Refusal Mechanism
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        Universit√§t Hamburg, The University of Sydney
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.18715">
         Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Instruction Contrastive Decoding
        </strong>
        &amp;
        <strong>
         Large Vision-Language Models
        </strong>
        &amp;
        <strong>
         Hallucination Mitigation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        AI Institute University of South Carolina, Indian Institute of Technology Kharagpur, Islamic University of Technology, Stanford University, Amazon AI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.18976">
         ‚ÄúSorry Come Again?‚Äù Prompting ‚Äì Enhancing Comprehension and Diminishing Hallucination with [PAUSE] -injected Optimal Paraphrasing
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt Engineering
        </strong>
        &amp;
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         [PAUSE] Injection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        Beihang University, School of Computer Science and Engineering, School of Software, Shandong University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.00971">
         Exploring and Evaluating Hallucinations in LLM-Powered Code Generation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Code Generation
        </strong>
        &amp;
        <strong>
         Hallucination
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        University of Illinois Urbana-Champaign
       </td>
       <td style="text-align: center;">
        NAACL2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.14952">
         Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Online Misinformation
        </strong>
        &amp;
        <strong>
         Retrieval Augmented Response
        </strong>
        &amp;
        <strong>
         Evidence-Based Countering
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        Department of Electronic Engineering, Tsinghua University, Pattern Recognition Center, WeChat AI, Tencent Inc, China
       </td>
       <td style="text-align: center;">
        NAACL 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.20009">
         On Large Language Models‚Äô Hallucination with Regard to Known Facts
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Inference Dynamics
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        Department of Electronic Engineering, Tsinghua University, Pattern Recognition Center, WeChat AI, Tencent Inc, China
       </td>
       <td style="text-align: center;">
        NAACL 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.20009">
         On Large Language Models‚Äô Hallucination with Regard to Known Facts
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Inference Dynamics
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.03
       </td>
       <td style="text-align: center;">
        Tsinghua University, WeChat AI, Tencent Inc.
       </td>
       <td style="text-align: center;">
        NAACL2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2403.20009">
         On Large Language Models‚Äô Hallucination with Regard to Known Facts
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Inference Dynamics
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        Technical University of Munich, University of Stavanger, University of Alberta
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.04722">
         PoLLMgraph: Unraveling Hallucinations in Large Language Models via State Transition Dynamics
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         State Transition Dynamics
        </strong>
        &amp;
        <strong>
         Large Language Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        University of Edinburgh, University College London, Peking University, Together AI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.05904">
         The Hallucinations Leaderboard ‚Äì An Open Effort to Measure Hallucinations in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Benchmarking
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        IIIT Hyderabad, Purdue University, Northwestern University, Indiana University Indianapolis
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.06948">
         Halu-NLP at SemEval-2024 Task 6: MetaCheckGPT - A Multi-task Hallucination Detection Using LLM Uncertainty and Meta-models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         LLM Uncertainty
        </strong>
        &amp;
        <strong>
         Meta-models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        Technion ‚Äì Israel Institute of Technology, Google Research
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.09971">
         Constructing Benchmarks and Interventions for Combating Hallucinations in LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucinations
        </strong>
        &amp;
        <strong>
         Benchmarks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        The University of Texas at Austin, Salesforce AI Research
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.10774">
         MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Fact-Checking
        </strong>
        &amp;
        <strong>
         Efficiency
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        Meta, Technical University of Munich
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.10960">
         Uncertainty-Based Abstention in LLMs Improves Safety and Reduces Hallucinations
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Safety
        </strong>
        &amp;
        <strong>
         Hallucinations
        </strong>
        &amp;
        <strong>
         Uncertainty
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        Zhejiang University, Alibaba Group,  Fudan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.14233">
         Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Large Vision Language Model
        </strong>
        &amp;
        <strong>
         Hallucination Detection And Mitigating
        </strong>
        &amp;
        <strong>
         Direct Preference Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        Cheriton School of Computer Science
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.16859">
         Rumour Evaluation with Very Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Misinformation in Social Networks
        </strong>
        &amp;
        <strong>
         Explainable AI
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        University of California, Berkeley
       </td>
       <td style="text-align: center;">
        NAACL 2024
       </td>
       <td style="text-align: center;">
        <a href="https://davidmchan.github.io/aloha">
         ALOHa: A New Measure for Hallucination in Captioning Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Attack
        </strong>
        &amp;
        <strong>
         AI-Text Detection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        ServiceNow
       </td>
       <td style="text-align: center;">
        NAACL 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.08189">
         Reducing hallucination in structured outputs via Retrieval-Augmented Generation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Retrieval-Augmented Generation
        </strong>
        &amp;
        <strong>
         Structured Outputs
        </strong>
        &amp;
        <strong>
         Generative AI
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        Stanford University
       </td>
       <td style="text-align: center;">
        NAACL2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.01651">
         NLP Systems That Can‚Äôt Tell Use from Mention Censor Counterspeech, but Teaching the Distinction Helps
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Counterspeech
        </strong>
        &amp;
        <strong>
         Censorship
        </strong>
        &amp;
        <strong>
         Use-Mention Distinction
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        Department of Computing Science, University of Aberdeen
       </td>
       <td style="text-align: center;">
        NAACL2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.04103">
         Improving Factual Accuracy of Neural Table-to-Text Output by Addressing Input Problems in ToTTo
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Neural Table-to-Text
        </strong>
        &amp;
        <strong>
         Factual Accuracy
        </strong>
        &amp;
        <strong>
         Input Problems
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        Seoul National University
       </td>
       <td style="text-align: center;">
        NAACL2024(findings)
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.09480">
         Mitigating Hallucination in Abstractive Summarization with Domain-Conditional Mutual Information
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Abstractive Summarization
        </strong>
        &amp;
        <strong>
         Domain-Conditional Mutual Information
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        The University of Tokyo, University of California Santa Barbara, Mila - Qu√©bec AI Institute,  Universit√© de Montr√©al, Speech Lab, Alibaba Group,  Hong Kong Baptist University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.00253">
         CodeHalu: Code Hallucinations in LLMs Driven by Execution-based Verification
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Code Hallucination
        </strong>
        &amp;
        <strong>
         Execution-based Verification
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        Department of Computer Science, The University of Sheffield
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.00611">
         Addressing Topic Granularity and Hallucination in Large Language Models for Topic Modelling
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Topic Modelling
        </strong>
        &amp;
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Topic Granularity
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.04
       </td>
       <td style="text-align: center;">
        School of Computing and Information Systems
       </td>
       <td style="text-align: center;">
        COLING 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2404.17283">
         Reinforcement Retrieval Leveraging Fine-grained Feedback for Fact Checking News Claims with Black-Box LLM
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Claim Verification
        </strong>
        &amp;
        <strong>
         Reinforcement Retrieval
        </strong>
        &amp;
        <strong>
         Fine-Grained Feedback
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        DeepMind
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.01563">
         Mitigating LLM Hallucinations via Conformal Abstention
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Conformal Prediction
        </strong>
        &amp;
        <strong>
         Hallucination Mitigation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        MBZUAI, Monash University, Sofia University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.05583">
         OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Factuality Evaluation
        </strong>
        &amp;
        <strong>
         Automatic Fact-Checking
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        Indian Institute of Technology Patna
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.09589">
         Unveiling Hallucination in Text, Image, Video, and Audio Foundation Models: A Comprehensive Review
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Multimodal Models
        </strong>
        &amp;
        <strong>
         Review
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        Dublin City University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.09454">
         Tell Me Why: Explainable Public Health Fact-Checking with Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Explainable AI
        </strong>
        &amp;
        <strong>
         Fact-Checking
        </strong>
        &amp;
        <strong>
         Public Health
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        University of Information Technology, Vietnam National University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.07615">
         ViWikiFC: Fact-Checking for Vietnamese Wikipedia-Based Textual Knowledge Source
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Fact Checking
        </strong>
        &amp;
        <strong>
         Information Verification
        </strong>
        &amp;
        <strong>
         Corpus
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        Imperial College London
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.06545">
         Mitigating Hallucinations in Large Language Models via Self-Refinement-Enhanced Knowledge Retrieval
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Knowledge Graph Retrieval
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        Paul G. Allen School of Computer Science &amp; Engineering
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.19285">
         MASSIVE Multilingual Abstract Meaning Representation: A Dataset and Baselines for Hallucination Detection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Multilingual AMR
        </strong>
        &amp;
        <strong>
         Dataset
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        Microsoft Corporation
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.19563">
         Unlearning Climate Misinformation in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Climate Misinformation
        </strong>
        &amp;
        <strong>
         Unlearning
        </strong>
        &amp;
        <strong>
         Fine-Tuning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        Baylor University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.19648">
         Detecting Hallucinations in Large Language Model Generation: A Token Probability Approach
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucinations Detection
        </strong>
        &amp;
        <strong>
         Token Probability Approach
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.05
       </td>
       <td style="text-align: center;">
        Shanghai AI Laboratory
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2405.20315">
         ANAH: Analytical Annotation of Hallucinations in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucinations
        </strong>
        &amp;
        <strong>
         Analytical Annotation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        University of Waterloo
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.01855">
         TruthEval: A Dataset to Evaluate LLM Truthfulness and Reliability
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Truthfulness
        </strong>
        &amp;
        <strong>
         Reliability
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Peking University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.03075">
         Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Markov Chain
        </strong>
        &amp;
        <strong>
         Multi-agent Debate
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Northeastern University
       </td>
       <td style="text-align: center;">
        ACL 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.03487">
         Analyzing LLM Behavior in Dialogue Summarization: Unveiling Circumstantial Hallucination Trends
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Dialogue Summarization
        </strong>
        &amp;
        <strong>
         Circumstantial Hallucination
        </strong>
        &amp;
        <strong>
         Error Detection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        McGill University
       </td>
       <td style="text-align: center;">
        ACL 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.04175">
         Confabulation: The Surprising Value of Large Language Model Hallucinations
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Confabulation
        </strong>
        &amp;
        <strong>
         Hallucinations
        </strong>
        &amp;
        <strong>
         Narrativity
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        University of Michigan
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.05132">
         3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         3D-LLMs
        </strong>
        &amp;
        <strong>
         Grounding
        </strong>
        &amp;
        <strong>
         Hallucination
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Arizona State University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.05494">
         Investigating and Addressing Hallucinations of LLMs in Tasks Involving Negation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucinations
        </strong>
        &amp;
        <strong>
         Negation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Tsinghua University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.07057">
         Benchmarking Trustworthiness of Multimodal Large Language Models: A Comprehensive Study
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Trustworthiness
        </strong>
        &amp;
        <strong>
         MLLMs
        </strong>
        &amp;
        <strong>
         Benchmark
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Beijing Academy of Artificial Intelligence
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.07070">
         HalluDial: A Large-Scale Benchmark for Automatic Dialogue-Level Hallucination Evaluation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Evaluation
        </strong>
        &amp;
        <strong>
         Dialogue-Level
        </strong>
        &amp;
        <strong>
         HalluDial
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        KFUPM
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.09155">
         DefAn: Definitive Answer Dataset for LLMs Hallucination Evaluation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Evaluation
        </strong>
        &amp;
        <strong>
         Definitive Answers
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Harbin Institute of Technology
       </td>
       <td style="text-align: center;">
        ACL 2024 findings
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.07036">
         Paying More Attention to Source Context: Mitigating Unfaithful Translations from Large Language Model
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Unfaithful Translations
        </strong>
        &amp;
        <strong>
         Source Context
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        National Taiwan University
       </td>
       <td style="text-align: center;">
        Interspeech 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.08402">
         Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Large audio-language models
        </strong>
        &amp;
        <strong>
         Object hallucination
        </strong>
        &amp;
        <strong>
         Discriminative questions
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        University of Texas at San Antonio
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.10279">
         We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Package Hallucinations
        </strong>
        &amp;
        <strong>
         Code Generating LLMs
        </strong>
        &amp;
        <strong>
         Software Supply Chain Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        The University of Manchester
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.11093">
         RAEmoLLM: Retrieval Augmented LLMs for Cross-Domain Misinformation Detection Using In-Context Learning based on Emotional Information
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         RAEmoLLM
        </strong>
        &amp;
        <strong>
         Cross-Domain Misinformation Detection
        </strong>
        &amp;
        <strong>
         Affective Information
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        KAIST
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.11260">
         Adversarial Style Augmentation via Large Language Model for Robust Fake News Detection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Style Augmentation
        </strong>
        &amp;
        <strong>
         Fake News Detection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        The Chinese University of Hong Kong
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.11267">
         Mitigating Large Language Model Hallucination with Faithful Finetuning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Faithful Finetuning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Gaoling School of Artificial Intelligence, Renmin University of China
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.11277">
         Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Small Language Models
        </strong>
        &amp;
        <strong>
         HaluAgent
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        University of Science and Technology of China
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.11497">
         CrAM: Credibility-Aware Attention Modification in LLMs for Combating Misinformation in RAG
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         CrAM
        </strong>
        &amp;
        <strong>
         Credibility-Aware Attention
        </strong>
        &amp;
        <strong>
         Retrieval-Augmented Generation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        University of Rochester
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.12663">
         Do More Details Always Introduce More Hallucinations in LVLM-based Image Captioning?
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LVLMs
        </strong>
        &amp;
        <strong>
         Image Captioning
        </strong>
        &amp;
        <strong>
         Object Hallucination
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Xi'an Jiaotong University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.12718">
         AGLA: Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         AGLA
        </strong>
        &amp;
        <strong>
         Object Hallucinations
        </strong>
        &amp;
        <strong>
         Large Vision-Language Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        University of Groningen, University of Amsterdam
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.13663">
         Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Retrieval-Augmented Generation
        </strong>
        &amp;
        <strong>
         Trustworthy AI
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Seoul National University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.13929">
         Large Language Models are Skeptics: False Negative Problem of Input-conflicting Hallucination
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         False Negative Problem
        </strong>
        &amp;
        <strong>
         Input-conflicting Hallucination
        </strong>
        &amp;
        <strong>
         Bias
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        University of Houston
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.14012">
         Seeing Through AI‚Äôs Lens: Enhancing Human Skepticism Towards LLM-Generated Fake News
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Fake news
        </strong>
        &amp;
        <strong>
         LLM-generated news
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        University of Oxford
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.15927">
         Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Semantic Entropy
        </strong>
        &amp;
        <strong>
         Probes
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        UC San Diego
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.17260">
         Mitigating Hallucination in Fictional Character Role-Play
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Role-Play
        </strong>
        &amp;
        <strong>
         Fictional Characters
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Lamini
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.17642">
         Banishing LLM Hallucinations Requires Rethinking Generalization
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucinations
        </strong>
        &amp;
        <strong>
         Generalization
        </strong>
        &amp;
        <strong>
         Memory Experts
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.06
       </td>
       <td style="text-align: center;">
        Waseda University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2406.20015">
         ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Tool-Augmented Large Language Models
        </strong>
        &amp;
        <strong>
         Hallucination Diagnostic Benchmark
        </strong>
        &amp;
        <strong>
         Tool Usage
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Beihang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.00488">
         PFME: A Modular Approach for Fine-grained Hallucination Detection and Editing of Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Model Editing
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Tsinghua University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.02042">
         Fake News Detection and Manipulation Reasoning via Large Vision-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Large Vision-Language Models
        </strong>
        &amp;
        <strong>
         Fake News Detection
        </strong>
        &amp;
        <strong>
         Manipulation Reasoning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Brno University of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.02351">
         Generative Large Language Models in Automated Fact-Checking: A Survey
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Automated Fact-Checking
        </strong>
        &amp;
        <strong>
         Survey
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        SRI International
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.02352">
         Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision-LLMs
        </strong>
        &amp;
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Claim Verification
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Hong Kong University of Science and Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.03282">
         LLM Internal States Reveal Hallucination Risk Faced With a Query
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Uncertainty Estimation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Harbin Institute of Technology
       </td>
       <td style="text-align: center;">
        ICLR 2024 AGI Workshop
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.00569">
         Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Multimodal Hallucination
        </strong>
        &amp;
        <strong>
         Residual Visual Decoding
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Harbin Institute of Technology
       </td>
       <td style="text-align: center;">
        ACL 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.00569">
         Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal Hallucinations
        </strong>
        &amp;
        <strong>
         LVLMs
        </strong>
        &amp;
        <strong>
         Residual Visual Decoding
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        University of Amsterdam
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.04485">
         Leveraging Graph Structures to Detect Hallucinations in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Graph Attention Network
        </strong>
        &amp;
        <strong>
         Large Language Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Cisco Research
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.04831">
         Code Hallucination
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Code Hallucination
        </strong>
        &amp;
        <strong>
         Generative Models
        </strong>
        &amp;
        <strong>
         HallTrigger
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Beijing Jiaotong University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.05868">
         KG-FPQ: Evaluating Factuality Hallucination in LLMs with Knowledge Graph-based False Premise Questions
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Factuality Hallucination
        </strong>
        &amp;
        <strong>
         Knowledge Graph
        </strong>
        &amp;
        <strong>
         False Premise Questions
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        University of California, Santa Barbara
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.06426">
         DebUnc: Mitigating Hallucinations in Large Language Model Agent Communication with Uncertainty Estimations
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucinations
        </strong>
        &amp;
        <strong>
         Uncertainty Estimations
        </strong>
        &amp;
        <strong>
         Multi-agent Systems
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Massachusetts Institute of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.07071">
         Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Contextual Hallucinations
        </strong>
        &amp;
        <strong>
         Attention Maps
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        University of Illinois Urbana-Champaign
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.08039">
         Knowledge Overshadowing Causes Amalgamated Hallucination in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Knowledge Overshadowing
        </strong>
        &amp;
        <strong>
         Hallucination
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Patronus AI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.08488">
         Lynx: An Open Source Hallucination Evaluation Model
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         RAG
        </strong>
        &amp;
        <strong>
         Evaluation Model
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Shanghai Jiao Tong University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.08582">
         On the Universal Truthfulness Hyperplane Inside LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Truthfulness Hyperplane
        </strong>
        &amp;
        <strong>
         Hallucination
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        University of Michigan
       </td>
       <td style="text-align: center;">
        ACL 2024 ALVR
       </td>
       <td style="text-align: center;">
        <a href="https://multi-object-hallucination.github.io/">
         Multi-Object Hallucination in Vision-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multi-Object Hallucination
        </strong>
        &amp;
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Evaluation Protocol
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        ASAPP, Inc.
       </td>
       <td style="text-align: center;">
        ACL 2024 Findings
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.05474">
         Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Synthetic Data
        </strong>
        &amp;
        <strong>
         System Responses
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        FAR AI
       </td>
       <td style="text-align: center;">
        COLM 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.08734">
         Transformer Circuit Faithfulness Metrics Are Not Robust
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Transformer Circuits
        </strong>
        &amp;
        <strong>
         Ablation Studies
        </strong>
        &amp;
        <strong>
         Faithfulness Metrics
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        University of Science and Technology of China
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.08952">
         Detect, Investigate, Judge and Determine: A Novel LLM-based Framework for Few-shot Fake News Detection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Fake News Detection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Tsinghua University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.09417">
         Mitigating Entity-Level Hallucination in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Retrieval Augmented Generation
        </strong>
        &amp;
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Amazon Web Services
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.09726">
         On Mitigating Code LLM Hallucinations with API Documentation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         API Hallucinations
        </strong>
        &amp;
        <strong>
         Code LLMs
        </strong>
        &amp;
        <strong>
         Documentation Augmented Generation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Technical University of Darmstadt
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.11930">
         Fine-grained Hallucination Detection and Mitigation in Long-form Question Answering
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Error Annotation
        </strong>
        &amp;
        <strong>
         Factuality
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Heidelberg University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.12831">
         Truth is Universal: Robust Detection of Lies in LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Lie Detection
        </strong>
        &amp;
        <strong>
         Activation Vectors
        </strong>
        &amp;
        <strong>
         Truth Direction
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Shanghai Jiao Tong University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.12943">
         HALU-J: Critique-Based Hallucination Judge
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Critique-Based Evaluation
        </strong>
        &amp;
        <strong>
         Evidence Categorization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        TH K√∂ln ‚Äì University of Applied Sciences
       </td>
       <td style="text-align: center;">
        CLEF 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.13757">
         The Two Sides of the Coin: Hallucination Generation and Detection with LLMs as Evaluators for LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Generation
        </strong>
        &amp;
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Multilingual Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        POSTECH
       </td>
       <td style="text-align: center;">
        ECCV 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.13442">
         BEAF: Observing BEfore-AFter Changes to Evaluate Hallucination in Vision-language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Vision-Language Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        University College London
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.16470">
         Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Machine Translation
        </strong>
        &amp;
        <strong>
         Hallucination Detection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Cornell University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.17468">
         WILDHALLUCINATIONS: Evaluating Long-form Factuality in LLMs with Real-World Entity Queries
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         WildHallucinations
        </strong>
        &amp;
        <strong>
         Factuality Evaluation
        </strong>
        &amp;
        <strong>
         Real-World Entities
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Columbia University
       </td>
       <td style="text-align: center;">
        ECCV 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.15680">
         HaloQuest: A Visual Hallucination Dataset for Advancing Multimodal Reasoning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Datasets
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        IBM Research
       </td>
       <td style="text-align: center;">
        ICML 2024 Workshop
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.16908">
         Generation Constraint Scaling Can Mitigate Hallucination
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Memory-Augmented Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Harvard-MIT
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.18322">
         The Need for Guardrails with Large Language Models in Medical Safety-Critical Settings: An Artificial Intelligence Application in the Pharmacovigilance Ecosystem
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Pharmacovigilance
        </strong>
        &amp;
        <strong>
         Drug Safety
        </strong>
        &amp;
        <strong>
         Guardrails
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Illinois Institute of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.20224">
         Can Editing LLMs Inject Harm?
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Knowledge Editing
        </strong>
        &amp;
        <strong>
         Misinformation Injection
        </strong>
        &amp;
        <strong>
         Bias Injection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Stanford University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.21417">
         Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Instruction Following
        </strong>
        &amp;
        <strong>
         Faithfulness
        </strong>
        &amp;
        <strong>
         Multi-task Learning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Jilin University
       </td>
       <td style="text-align: center;">
        ACM MM 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.19192">
         Harmfully Manipulated Images Matter in Multimodal Misinformation Detection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Social media
        </strong>
        &amp;
        <strong>
         Misinformation detection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.07
       </td>
       <td style="text-align: center;">
        Zhejiang University
       </td>
       <td style="text-align: center;">
        COLING 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2407.21443">
         Improving Faithfulness of Large Language Models in Summarization via Sliding Generation and Self-Consistency
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Summarization
        </strong>
        &amp;
        <strong>
         Faithfulness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Huazhong University of Science and Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.00550">
         Mitigating Multilingual Hallucination in Large Vision-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Large Vision-Language Models
        </strong>
        &amp;
        <strong>
         Multilingual Hallucination
        </strong>
        &amp;
        <strong>
         Supervised Fine-tuning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Huazhong University of Science and Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.00555">
         Alleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Vision-Language Models (VLMs)
        </strong>
        &amp;
        <strong>
         Active Retrieval Augmentation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        DFKI
       </td>
       <td style="text-align: center;">
        UbiComp Companion '24
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.01168">
         Misinforming LLMs: Vulnerabilities, Challenges and Opportunities
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Misinformation
        </strong>
        &amp;
        <strong>
         Trustworthy AI
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Bar Ilan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.04664">
         Mitigating Hallucinations in Large Vision-Language Models (LVLMs) via Language-Contrastive Decoding (LCD)
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Large Vision-Language Models
        </strong>
        &amp;
        <strong>
         Object Hallucinations
        </strong>
        &amp;
        <strong>
         Language-Contrastive Decoding
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        University of Liverpool
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.05093">
         Order Matters in Hallucination: Reasoning Order as Benchmark and Reflexive Prompting for Large-Language-Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Reasoning Order
        </strong>
        &amp;
        <strong>
         Reflexive Prompting
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        The Alan Turing Institute
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.06731">
         Large Language Models Can Consistently Generate High-Quality Content for Election Disinformation Operations
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Election Disinformation
        </strong>
        &amp;
        <strong>
         DisElect Dataset
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Google DeepMind
       </td>
       <td style="text-align: center;">
        COLM 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.07852">
         Training Language Models on the Knowledge Graph: Insights on Hallucinations and Their Detectability
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Knowledge Graph
        </strong>
        &amp;
        <strong>
         Hallucinations
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        The Hong Kong Polytechnic University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.11871">
         MegaFake: A Theory-Driven Dataset of Fake News Generated by Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Fake News
        </strong>
        &amp;
        <strong>
         MegaFake Dataset
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        IIT Kharagpur
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.12060">
         Evidence-backed Fact Checking using RAG and Few-Shot In-Context Learning with LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Fact Checking
        </strong>
        &amp;
        <strong>
         RAG
        </strong>
        &amp;
        <strong>
         In-Context Learning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Fudan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.12325">
         Improving Factuality in Large Language Models via Decoding-Time Hallucinatory and Truthful Comparators
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Factuality Improvement
        </strong>
        &amp;
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Decoding-Time Intervention
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        The University of Tokyo
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.12326">
         Interactive DualChecker for Mitigating Hallucinations in Distilling Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Knowledge Distillation
        </strong>
        &amp;
        <strong>
         Large Language Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        University of Surrey
       </td>
       <td style="text-align: center;">
        IJCAI 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.08333">
         CodeMirage: Hallucinations in Code Generated by Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Code Hallucinations
        </strong>
        &amp;
        <strong>
         CodeMirage Dataset
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Sichuan Normal University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.13184">
         Can LLM Be a Good Path Planner Based on Prompt Engineering? Mitigating the Hallucination for Path Planning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Path Planning
        </strong>
        &amp;
        <strong>
         Spatial Reasoning
        </strong>
        &amp;
        <strong>
         Hallucination Mitigation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Alibaba Cloud
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.15533">
         LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via Layer-wise Relevance Propagation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         RAG
        </strong>
        &amp;
        <strong>
         Layer-wise Relevance Propagation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.08
       </td>
       <td style="text-align: center;">
        Royal Holloway, University of London
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2408.16081">
         Logic-Enhanced Language Model Agents for Trustworthy Social Simulations
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Social Simulations
        </strong>
        &amp;
        <strong>
         Trustworthy AI
        </strong>
        &amp;
        <strong>
         Game Theory
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Inria, University of Rennes
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.00159">
         LLMs hallucinate graphs too: a structural perspective
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Graph Analysis
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Scale AI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.00238">
         Pre-Training Multimodal Hallucination Detectors with Corrupted Grounding Data
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal Hallucination
        </strong>
        &amp;
        <strong>
         Grounding Data
        </strong>
        &amp;
        <strong>
         Sample Efficiency
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Fudan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.01787">
         LLM-GAN: Construct Generative Adversarial Network Through Large Language Models For Explainable Fake News Detection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Explainable Fake News Detection
        </strong>
        &amp;
        <strong>
         Generative Adversarial Network
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        University of Oslo
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.02976">
         Hallucination Detection in LLMs: Fast and Memory-Efficient Fine-tuned Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Memory Efficiency
        </strong>
        &amp;
        <strong>
         Ensemble Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Univ. Polytechnique Hauts-de-France
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.03109">
         FIDAVL: Fake Image Detection and Attribution using Vision-Language Model
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Fake Image Detection
        </strong>
        &amp;
        <strong>
         Vision-Language Model
        </strong>
        &amp;
        <strong>
         Synthetic Image Attribution
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        EPFL
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.03291">
         LLM Detectors Still Fall Short of Real World: Case of LLM-Generated Short News-Like Posts
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Detectors
        </strong>
        &amp;
        <strong>
         Disinformation
        </strong>
        &amp;
        <strong>
         Adversarial Evasion
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Geely Automobile Research Institute, Beihang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.06601">
         Alleviating Hallucinations in Large Language Models with Scepticism Modeling
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucinations
        </strong>
        &amp;
        <strong>
         Scepticism Modeling
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        AppCubic, Georgia Institute of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.08087">
         Securing Large Language Models: Addressing Bias, Misinformation, and Prompt Attacks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Misinformation
        </strong>
        &amp;
        <strong>
         Jailbreak Attacks
        </strong>
        &amp;
        <strong>
         Prompt Injection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Carnegie Mellon University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.09013">
         AI-LIEDAR: Examine the Trade-off Between Utility and Truthfulness in LLM Agents
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Utility
        </strong>
        &amp;
        <strong>
         Truthfulness
        </strong>
        &amp;
        <strong>
         LLM Agents
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Salesforce AI Research
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.09916">
         SFR-RAG: Towards Contextually Faithful LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Retrieval Augmented Generation
        </strong>
        &amp;
        <strong>
         Contextual Comprehension
        </strong>
        &amp;
        <strong>
         Hallucination Minimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        University of North Texas
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.10011">
         HALO: Hallucination Analysis and Learning Optimization to Empower LLMs with Retrieval-Augmented Context for Guided Clinical Decision Making
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Retrieval Augmented Generation
        </strong>
        &amp;
        <strong>
         Medical Question Answering
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Tsinghua University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.10102">
         Trustworthiness in Retrieval-Augmented Generation Systems: A Survey
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Trustworthiness
        </strong>
        &amp;
        <strong>
         RAG
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        National University of Defense Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.11283">
         Zero-resource Hallucination Detection for Text Generation via Graph-based Contextual Knowledge Triples Modeling
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Zero-resource Hallucination Detection
        </strong>
        &amp;
        <strong>
         Text Generation
        </strong>
        &amp;
        <strong>
         Graph-based Knowledge Triples
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        The University of Manchester
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.16452">
         FMDLlama: Financial Misinformation Detection based on Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Financial Misinformation Detection
        </strong>
        &amp;
        <strong>
         Instruction Tuning
        </strong>
        &amp;
        <strong>
         FMDLlama
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        University of Montreal
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.17416">
         From Deception to Detection: The Dual Roles of Large Language Models in Fake News
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Fake News
        </strong>
        &amp;
        <strong>
         Fake News Detection
        </strong>
        &amp;
        <strong>
         Bias Mitigation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Korea University
       </td>
       <td style="text-align: center;">
        EMNLP 2024 Findings
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.16658">
         Pre-trained Language Models Return Distinguishable Probability Distributions to Unfaithfully Hallucinated Texts
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Unfaithful Texts
        </strong>
        &amp;
        <strong>
         Uncertainty Distribution
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        The University of Texas at Dallas
       </td>
       <td style="text-align: center;">
        TMLR
       </td>
       <td style="text-align: center;">
        <a href="https://openreview.net/forum?id=ZVDWzgk6L6">
         A Unified Hallucination Mitigation Framework for Large Vision-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Reasoning Queries
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        The Chinese University of Hong Kong
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.18786">
         A Survey on the Honesty of Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Honesty
        </strong>
        &amp;
        <strong>
         Self-knowledge
        </strong>
        &amp;
        <strong>
         Self-expression
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        University of Surrey
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.19492">
         MEDHALU: Hallucinations in Responses to Healthcare Queries by Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Hallucinations
        </strong>
        &amp;
        <strong>
         Healthcare Queries
        </strong>
        &amp;
        <strong>
         Hallucination Detection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Harvard Medical School
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.20385">
         Wait, but Tylenol is Acetaminophen‚Ä¶ Investigating and Improving Language Models' Ability to Resist Requests for Misinformation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Misinformation Resistance
        </strong>
        &amp;
        <strong>
         Healthcare
        </strong>
        &amp;
        <strong>
         Instruction Tuning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Nanjing University of Aeronautics and Astronautics
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.20429">
         HELPD: Mitigating Hallucination of LVLMs by Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         LVLMs
        </strong>
        &amp;
        <strong>
         Feedback Learning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.09
       </td>
       <td style="text-align: center;">
        Sun Yat-sen University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2409.20550">
         LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Hallucinations
        </strong>
        &amp;
        <strong>
         Code Generation
        </strong>
        &amp;
        <strong>
         Mitigation Strategies
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Technion
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.02707">
         LLMS KNOW MORE THAN THEY SHOW: ON THE INTRINSIC REPRESENTATION OF LLM HALLUCINATIONS
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Hallucinations
        </strong>
        &amp;
        <strong>
         Error Detection
        </strong>
        &amp;
        <strong>
         Truthfulness Encoding
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Meta
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.02825">
         Ingest-And-Ground: Dispelling Hallucinations from Continually-Pretrained LLMs with RAG
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         RAG
        </strong>
        &amp;
        <strong>
         Hallucination
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        IBM Research
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.06703">
         ST-WebAgentBench: A Benchmark for Evaluating Safety and Trustworthiness in Web Agents
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Web Agents
        </strong>
        &amp;
        <strong>
         Safety
        </strong>
        &amp;
        <strong>
         Trustworthiness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        National University of Singapore
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.08085">
         Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study Over Open-ended Question Answering
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Knowledge Graphs
        </strong>
        &amp;
        <strong>
         Trustworthiness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Tongji University
       </td>
       <td style="text-align: center;">
        EMNLP 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.04514">
         DAMRO: Dive into the Attention Mechanism of LVLM to Reduce Object Hallucination
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LVLM
        </strong>
        &amp;
        <strong>
         Object Hallucination
        </strong>
        &amp;
        <strong>
         Attention Mechanism
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        The University of Sydney, The University of Hong Kong
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.08970">
         NOVO: Norm Voting Off Hallucinations with Attention Heads in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination mitigation
        </strong>
        &amp;
        <strong>
         Attention heads
        </strong>
        &amp;
        <strong>
         Norm voting
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Purdue University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.09997">
         COLLU-BENCH: A Benchmark for Predicting Language Model Hallucinations in Code
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Code hallucinations
        </strong>
        &amp;
        <strong>
         Code generation
        </strong>
        &amp;
        <strong>
         Automated program repair
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        National University of Sciences and Technology, Rawalpindi Medical University, King Faisal University, Sejong University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.10853">
         Mitigating Hallucinations Using Ensemble of Knowledge Graph and Vector Store in Large Language Models to Enhance Mental Health Support
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination mitigation
        </strong>
        &amp;
        <strong>
         Knowledge graphs
        </strong>
        &amp;
        <strong>
         Mental health support
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Renmin University of China, Kuaishou Technology Co., Ltd., University of International Business and Economics
       </td>
       <td style="text-align: center;">
        ICLR 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.11414">
         ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via Mechanistic Interpretability
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Retrieval-Augmented Generation (RAG)
        </strong>
        &amp;
        <strong>
         Hallucination detection
        </strong>
        &amp;
        <strong>
         Mechanistic interpretability
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Zhejiang University, National University of Singapore
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.11779">
         MLLM CAN SEE? Dynamic Correction Decoding for Hallucination Mitigation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination mitigation
        </strong>
        &amp;
        <strong>
         Multimodal LLMs
        </strong>
        &amp;
        <strong>
         Dynamic correction decoding
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Vectara, Inc., Iowa State University, University of Southern California, Entropy Technologies, University of Waterloo, Funix.io, University of Wisconsin, Madison
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.13210">
         FaithBench: A Diverse Hallucination Benchmark for Summarization by Modern LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination detection
        </strong>
        &amp;
        <strong>
         Human-annotated benchmark
        </strong>
        &amp;
        <strong>
         Faithfulness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Harbin Institute of Technology (Shenzhen), Huawei Cloud
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.10408">
         MEDICO: Towards Hallucination Detection and Correction with Multi-source Evidence Fusion
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination detection
        </strong>
        &amp;
        <strong>
         Multi-source evidence fusion
        </strong>
        &amp;
        <strong>
         Hallucination correction
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Independent Researchers
       </td>
       <td style="text-align: center;">
        KDD 2024 RAG Workshop
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.09699">
         Honest AI: Fine-Tuning "Small" Language Models to Say "I Don‚Äôt Know", and Reducing Hallucination in RAG
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination reduction
        </strong>
        &amp;
        <strong>
         Small LLMs
        </strong>
        &amp;
        <strong>
         False premise
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        University of California Irvine
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.13961">
         From Single to Multi: How LLMs Hallucinate in Multi-Document Summarization
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multi-Document Summarization
        </strong>
        &amp;
        <strong>
         LLM Hallucination
        </strong>
        &amp;
        <strong>
         Benchmarking
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Harvard University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.14262">
         Good Parenting is All You Need: Multi-agentic LLM Hallucination Mitigation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Hallucination
        </strong>
        &amp;
        <strong>
         Multi-agent Systems
        </strong>
        &amp;
        <strong>
         Self-reflection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        University of Science and Technology of China
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.15116">
         Coarse-to-Fine Highlighting: Reducing Knowledge Hallucination in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Knowledge Hallucination
        </strong>
        &amp;
        <strong>
         Retrieval-Augmented Language Models
        </strong>
        &amp;
        <strong>
         Highlighting Techniques
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        McGill University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.15460">
         Hallucination Detox: Sensitive Neuron Dropout (SEND) for Large Language Model Training
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Sensitive Neurons
        </strong>
        &amp;
        <strong>
         Training Protocols
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        National Taiwan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.16130">
         Can Large Audio-Language Models Truly Hear? Tackling Hallucinations with Multi-Task Assessment and Stepwise Audio Reasoning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Audio-Language Models
        </strong>
        &amp;
        <strong>
         Hallucination Analysis
        </strong>
        &amp;
        <strong>
         Multi-Task Evaluation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Mila - Quebec AI Institute
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.18270">
         Multilingual Hallucination Gaps in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multilingual Hallucination
        </strong>
        &amp;
        <strong>
         FACTSCORE
        </strong>
        &amp;
        <strong>
         Low-Resource Languages
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        University of Edinburgh
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.18860">
         DECORE: Decoding by Contrasting Retrieval Heads to Mitigate Hallucinations
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Contrastive Decoding
        </strong>
        &amp;
        <strong>
         Retrieval Heads
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        University of Science and Technology of China
       </td>
       <td style="text-align: center;">
        EMNLP 2024 Findings
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.15702">
         Mitigating Hallucinations of Large Language Models in Medical Information Extraction via Contrastive Decoding
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Medical Information Extraction
        </strong>
        &amp;
        <strong>
         Contrastive Decoding
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        University of Science and Technology of China
       </td>
       <td style="text-align: center;">
        ICML 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.16843">
         Trustworthy Alignment of Retrieval-Augmented Large Language Models via Reinforcement Learning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Trustworthy Alignment
        </strong>
        &amp;
        <strong>
         Reinforcement Learning
        </strong>
        &amp;
        <strong>
         Retrieval-Augmented Generation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Intel Labs
       </td>
       <td style="text-align: center;">
        NeurIPS 2024 Workshop on SafeGenAI
       </td>
       <td style="text-align: center;">
        Debiasing Large Vision-Language Models by Ablating Protected Attribute Representations
       </td>
       <td style="text-align: center;">
        <strong>
         Debiasing
        </strong>
        &amp;
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Attribute Ablation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        The Pennsylvania State University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.19250">
         The Reopening of Pandora‚Äôs Box: Analyzing the Role of LLMs in the Evolving Battle Against AI-Generated Fake News
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Fake News Detection
        </strong>
        &amp;
        <strong>
         Human-AI Collaboration
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Stellenbosch University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.19385">
         Investigating the Role of Prompting and External Tools in Hallucination Rates of Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Prompt Engineering
        </strong>
        &amp;
        <strong>
         External Tools
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Algoverse AI Research
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.19485">
         A Debate-Driven Experiment on LLM Hallucinations and Accuracy
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Hallucinations
        </strong>
        &amp;
        <strong>
         Accuracy Improvement
        </strong>
        &amp;
        <strong>
         Model Interaction
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Narrative BI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.20024">
         Beyond Fine-Tuning: Effective Strategies for Mitigating Hallucinations in Large Language Models for Data Analytics
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Data Analytics
        </strong>
        &amp;
        <strong>
         Prompt Engineering
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        HKUST (GZ)
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.20340">
         Maintaining Informative Coherence: Migrating Hallucinations in Large Language Models via Absorbing Markov Chains
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Markov Chains
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        National Taiwan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.20833">
         LLMs are Biased Evaluators But Not Biased for Retrieval Augmented Generation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Bias Analysis
        </strong>
        &amp;
        <strong>
         LLM Evaluation
        </strong>
        &amp;
        <strong>
         Retrieval-Augmented Generation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        University Hospital Leipzig
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.21330">
         LLM Robustness Against Misinformation in Biomedical Question Answering
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Biomedical Question Answering
        </strong>
        &amp;
        <strong>
         Robustness
        </strong>
        &amp;
        <strong>
         Misinformation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        Technion ‚Äì Israel Institute of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.22071">
         Distinguishing Ignorance from Error in LLM Hallucinations
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Hallucinations
        </strong>
        &amp;
        <strong>
         Error Classification
        </strong>
        &amp;
        <strong>
         Knowledge Detection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        The Hong Kong University of Science and Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.23114">
         Unified Triplet-Level Hallucination Evaluation for Large Vision-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Hallucination Evaluation
        </strong>
        &amp;
        <strong>
         Relation Analysis
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.10
       </td>
       <td style="text-align: center;">
        University of Notre Dame, MBZUAI, IBM Research, UW, Peking University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2410.02736">
         Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Bias
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        New York University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.00878">
         Exploring the Knowledge Mismatch Hypothesis: Hallucination Propensity in Small Models Fine-tuned on Data from Larger Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Knowledge Mismatch
        </strong>
        &amp;
        <strong>
         Fine-tuning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        Nankai University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.04847">
         Prompt-Guided Internal States for Hallucination Detection of Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Prompt-Guided Internal States
        </strong>
        &amp;
        <strong>
         Cross-Domain Generalization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        Georgia Institute of Technology
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.09689">
         LLM Hallucination Reasoning with Zero-shot Knowledge Test
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Zero-shot Methods
        </strong>
        &amp;
        <strong>
         Model Knowledge Test
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        Shanghai Jiao Tong University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.09968">
         Seeing Clearly by Layer Two: Enhancing Attention Heads to Alleviate Hallucination in LVLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal Large Language Models
        </strong>
        &amp;
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Attention Mechanism
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        Renmin University of China
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.10436">
         Mitigating Hallucination in Multimodal Large Language Models via Hallucination-targeted Direct Preference Optimization
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal Large Language Models
        </strong>
        &amp;
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Direct Preference Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        AIRI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.11531">
         Addressing Hallucinations in Language Models with Knowledge Graph Embeddings as an Additional Modality
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Knowledge Graphs
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        University of Pennsylvania
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.12591">
         Thinking Before Looking: Improving Multimodal LLM Reasoning via Mitigating Visual Hallucination
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal Large Language Models
        </strong>
        &amp;
        <strong>
         Visual Hallucination
        </strong>
        &amp;
        <strong>
         Reasoning Accuracy
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        Tsinghua University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.12713">
         CATCH: Complementary Adaptive Token-level Contrastive Decoding to Mitigate Hallucinations in LVLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Large Vision-Language Models
        </strong>
        &amp;
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Contrastive Decoding
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        Stony Brook University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.12759">
         A Novel Approach to Eliminating Hallucinations in Large Language Model-Assisted Causal Discovery
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Causal Discovery
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        ETH Z√ºrich
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.14257">
         Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucinations
        </strong>
        &amp;
        <strong>
         Knowledge Awareness
        </strong>
        &amp;
        <strong>
         Sparse Autoencoders
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        Aalborg University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.14258">
         Knowledge Graphs, Large Language Models, and Hallucinations: An NLP Perspective
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Knowledge Graphs
        </strong>
        &amp;
        <strong>
         Hallucinations
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        China Telecom Shanghai Company, Ferret Relationship Intelligence
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.16189">
         Enhancing Multi-Agent Consensus through Third-Party LLM Integration: Analyzing Uncertainty and Mitigating Hallucinations in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multi-Agent Systems
        </strong>
        &amp;
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Uncertainty Analysis
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        Salah Boubnider University, Abdelhamid Mehri University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.02790">
         An Evolutionary Large Language Model for Hallucination Mitigation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Evolutionary Computation
        </strong>
        &amp;
        <strong>
         Hallucination Mitigation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        Adobe Research, IIT Kanpur, IIT Bombay
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.19187">
         Beyond Logit Lens: Contextual Embeddings for Robust Hallucination Detection &amp; Grounding in VLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Contextual Embeddings
        </strong>
        &amp;
        <strong>
         Multimodal Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.11
       </td>
       <td style="text-align: center;">
        Tsinghua University, Tencent, University of Science and Technology Beijing, University of Macau
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2411.18659">
         DHCP: Detecting Hallucinations by Cross-modal Attention Patterns in Large Vision-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Cross-modal Attention
        </strong>
        &amp;
        <strong>
         Vision-Language Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        Acurai, Inc.
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.05223">
         Hallucination Elimination Using Acurai
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         AI Trustworthiness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        National Yang Ming Chiao Tung University, Atmanity
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.06775">
         Delve into Visual Contrastive Decoding for Hallucination Mitigation of Large Vision-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Visual Contrastive Decoding
        </strong>
        &amp;
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Vision-Language Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        Amazon Alexa AI, Cambridge, UK
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.07965">
         HALLUCANA: Fixing LLM Hallucination with A Canary Lookahead
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Lookahead Strategy
        </strong>
        &amp;
        <strong>
         LLM Factuality
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        Peking University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.14905">
         Dehallucinating Parallel Context Extension for Retrieval-Augmented Generation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         In-Context Hallucination
        </strong>
        &amp;
        <strong>
         Retrieval-Augmented Generation (RAG)
        </strong>
        &amp;
        <strong>
         Parallel Context Extension (PCE)
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        AE Studio
       </td>
       <td style="text-align: center;">
        NeurIPS 2024 SafeGenAI Workshop
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.16325">
         Towards Safe and Honest AI Agents with Neural Self-Other Overlap
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         AI Safety
        </strong>
        &amp;
        <strong>
         Neural Self-Other Overlap
        </strong>
        &amp;
        <strong>
         Deceptive Behavior Mitigation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        EPFL
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.18022">
         Trustworthy and Efficient LLMs Meet Databases
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Trustworthy LLMs
        </strong>
        &amp;
        <strong>
         Efficient LLM Inference
        </strong>
        &amp;
        <strong>
         LLMs and Databases
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        University of M√ºnster
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.17056">
         The HalluRAG Dataset: Detecting Closed-Domain Hallucinations in RAG Applications Using an LLM‚Äôs Internal States
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Retrieval-Augmented Generation (RAG)
        </strong>
        &amp;
        <strong>
         Internal States
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        Purdue University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.15269">
         The Reliability Paradox: Exploring How Shortcut Learning Undermines Language Model Calibration
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Calibration
        </strong>
        &amp;
        <strong>
         Shortcut Learning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        24.12
       </td>
       <td style="text-align: center;">
        IIT Patna
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2412.18672">
         From Hallucinations to Facts: Enhancing Language Models with Curated Knowledge Graphs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Knowledge Graphs
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Brunel University London
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.00269">
         A Review of Faithfulness Metrics for Hallucination Assessment in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Faithfulness Evaluation
        </strong>
        &amp;
        <strong>
         Hallucination Assessment
        </strong>
        &amp;
        <strong>
         Large Language Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        University of Notre Dame, Deloitte &amp; Touche LLP
       </td>
       <td style="text-align: center;">
        AAAI 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.01303">
         Citations and Trust in LLM Generated Answers
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         User Trust
        </strong>
        &amp;
        <strong>
         Citations in LLMs
        </strong>
        &amp;
        <strong>
         Social Proof Theory
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Singapore Management University, National University of Singapore
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.01336">
         Aligning Large Language Models for Faithful Integrity Against Opposing Argument
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Faithful Integrity
        </strong>
        &amp;
        <strong>
         Confidence Estimation
        </strong>
        &amp;
        <strong>
         Direct Preference Optimization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Harvard University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.00418">
         Generalizing Trust: Weak-to-Strong Trustworthiness in Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Trustworthiness Generalization
        </strong>
        &amp;
        <strong>
         Weak-to-Strong Transfer
        </strong>
        &amp;
        <strong>
         Language Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Carnegie Mellon University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.00745">
         Dynamics of Adversarial Attacks on Large Language Model-Based Search Engines
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Adversarial Attacks
        </strong>
        &amp;
        <strong>
         Ranking Manipulation
        </strong>
        &amp;
        <strong>
         Game Theory
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Imperial College London
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.00879">
         TrustRAG: Enhancing Robustness and Trustworthiness in RAG
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Robust Retrieval-Augmented Generation
        </strong>
        &amp;
        <strong>
         Corpus Poisoning Defense
        </strong>
        &amp;
        <strong>
         Knowledge Conflict Resolution
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Renmin University of China
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.01306">
         Think More, Hallucinate Less: Mitigating Hallucinations via Dual Process of Fast and Slow Thinking
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Dual Process Theory
        </strong>
        &amp;
        <strong>
         Monte Carlo Tree Search
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Tsinghua University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.09431">
         A Survey on Responsible LLMs: Inherent Risk, Malicious Use, and Mitigation Strategy
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Responsible LLMs
        </strong>
        &amp;
        <strong>
         Privacy
        </strong>
        &amp;
        <strong>
         Hallucination
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        University of Science and Technology of China
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.08985">
         Personality Modeling for Persuasion of Misinformation using AI Agent
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Personality Traits
        </strong>
        &amp;
        <strong>
         Misinformation Dynamics
        </strong>
        &amp;
        <strong>
         Agent-Based Modeling
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        University of Washington
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.08292">
         HALOGEN: Fantastic LLM Hallucinations and Where to Find Them
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Benchmark
        </strong>
        &amp;
        <strong>
         Automatic Verifiers
        </strong>
        &amp;
        <strong>
         LLM Factuality
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.09997">
         Attention-guided Self-reflection for Zero-shot Hallucination Detection in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Zero-shot Hallucination Detection
        </strong>
        &amp;
        <strong>
         Attention Mechanisms
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Carnegie Mellon University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.11967">
         A Hybrid Attention Framework for Fake News Detection with Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Fake News Detection
        </strong>
        &amp;
        <strong>
         Hybrid Attention Mechanism
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Virginia Tech
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.12206">
         Fixing Imbalanced Attention to Mitigate In-Context Hallucination of Large Vision-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Attention Mechanisms
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        McGill University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.12975">
         OnionEval: A Unified Evaluation of Fact-conflicting Hallucination for Small-Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Fact-conflicting Hallucination
        </strong>
        &amp;
        <strong>
         Small-Large Language Models (SLLMs)
        </strong>
        &amp;
        <strong>
         Benchmark
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Harbin Institute of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.13573">
         Improving Contextual Faithfulness of Large Language Models via Retrieval Heads-Induced Optimization
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Contextual Faithfulness
        </strong>
        &amp;
        <strong>
         Retrieval-Augmented Models
        </strong>
        &amp;
        <strong>
         Long-Form Question Answering
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Dresden University of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.13824">
         Hallucinations Can Improve Large Language Models in Drug Discovery
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucinations
        </strong>
        &amp;
        <strong>
         Drug Discovery
        </strong>
        &amp;
        <strong>
         SMILES Strings
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        China Telecom Shanghai Company
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.13942">
         Prompt-Based Monte Carlo Tree Search for Mitigating Hallucinations in Large Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Monte Carlo Tree Search
        </strong>
        &amp;
        <strong>
         Hallucinations
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        XCALLY, Linux Foundation AI &amp; Data
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.13946">
         Hallucination Mitigation Using Agentic AI Natural Language-Based Frameworks
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Agentic AI
        </strong>
        &amp;
        <strong>
         LLM Hallucination
        </strong>
        &amp;
        <strong>
         AI Interoperability
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        Boston University, Apple
       </td>
       <td style="text-align: center;">
        NAACL 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.17295">
         Mitigating Hallucinated Translations in Large Language Models with Hallucination-focused Preference Optimization
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Hallucinations
        </strong>
        &amp;
        <strong>
         Preference Optimization
        </strong>
        &amp;
        <strong>
         Contrastive Fine-tuning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.01
       </td>
       <td style="text-align: center;">
        University of Waterloo
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2501.19012">
         Importing Phantoms: Measuring LLM Package Hallucination Vulnerabilities
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Security
        </strong>
        &amp;
        <strong>
         Package Hallucination
        </strong>
        &amp;
        <strong>
         Software Supply Chain
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Shanghai University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.01056">
         Mitigating Hallucinations in Large Vision-Language Models with Internal Fact-based Contrastive Decoding
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Contrastive Decoding
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Wuhan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.01386">
         Topic-FlipRAG: Topic-Orientated Adversarial Opinion Manipulation Attacks to Retrieval-Augmented Generation Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Retrieval-Augmented Generation
        </strong>
        &amp;
        <strong>
         Opinion Manipulation
        </strong>
        &amp;
        <strong>
         Adversarial Attacks
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Leibniz University Hannover
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.01812">
         SelfCheckAgent: Zero-Resource Hallucination Detection in Generative Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Zero-Resource Evaluation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        University of Sydney
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.01969">
         Mitigating Object Hallucinations in Large Vision-Language Models via Attention Calibration
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Object Hallucination
        </strong>
        &amp;
        <strong>
         Attention Calibration
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Rutgers University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.03628">
         The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Token-Logit Analysis
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Qualcomm AI Research
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.03799">
         Enhancing Hallucination Detection through Noise Injection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Uncertainty Estimation
        </strong>
        &amp;
        <strong>
         Noise Injection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        University of Toronto
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.04360">
         MARAGE: Transferable Multi-Model Adversarial Attack for Retrieval-Augmented Generation Data Extraction
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Retrieval-Augmented Generation
        </strong>
        &amp;
        <strong>
         RAG
        </strong>
        &amp;
        <strong>
         Adversarial Attack
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Pennsylvania State University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.04556">
         TruthFlow: Truthful LLM Generation via Representation Flow Correction
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Truthfulness
        </strong>
        &amp;
        <strong>
         Representation Intervention
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        National Taiwan University of Science and Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.05825">
         DELTA - Contrastive Decoding Mitigates Text Hallucinations in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Contrastive Decoding
        </strong>
        &amp;
        <strong>
         Text Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         LLM Reliability
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Shanghai Artificial Intelligence Laboratory
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.05911">
         GRAIT: Gradient-Driven Refusal-Aware Instruction Tuning for Effective Hallucination Mitigation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Refusal-Aware Instruction Tuning
        </strong>
        &amp;
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Gradient-Based Learning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Fraunhofer Institute for Integrated Circuits (IIS)
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.06652">
         Transparent NLP: Using RAG and LLM Alignment for Privacy Q&amp;A
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Retrieval-Augmented Generation (RAG)
        </strong>
        &amp;
        <strong>
         LLM Alignment
        </strong>
        &amp;
        <strong>
         Privacy Q&amp;A
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Vanderbilt University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.06872">
         Towards Trustworthy Retrieval Augmented Generation for Large Language Models: A Survey
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Retrieval-Augmented Generation (RAG)
        </strong>
        &amp;
        <strong>
         Trustworthy AI
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Tsinghua University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.07340">
         Aligning Large Language Models to Follow Instructions and Hallucinate Less via Effective Data Filtering
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Instruction Tuning
        </strong>
        &amp;
        <strong>
         Hallucination Reduction
        </strong>
        &amp;
        <strong>
         Data Filtering
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Inha University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.08109">
         HUDEX: Integrating Hallucination Detection and Explainability for Enhancing the Reliability of LLM Responses
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Explainability
        </strong>
        &amp;
        <strong>
         LLM Reliability
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Huazhong Agricultural University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.08353">
         Trustworthy GNNs with LLMs: A Systematic Review and Taxonomy
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Graph Neural Networks (GNNs)
        </strong>
        &amp;
        <strong>
         Trustworthiness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Stony Brook University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.08514">
         Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial Stance for Summary Evaluation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Faithfulness Evaluation
        </strong>
        &amp;
        <strong>
         Multi-Agent Debate
        </strong>
        &amp;
        <strong>
         Summarization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        KAUST, University of Pisa
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.08663">
         Hallucination Detection: A Probabilistic Framework Using Embeddings Distance Analysis
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Embedding Distance
        </strong>
        &amp;
        <strong>
         LLM Evaluation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        University of Pennsylvania
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.08666">
         Hallucination, Monofacts, and Miscalibration: An Empirical Investigation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Monofact Rate
        </strong>
        &amp;
        <strong>
         Miscalibration
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Tsinghua University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.08904">
         MIH-TCCT: Mitigating Inconsistent Hallucinations in LLMs via Event-Driven Text-Code Cyclic Training
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Event-Driven Training
        </strong>
        &amp;
        <strong>
         Logical Consistency
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        University of the Basque Country
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.09387">
         Truth Knows No Language: Evaluating Truthfulness Beyond English
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Truthfulness Evaluation
        </strong>
        &amp;
        <strong>
         Multilingual LLMs
        </strong>
        &amp;
        <strong>
         Benchmarking
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Amazon
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.10497">
         Hallucinations and Truth: A Comprehensive Accuracy Evaluation of RAG, LoRA and DORA
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         RAG
        </strong>
        &amp;
        <strong>
         LoRA
        </strong>
        &amp;
        <strong>
         DoRA
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        University of Southern California
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.11306">
         Smoothing Out Hallucinations: Mitigating LLM Hallucination with Smoothed Knowledge Distillation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Knowledge Distillation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        The University of Hong Kong
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.12187">
         Hallucinations are Inevitable but Statistically Negligible
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Probability Theory
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        University of W√ºrzburg
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.12769">
         How Much Do LLMs Hallucinate across Languages? On Multilingual Estimation of LLM Hallucination in the Wild
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Multilingual LLMs
        </strong>
        &amp;
        <strong>
         Factuality
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Technion ‚Äì Israel Institute of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.12964">
         Trust Me, I‚Äôm Wrong: High-Certainty Hallucinations in LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Model Certainty
        </strong>
        &amp;
        <strong>
         LLM Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Mila
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.13369">
         Reducing Hallucinations in Language Model-based SPARQL Query Generation Using Post-Generation Memory Retrieval
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         SPARQL Query Generation
        </strong>
        &amp;
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Memory Retrieval
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Huazhong University of Science and Technology
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.13416">
         Detecting LLM Fact-conflicting Hallucinations Enhanced by Temporal-logic-based Reasoning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Temporal Logic
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Columbia University
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.13442">
         TREECUT: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Math Word Problem
        </strong>
        &amp;
        <strong>
         LLM Hallucination
        </strong>
        &amp;
        <strong>
         Synthetic Dataset
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Pohang University of Science and Technology (POSTECH)
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.13622">
         REFIND: Retrieval-Augmented Factuality Hallucination Detection in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Retrieval-Augmented Model
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Michigan State University
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.14182">
         Multi-Faceted Studies on Data Poisoning can Advance LLM Development
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Data Poisoning
        </strong>
        &amp;
        <strong>
         LLM Security
        </strong>
        &amp;
        <strong>
         Trustworthy AI
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        University of Texas at Austin
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.14302">
         MedHallu: A Comprehensive Benchmark for Detecting Medical Hallucinations in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Medical Hallucination
        </strong>
        &amp;
        <strong>
         LLM Evaluation
        </strong>
        &amp;
        <strong>
         Benchmark
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Cornell University
       </td>
       <td style="text-align: center;">
        ICLP 2024
       </td>
       <td style="text-align: center;">
        <a href="https://doi.org/10.4204/EPTCS.416.5">
         LP-LM: No Hallucinations in Question Answering with Logic Programming
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Logic Programming
        </strong>
        &amp;
        <strong>
         Hallucination-Free QA
        </strong>
        &amp;
        <strong>
         Prolog
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Carnegie Mellon University
       </td>
       <td style="text-align: center;">
        ICLR 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.06130">
         Self-Correcting Decoding with Generative Feedback for Mitigating Hallucinations in Large Vision-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Large Vision-Language Models
        </strong>
        &amp;
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Generative Feedback
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Wroclaw University of Science and Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.17598">
         Hallucination Detection in LLMs Using Spectral Features of Attention Maps
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Spectral Features
        </strong>
        &amp;
        <strong>
         Attention Maps
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        University of Illinois Urbana-Champaign
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.16143">
         The Law of Knowledge Overshadowing: Towards Understanding, Predicting, and Preventing LLM Hallucination
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Knowledge Overshadowing
        </strong>
        &amp;
        <strong>
         LLM Factuality
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        University of California, Los Angeles
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.15845">
         Verify when Uncertain: Beyond Self-Consistency in Black Box Hallucination Detection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Self-Consistency
        </strong>
        &amp;
        <strong>
         Cross-Model Verification
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        King's College London
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.15844">
         Hallucination Detection in Large Language Models with Metamorphic Relations
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Metamorphic Relations
        </strong>
        &amp;
        <strong>
         LLM Reliability
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        The Ohio State University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.19545">
         Winning Big with Small Models: Knowledge Distillation vs. Self-Training for Reducing Hallucination in QA Agents
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Knowledge Distillation
        </strong>
        &amp;
        <strong>
         Self-Training
        </strong>
        &amp;
        <strong>
         Hallucination Reduction
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        University of Seoul
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.20034">
         Vision-Encoders (Already) Know What They See: Mitigating Object Hallucination via Simple Fine-Grained CLIPScore
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Object Hallucination
        </strong>
        &amp;
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         CLIPScore
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Ant Group
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.19209">
         Bi‚Äôan: A Bilingual Benchmark and Model for Hallucination Detection in Retrieval-Augmented Generation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Retrieval-Augmented Generation
        </strong>
        &amp;
        <strong>
         Bilingual Benchmark
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        University of Maryland, Baltimore County
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.18536">
         FilterRAG: Zero-Shot Informed Retrieval-Augmented Generation to Mitigate Hallucinations in VQA
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Visual Question Answering
        </strong>
        &amp;
        <strong>
         Retrieval-Augmented Generation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        Guangzhou University, Beihang University
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.20750">
         Mitigating Hallucinations in Large Vision-Language Models by Adaptively Constraining Information Flow
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Variational Information Bottleneck
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.02
       </td>
       <td style="text-align: center;">
        The Chinese University of Hong Kong, Shanghai Artificial Intelligence Laboratory
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2502.20780">
         MedHallTune: An Instruction-Tuning Benchmark for Mitigating Medical Hallucination in Vision-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Medical Hallucination
        </strong>
        &amp;
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Large Vision-Language Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        University of Oxford, Oxford University Hospitals NHS Foundation Trust
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.00269">
         Reducing Large Language Model Safety Risks in Women‚Äôs Health using Semantic Entropy
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Semantic Entropy
        </strong>
        &amp;
        <strong>
         Women‚Äôs Health
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Northwestern Polytechnical University, Swansea University
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.00361">
         Octopus: Alleviating Hallucination via Dynamic Contrastive Decoding
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Contrastive Decoding
        </strong>
        &amp;
        <strong>
         Large Vision-Language Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        National Technical University of Athens
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.00436">
         HalCECE: A Framework for Explainable Hallucination Detection through Conceptual Counterfactuals in Image Captioning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Explainable Evaluation
        </strong>
        &amp;
        <strong>
         Vision-Language Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        King‚Äôs College London, Cambridge University
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.01670">
         Evaluating LLMs‚Äô Assessment of Mixed-Context Hallucination Through the Lens of Summarization
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Mixed-Context Hallucination
        </strong>
        &amp;
        <strong>
         Hallucination Evaluation
        </strong>
        &amp;
        <strong>
         Summarization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Capital One
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.01742">
         Building Safe GenAI Applications: An End-to-End Overview of Red Teaming for Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Red Teaming
        </strong>
        &amp;
        <strong>
         AI Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        The Pennsylvania State University, GE Healthcare
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.02157">
         MedHEval: Benchmarking Hallucinations and Mitigation Strategies in Medical Large Vision-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Medical Hallucination
        </strong>
        &amp;
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Vision-Language Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Georgia Institute of Technology, Wuhan University
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.02851">
         Shakespearean Sparks: The Dance of Hallucination and Creativity in LLMs‚Äô Decoding Layers
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Creativity
        </strong>
        &amp;
        <strong>
         Decoding Layers
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Pennsylvania State University
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.03106">
         Monitoring Decoding: Mitigating Hallucination via Evaluating the Factuality of Partial Response during Generation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Factuality Evaluation
        </strong>
        &amp;
        <strong>
         Decoding Monitoring
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        IIIT Hyderabad &amp; TCS Research, Hyderabad, India
       </td>
       <td style="text-align: center;">
        arXiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.04615">
         HalluCounter: Reference-free LLM Hallucination Detection in the Wild!
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Reference-free Hallucination Detection
        </strong>
        &amp;
        <strong>
         LLMs
        </strong>
        &amp;
        <strong>
         Response Consistency
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        University of Wisconsin-MadisonÔºå Zhejiang University
       </td>
       <td style="text-align: center;">
        QUHFM Workshop @ ICLR 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.01917">
         How to Steer LLM Latents for Hallucination Detection?
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         LLM Latent Space
        </strong>
        &amp;
        <strong>
         Optimal Transport
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Massachusetts Institute of Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.05777">
         Medical Hallucination in Foundation Models and Their Impact on Healthcare
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Medical Hallucination
        </strong>
        &amp;
        <strong>
         Foundation Models
        </strong>
        &amp;
        <strong>
         Clinical Safety
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Texas A&amp;M University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.05980">
         SINdex: Semantic INconsistency Index for Hallucination Detection in LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Semantic Clustering
        </strong>
        &amp;
        <strong>
         LLM Evaluation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        University of Southern California
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.06169">
         Treble Counterfactual VLMs: A Causal Approach to Hallucination
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Causal Inference
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Shanghai Jiao Tong University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.06709">
         Delusions of Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Delusion
        </strong>
        &amp;
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Model Uncertainty
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Texas A&amp;M University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.07833">
         HalluVerse25: Fine-grained Multilingual Benchmark Dataset for LLM Hallucinations
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Multilingual Benchmark
        </strong>
        &amp;
        <strong>
         Fine-grained Annotation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Yangzhou University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.09153">
         Is LLMs Hallucination Usable? LLM-based Negative Reasoning for Fake News Detection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Hallucination
        </strong>
        &amp;
        <strong>
         Negative Reasoning
        </strong>
        &amp;
        <strong>
         Fake News Detection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Drexel University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.10602">
         TruthPrInt: Mitigating LVLM Object Hallucination Via Latent Truthful-Guided Pre-Intervention
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Object Hallucination
        </strong>
        &amp;
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Latent Truthfulness
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        University of California, Santa Barbara
       </td>
       <td style="text-align: center;">
        Finding of NAACL 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.08963">
         Gradient-guided Attention Map Editing: Towards Efficient Contextual Hallucination Mitigation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Contextual Hallucination
        </strong>
        &amp;
        <strong>
         Attention Map Editing
        </strong>
        &amp;
        <strong>
         Summarization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Amazon
       </td>
       <td style="text-align: center;">
        WWW 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.05757">
         Uncertainty-Aware Fusion: An Ensemble Framework for Mitigating Hallucinations in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Uncertainty
        </strong>
        &amp;
        <strong>
         Ensemble
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        University of Central Missouri
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.10793">
         HALURust: Exploiting Hallucinations of Large Language Models to Detect Vulnerabilities in Rust
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Rust
        </strong>
        &amp;
        <strong>
         LLM Hallucination
        </strong>
        &amp;
        <strong>
         Vulnerability Detection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Honda Research Institute
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.10941">
         Graph-Grounded LLMs: Leveraging Graphical Function Calling to Minimize LLM Hallucinations
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Graph Reasoning
        </strong>
        &amp;
        <strong>
         Function Calling
        </strong>
        &amp;
        <strong>
         Hallucination Reduction
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Shanghai Advanced Research Institute, Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.12908">
         HICD: Hallucination-Inducing via Attention Dispersion for Contrastive Decoding to Mitigate Hallucinations in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Contrastive Decoding
        </strong>
        &amp;
        <strong>
         Attention Dispersion
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        University of Science and Technology of China
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.13107">
         ClearSight: Visual Signal Enhancement for Object Hallucination Mitigation in Multimodal Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal LLMs
        </strong>
        &amp;
        <strong>
         Object Hallucination
        </strong>
        &amp;
        <strong>
         Attention Modulation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        University of Derby
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.13514">
         RAG-KG-IL: A Multi-Agent Hybrid Framework for Reducing Hallucinations and Enhancing LLM Reasoning through RAG and Incremental Knowledge Graph Learning Integration
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         RAG
        </strong>
        &amp;
        <strong>
         Knowledge Graph
        </strong>
        &amp;
        <strong>
         Multi-Agent Systems
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Vanderbilt University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.14392">
         From "Hallucination" to "Suture": Insights from Language Philosophy to Enhance Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Hallucination
        </strong>
        &amp;
        <strong>
         Language Philosophy
        </strong>
        &amp;
        <strong>
         Anchor-RAG
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Fudan University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.14895">
         Mitigating Object Hallucinations in MLLMs via Multi-Frequency Perturbations
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Object Hallucination
        </strong>
        &amp;
        <strong>
         Multimodal LLMs
        </strong>
        &amp;
        <strong>
         Frequency-Domain Perturbation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Skoltech
       </td>
       <td style="text-align: center;">
        AAAI 2024
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.15948">
         Don‚Äôt Fight Hallucinations, Use Them: Estimating Image Realism using NLI over Atomic Facts
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LVLM
        </strong>
        &amp;
        <strong>
         NLI
        </strong>
        &amp;
        <strong>
         Image Realism
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        UNC Chapel Hill
       </td>
       <td style="text-align: center;">
        NAACL 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.15272">
         MAMM-REFINE: A Recipe for Improving Faithfulness in Generation with Multi-Agent Collaboration
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Faithful Generation
        </strong>
        &amp;
        <strong>
         Multi-Agent Collaboration
        </strong>
        &amp;
        <strong>
         Refinement Pipeline
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Paris-Saclay University
       </td>
       <td style="text-align: center;">
        ICLR 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.16161">
         TOWARDS LIGHTER AND ROBUST EVALUATION FOR RETRIEVAL AUGMENTED GENERATION
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         RAG Evaluation
        </strong>
        &amp;
        <strong>
         Faithfulness Assessment
        </strong>
        &amp;
        <strong>
         Quantized LLMs
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        University of Southern California
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.16528">
         HDLCoRe: A Training-Free Framework for Mitigating Hallucinations in LLM-Generated HDL
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         HDL Generation
        </strong>
        &amp;
        <strong>
         Prompt Engineering
        </strong>
        &amp;
        <strong>
         RAG
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Wroc≈Çaw University of Science and Technology, University of Technology Sydney
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.17229">
         FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Knowledge Graph
        </strong>
        &amp;
        <strong>
         Black-Box Method
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Stanford University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.18242">
         ShED-HD: A Shannon Entropy Distribution Framework for Lightweight Hallucination Detection on Edge Devices
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Entropy Analysis
        </strong>
        &amp;
        <strong>
         Edge Deployment
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        HausaNLP, University of Abuja, Bayero University Kano, University of Pretoria, Imperial College London, Northeastern University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.19650">
         HausaNLP at SemEval-2025 Task 3: Towards a Fine-Grained Model-Aware Hallucination Detection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Model-Aware Method
        </strong>
        &amp;
        <strong>
         Synthetic Dataset
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Ant Group, Shanghai Jiao Tong University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.21098">
         Alleviating LLM-based Generative Retrieval Hallucination in Alipay Search
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Generative Retrieval
        </strong>
        &amp;
        <strong>
         LLM Hallucination
        </strong>
        &amp;
        <strong>
         Knowledge Distillation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        ETH Z√ºrich, Google DeepMind
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.21676">
         How do language models learn facts? Dynamics, curricula and hallucinations
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Knowledge Acquisition
        </strong>
        &amp;
        <strong>
         Learning Dynamics
        </strong>
        &amp;
        <strong>
         Hallucination
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.03
       </td>
       <td style="text-align: center;">
        Northwestern Polytechnical University, Alibaba Group, Zhejiang University of Technology
       </td>
       <td style="text-align: center;">
        ICME 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2503.18556">
         Instruction-Aligned Visual Attention for Mitigating Hallucinations in Large Vision-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Visual Hallucination
        </strong>
        &amp;
        <strong>
         Contrastive Decoding
        </strong>
        &amp;
        <strong>
         Multimodal Attention
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        University of Science and Technology of China
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.04099">
         TARAC: Mitigating Hallucination in LVLMs via Temporal Attention Real-time Accumulative Connection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LVLM
        </strong>
        &amp;
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Temporal Attention
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        University of Southern California
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.06438">
         Don‚Äôt Let It Hallucinate: Premise Verification via Retrieval-Augmented Logical Reasoning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         False Premise Detection
        </strong>
        &amp;
        <strong>
         Retrieval-Augmented Generation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        AIMon Labs
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.07069">
         HALLUCINOT: Hallucination Detection Through Context and Common Knowledge Verification
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Enterprise LLMs
        </strong>
        &amp;
        <strong>
         Common Knowledge Verification
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        University of Pisa
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.05324">
         Hybrid Retrieval for Hallucination Mitigation in Large Language Models: A Comparative Analysis
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Retrieval Augmented Generation
        </strong>
        &amp;
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Query Expansion
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        University of Luxembourg
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.03302">
         Noise Augmented Fine Tuning for Mitigating Hallucinations in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Noise Injection
        </strong>
        &amp;
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         LLM Fine-Tuning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        University of Osaka
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.04335">
         Hallucination Detection using Multi-View Attention Features
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Attention Features
        </strong>
        &amp;
        <strong>
         Token-level Classification
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Tsinghua University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.08758">
         Combating LLM Hallucinations using Hypergraph-Driven Retrieval-Augmented Generation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Large Language Models
        </strong>
        &amp;
        <strong>
         Retrieval-Augmented Generation
        </strong>
        &amp;
        <strong>
         Hypergraph
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        The Hong Kong University of Science and Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.09440">
         Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Mathematical Reasoning
        </strong>
        &amp;
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Self-Consistency
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Indian Statistical Institute
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.09482">
         HALLUSHIFT: Measuring Distribution Shifts towards Hallucination Detection in LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Distribution Shift
        </strong>
        &amp;
        <strong>
         Token Probability
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        University of Science and Technology of China
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.10020">
         The Mirage of Performance Gains: Why Contrastive Decoding Fails to Address Multimodal Hallucination
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal Hallucination
        </strong>
        &amp;
        <strong>
         Contrastive Decoding
        </strong>
        &amp;
        <strong>
         MLLM Evaluation Bias
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Skolkovo Institute of Science and Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.10063">
         Hallucination Detection in LLMs via Topological Divergence on Attention Graphs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Topological Divergence
        </strong>
        &amp;
        <strong>
         Attention Graphs
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Newgiza University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.10168">
         HalluSearch at SemEval-2025 Task 3: A Search-Enhanced RAG Pipeline for Hallucination Detection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Multilingual RAG
        </strong>
        &amp;
        <strong>
         Factual Verification
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Giskard AI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.10277">
         RealHarm: A Collection of Real-World Language Model Application Failures
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Safety
        </strong>
        &amp;
        <strong>
         Real-World Failures
        </strong>
        &amp;
        <strong>
         Content Moderation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Korea University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.10831">
         Hallucination-Aware Generative Pretrained Transformer for Cooperative Aerial Mobility Control
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Reinforcement Learning
        </strong>
        &amp;
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Mobility Control
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Massachusetts Institute of Technology
       </td>
       <td style="text-align: center;">
        CHI ‚Äô25
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.12012">
         Purposefully Induced Psychosis (PIP): Embracing Hallucination as Imagination in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Computational Creativity
        </strong>
        &amp;
        <strong>
         Human‚ÄìAI Collaboration
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Volkswagen AG
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.12137">
         Efficient Contrastive Decoding with Probabilistic Hallucination Detection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Contrastive Decoding
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        University of California Berkeley
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.12691">
         Why and How LLMs Hallucinate: Connecting the Dots with Subsequence Associations
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Hallucination
        </strong>
        &amp;
        <strong>
         Subsequence Association
        </strong>
        &amp;
        <strong>
         Causal Tracing
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        MIT Lincoln Laboratory
       </td>
       <td style="text-align: center;">
        HCXAI @ CHI 2025
       </td>
       <td style="text-align: center;">
        <a href="https://doi.org/10.5281/zenodo.15170463">
         Mitigating LLM Hallucinations with Knowledge Graphs: A Case Study
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Hallucination
        </strong>
        &amp;
        <strong>
         Knowledge Graph
        </strong>
        &amp;
        <strong>
         Human-in-the-loop QA
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        University of West Florida
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.14395">
         HYDRA: An Agentic Reasoning Approach for Enhancing Adversarial Robustness and Mitigating Hallucinations in Vision-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Adversarial Robustness
        </strong>
        &amp;
        <strong>
         Agentic Reasoning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        University of Windsor
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.14429">
         ResNetVLLM-2: Addressing ResNetVLLM‚Äôs Multi-Modal Hallucinations
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         VideoLLM
        </strong>
        &amp;
        <strong>
         Multi-Modal Hallucination
        </strong>
        &amp;
        <strong>
         Retrieval-Augmented Generation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Yale University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.17004">
         (Im)possibility of Automated Hallucination Detection in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Language Identification Theory
        </strong>
        &amp;
        <strong>
         LLM Reliability
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        FAIR at Meta
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.17550">
         HalluLens: LLM Hallucination Benchmark
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LLM Hallucination
        </strong>
        &amp;
        <strong>
         Benchmark
        </strong>
        &amp;
        <strong>
         Intrinsic vs. Extrinsic Evaluation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        University of Southern California
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.18114">
         Evaluating Evaluation Metrics ‚Äì The Mirage of Hallucination Detection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Evaluation Metrics
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Ulm University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.18639">
         Hallucination Detectives at SemEval-2025 Task 3: Span-Level Hallucination Detection for LLM-Generated Answers
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Semantic Role Labeling
        </strong>
        &amp;
        <strong>
         Multilingual NLP
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        The University of Akron
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.19061">
         Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Medical Text Summarization
        </strong>
        &amp;
        <strong>
         Hallucinations
        </strong>
        &amp;
        <strong>
         Key Information Extraction
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        AWS AI Labs
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.19457">
         Towards Long Context Hallucination Detection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Long-Context Processing
        </strong>
        &amp;
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Natural Language Inference
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        RISE Research Institutes of Sweden
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.20699">
         Can LLMs Detect Intrinsic Hallucinations in Paraphrasing and Machine Translation?
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Machine Translation
        </strong>
        &amp;
        <strong>
         Paraphrasing
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        UNIST
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.20799">
         Hallucination by Code Generation LLMs: Taxonomy, Benchmarks, Mitigation, and Challenges
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Code Generation
        </strong>
        &amp;
        <strong>
         Hallucination Taxonomy
        </strong>
        &amp;
        <strong>
         Mitigation Strategies
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Jilin University
       </td>
       <td style="text-align: center;">
        IJCAI 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.21604">
         Robust Misinformation Detection by Visiting Potential Commonsense Conflict
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Misinformation Detection
        </strong>
        &amp;
        <strong>
         Commonsense Conflict
        </strong>
        &amp;
        <strong>
         Data Augmentation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Quotient AI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.00506">
         HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Real-World Hallucination Detection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Benchmark
        </strong>
        &amp;
        <strong>
         Retrieval-Augmented Generation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Kanazawa University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.00557">
         Triggering Hallucinations in LLMs: A Quantitative Study of Prompt-Induced Hallucination in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Prompt-Induced Hallucination
        </strong>
        &amp;
        <strong>
         Conceptual Fusion
        </strong>
        &amp;
        <strong>
         LLM Evaluation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.04
       </td>
       <td style="text-align: center;">
        Amazon AWS AI
       </td>
       <td style="text-align: center;">
        NAACL 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2504.21559">
         Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Visual Prompt Engineering
        </strong>
        &amp;
        <strong>
         Object Hallucination
        </strong>
        &amp;
        <strong>
         Vision Language Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Stanford University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.04844">
         Osiris: A Lightweight Open-Source Hallucination Detection System
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         RAG Systems
        </strong>
        &amp;
        <strong>
         Open-Source Models
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        MBZUAI
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.08200v1">
         A Head to Predict and a Head to Question: Pre-trained Uncertainty Quantification Heads for Hallucination Detection in LLM Outputs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Uncertainty Quantification
        </strong>
        &amp;
        <strong>
         LLM Outputs
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Lappeenranta-Lahti University of Technology LUT
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.11405">
         EmotionHallucer: Evaluating Emotion Hallucinations in Multimodal Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Emotion Understanding
        </strong>
        &amp;
        <strong>
         Multimodal Large Language Models
        </strong>
        &amp;
        <strong>
         Hallucination Evaluation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Los Alamos National Laboratory
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.11741">
         Diverging Towards Hallucination: Detection of Failures in Vision-Language Models via Multi-token Aggregation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Multi-token Logits
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        York University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.12151">
         Reasoning Large Language Model Errors Arise from Hallucinating Critical Problem Features
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Reasoning LLMs
        </strong>
        &amp;
        <strong>
         Graph Coloring
        </strong>
        &amp;
        <strong>
         Edge Hallucination
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        HKUST(GZ)
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.12265">
         Learning Auxiliary Tasks Improves Reference-Free Hallucination Detection in Open-Domain Long-Form Generation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Long-Form Generation
        </strong>
        &amp;
        <strong>
         Auxiliary Task Learning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        De Artificial Intelligence Lab
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.12343">
         Mitigating Hallucinations via Inter-Layer Consistency Aggregation in Large Vision-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Vision-Language Models
        </strong>
        &amp;
        <strong>
         Layer Aggregation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Renmin University of China
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.12886">
         Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Reasoning Hallucination
        </strong>
        &amp;
        <strong>
         Mechanistic Interpretability
        </strong>
        &amp;
        <strong>
         Reinforcement Learning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        University of Southern California
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.13988">
         The Hallucination Tax of Reinforcement Finetuning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Reinforcement Finetuning
        </strong>
        &amp;
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Refusal Behavior
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Aalborg University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.14101">
         MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation of LLM Hallucinations
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Evaluation
        </strong>
        &amp;
        <strong>
         Knowledge Graph
        </strong>
        &amp;
        <strong>
         Multilingual QA
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Chung-Ang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.15291v1">
         Hallucinate at the Last in Long Response Generation: A Case Study on Long Document Summarization
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Long-form Generation
        </strong>
        &amp;
        <strong>
         Summarization
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        The Hong Kong University of Science and Technology
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.15386v1">
         RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation and Language Generation for Explainable QA Hallucination Detection
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Detection
        </strong>
        &amp;
        <strong>
         Uncertainty Estimation
        </strong>
        &amp;
        <strong>
         Explainable QA
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Southeast University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.16146v1">
         Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         LVLM Hallucination
        </strong>
        &amp;
        <strong>
         Sparse Autoencoder
        </strong>
        &amp;
        <strong>
         Latent Steering
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Politecnico di Bari
       </td>
       <td style="text-align: center;">
        ACL 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.16520v1">
         Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Factuality Encoding
        </strong>
        &amp;
        <strong>
         LLM Probing
        </strong>
        &amp;
        <strong>
         Self-Evaluation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Computer Network Information Center
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.16894v1">
         Shadows in the Attention: Contextual Perturbation and Representation Drift in the Dynamics of Hallucination in LLMs
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucinations
        </strong>
        &amp;
        <strong>
         Representation Drift
        </strong>
        &amp;
        <strong>
         Attention-Locking
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        University of Virginia
       </td>
       <td style="text-align: center;">
        IJCAI 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.14599v1">
         Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Biomedical Hypothesis Generation
        </strong>
        &amp;
        <strong>
         Truthfulness Evaluation
        </strong>
        &amp;
        <strong>
         Hallucination Detection
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Chinese Academy of Sciences
       </td>
       <td style="text-align: center;">
        ACL 2025 Findings
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.17061">
         Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Large Vision-Language Models
        </strong>
        &amp;
        <strong>
         Contrastive Decoding
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Southeast University, University of New South Wales
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.17118v1">
         After Retrieval, Before Generation: Enhancing the Trustworthiness of Large Language Models in RAG
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Trustworthiness
        </strong>
        &amp;
        <strong>
         RAG
        </strong>
        &amp;
        <strong>
         Soft Bias
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        University of Southern California
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.17222v1">
         Humans Hallucinate Too: Language Models Identify and Correct Subjective Annotation Errors With Label-in-a-Haystack Prompts
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Subjective Annotation
        </strong>
        &amp;
        <strong>
         Error Correction
        </strong>
        &amp;
        <strong>
         Label-in-a-Haystack
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        EPFL, Stony Brook University, University of Chicago
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.18555">
         Unraveling Misinformation Propagation in LLM Reasoning
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Misinformation Propagation
        </strong>
        &amp;
        <strong>
         LLM Reasoning
        </strong>
        &amp;
        <strong>
         Correction Strategies
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Zhejiang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.19474v1">
         Causal-LLaVA: Causal Disentanglement for Mitigating Hallucination in Multimodal Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Causal Disentanglement
        </strong>
        &amp;
        <strong>
         Multimodal Large Language Model
        </strong>
        &amp;
        <strong>
         Hallucination Mitigation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Beihang University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.19498v1">
         Enhancing Visual Reliance in Text Generation: A Bayesian Perspective on Mitigating Hallucination in Large Vision-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Visual Reliance
        </strong>
        &amp;
        <strong>
         Bayesian Perspective
        </strong>
        &amp;
        <strong>
         Hallucination Mitigation
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        George Mason University
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.21472v1">
         Mitigating Hallucination in Large Vision-Language Models via Adaptive Attention Calibration
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Attention Calibration
        </strong>
        &amp;
        <strong>
         Vision-Language Model
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        UC Santa Cruz
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.21523v1">
         More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Multimodal Reasoning
        </strong>
        &amp;
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Visual Attention
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Hasso Plattner Institute, University of Potsdam
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.21547v1">
         Image Tokens Matter: Mitigating Hallucination in Discrete Tokenizer-based Large Vision-Language Models via Latent Editing
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination
        </strong>
        &amp;
        <strong>
         Vision-Language Model
        </strong>
        &amp;
        <strong>
         Image Tokenizer
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Purdue University, University of California, Davis
       </td>
       <td style="text-align: center;">
        arxiv
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.23657">
         Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Layer-Contrastive Decoding
        </strong>
        &amp;
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Reinforcement Learning
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Yonsei University, Onoma AI
       </td>
       <td style="text-align: center;">
        ACL 2025 Findings
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.20569v2">
         Retrieval Visual Contrastive Decoding to Mitigate Object Hallucinations in Large Vision-Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Object Hallucination
        </strong>
        &amp;
        <strong>
         Vision-Language Model
        </strong>
        &amp;
        <strong>
         Contrastive Decoding
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        The Hong Kong University of Science and Technology (Guangzhou)
       </td>
       <td style="text-align: center;">
        ACL 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.21608v1">
         How does Misinformation Affect Large Language Model Behaviors and Preferences?
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Misinformation
        </strong>
        &amp;
        <strong>
         LLM Behavior
        </strong>
        &amp;
        <strong>
         Benchmark
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        The Hong Kong Polytechnic University
       </td>
       <td style="text-align: center;">
        ACL 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.18581v1">
         Removal of Hallucination on Hallucination: Debate-Augmented RAG
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Hallucination Mitigation
        </strong>
        &amp;
        <strong>
         Retrieval-Augmented Generation
        </strong>
        &amp;
        <strong>
         Multi-Agent Debate
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align: center;">
        25.05
       </td>
       <td style="text-align: center;">
        Central South University
       </td>
       <td style="text-align: center;">
        ACL 2025
       </td>
       <td style="text-align: center;">
        <a href="https://arxiv.org/abs/2505.19108v1">
         CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models
        </a>
       </td>
       <td style="text-align: center;">
        <strong>
         Cross-lingual Hallucination
        </strong>
        &amp;
        <strong>
         Cross-modal Hallucination
        </strong>
        &amp;
        <strong>
         Benchmark
        </strong>
       </td>
      </tr>
     </tbody>
    </table>
    <h2>
     üíªPresentations &amp; Talks
    </h2>
    <h2>
     üìñTutorials &amp; Workshops
    </h2>
    <table>
     <thead>
      <tr>
       <th style="text-align: center;">
        Date
       </th>
       <th style="text-align: center;">
        Type
       </th>
       <th style="text-align: center;">
        Title
       </th>
       <th style="text-align: center;">
        URL
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td style="text-align: center;">
        23.10
       </td>
       <td style="text-align: center;">
        Tutorials
       </td>
       <td style="text-align: center;">
        Awesome-LLM-Safety
       </td>
       <td style="text-align: center;">
        <a href="https://github.com/ydyjya/Awesome-LLM-Safety">
         link
        </a>
       </td>
      </tr>
     </tbody>
    </table>
    <h2>
     üì∞News &amp; Articles
    </h2>
    <h2>
     üßë‚Äçüè´Scholars
    </h2>
   </div>
   <a class="back-to-home" href="../index.html">
    <i class="fas fa-arrow-left">
    </i>
    Back to Home
   </a>
  </div>
  <footer>
   <div class="container">
    <div class="footer-content">
     <p>
      Created by
      <a href="https://github.com/ydyjya">
       ydyjya
      </a>
     </p>
     <p>
      Contact: zhouzhenhong@bupt.edu.cn
     </p>
    </div>
    <div class="footer-links">
     <a href="https://github.com/ydyjya/Awesome-LLM-Safety" target="_blank">
      <i class="fab fa-github">
      </i>
      GitHub Repository
     </a>
    </div>
   </div>
  </footer>
 </body>
</html>
