# Datasets & Benchmark


## 📑Papers

| Date  |                                                                                              Institute                                                                                              |           Publication            |                                                                                  Paper                                                                                  |                                           Keywords                                           |
|:-----:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:--------------------------------:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:--------------------------------------------------------------------------------------------:|
| 20.09 |                                                                                      University of Washington                                                                                       |       EMNLP2020(findings)        |                            [RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models](https://arxiv.org/abs/2009.11462)                             |                                         **Toxicity**                                         |
| 21.09 |                                                                                        University of Oxford                                                                                         |             ACL2022              |                                       [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)                                       |                                       **Truthfulness**                                       |
| 22.03 |                                                                                                 MIT                                                                                                 |             ACL2022              |                [ToxiGen: A Large-Scale Machine-Generated datasets for Adversarial and Implicit Hate Speech Detection](https://arxiv.org/abs/2203.09509)                 |                                         **Toxicity**                                         |
| 23.07 |                                                                   Zhejiang University; School of Engineering Westlake University                                                                    |              arxiv               |               [Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models](https://arxiv.org/abs/2307.08487)               |                        **Text Safety**&**Benchmark**&**Jailbreaking**                        |
| 23.07 |                                                                                   Stevens Institute of Technology                                                                                   |       NAACL2024(findings)        |                           [HateModerate: Testing Hate Speech Detectors against Content Moderation Policies](https://arxiv.org/abs/2307.12418)                           |            **Hate Speech Detection**&**Content Moderation**&**Machine Learning**             |
| 23.08 |                                                                                          Meta Reality Labs                                                                                          |            NAACL2024             |                                  [Head-to-Tail: How Knowledgeable are Large Language Models (LLMs)?](https://arxiv.org/abs/2308.10168)                                  |            **Large Language Models**&**Knowledge Graphs**&**Question Answering**             |
| 23.08 |                                                                                         Bocconi University                                                                                          |            NAACL2024             |                     [XSTEST: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models](https://arxiv.org/abs/2308.01263)                     |                **Large Language Models**&**Safety Behaviours**&**Test Suite**                |
| 23.09 |                                                                             LibrAI, MBZUAI, The University of Melbourne                                                                             |              arxiv               |                                     [Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs](https://arxiv.org/abs/2308.13387)                                      |                             **Safety Evaluation**&**Safeguards**                             |
| 23.10 |                                                                       University of Edinburgh, Huawei Technologies Co., Ltd.                                                                        |            NAACL2024             |                                     [Assessing the Reliability of Large Language Model Knowledge](https://arxiv.org/abs/2310.09820)                                     |            **Large Language Models**&**Factual Knowledge**&**Knowledge Probing**             |
| 23.10 |                                                                                     University of Pennsylvania                                                                                      |       NAACL2024(findings)        |              [Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks](https://arxiv.org/abs/2310.12516)              |        **Hallucination Assessment**&**Adversarial Attacks**&**Large Language Models**        |
| 23.11 |                                                                                          Fudan University                                                                                           |              arxiv               |                                     [JADE: A Linguistic-based Safety Evaluation Platform for LLM](https://arxiv.org/abs/2311.00286)                                     |                                    **Safety Benchmarks**                                     |
| 23.11 |                                                                                           UNC-Chapel Hill                                                                                           |              arxiv               |                        [Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges](https://arxiv.org/abs/2311.03287)                        |                        **Hallucination**&**Benchmark**&**Multimodal**                        |
| 23.11 |                                                                                           IBM Research AI                                                                                           |     EMNLP2023(GEM workshop)      |                                      [Unveiling Safety Vulnerabilities of Large Language Models](https://arxiv.org/abs/2311.04124)                                      |            **Adversarial Examples**&**Clustering**&**Automatically Identifying**             |
| 23.11 |                                                                         The Hong Kong University of Science and Technology                                                                          |              arxiv               |                               [P-Bench: A Multi-level Privacy Evaluation Benchmark for Language Models](https://arxiv.org/abs/2311.04044)                               |                       **Differential Privacy**&**Privacy Evaluation**                        |
| 23.11 |                                                                                             UC Berkeley                                                                                             |              arxiv               |                                                    [CAN LLMS FOLLOW SIMPLE RULES](https://arxiv.org/abs/2311.04235)                                                     |                             **Evaluation**&**Attack Strategies**                             |
| 23.11 |                                                                                    University of Central Florida                                                                                    |              arxiv               |                                  [THOS: A Benchmark Dataset for Targeted Hate and Offensive Speech](https://arxiv.org/abs/2311.06446)                                   |                       **Hate Speech**&**Offensive Speech**&**Dataset**                       |
| 23.11 |                                                              Beijing Jiaotong University; DAMO Academy, Alibaba Group, Peng Cheng Lab                                                               |              arXiv               |                          [AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation](https://arxiv.org/abs/2311.07397)                          |                  Multi-modal Large Language Models&Hallucination&Benchmark                   |
| 23.11 |                                                                        Patronus AI, University of Oxford, Bocconi University                                                                        |              arxiv               |                   [SIMPLESAFETYTESTS: a Test Suite for Identifying Critical Safety Risks in Large Language Models](https://arxiv.org/abs/2311.08370)                    |                        **Safety Risks**&**Test Suite**&**Evaluation**                        |
| 23.11 |                                                    University of Southern California, University of Pennsylvania, University of California Davis                                                    |              arxiv               |                   [Deceiving Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?](https://arxiv.org/abs/2311.09702)                    |                  **Hallucinations**&**Semantic Associations**&**Benchmark**                  |
| 23.11 |                                                 Seoul National University, Chung-Ang University, NAVER AI Lab, NAVER Cloud, University of Richmond                                                  |              arxiv               |                                         [LifeTox: Unveiling Implicit Toxicity in Life Advice](https://arxiv.org/abs/2311.09585)                                         |             **LifeTox Dataset**&**Toxicity Detection**&**Social Media Analysis**             |
| 23.11 |                                                                          School of Information Renmin University of China                                                                           |              arxiv               |                [UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation](https://arxiv.org/abs/2311.15296)                |                          **Hallucination**&**Evaluation Benchmark**                          |
| 23.11 |                                                                                   UC Santa Cruz, UNC-Chapel Hill                                                                                    |              arxiv               |                              [How Many Are in This Image? A Safety Evaluation Benchmark for Vision LLMs](https://arxiv.org/abs/2311.16101)                              |      **Vision Large Language Models**&**Safety Evaluation**&A**dversarial Robustness**       |
| 23.11 |                         Institute of Information Engineering, Chinese Academy of Sciences; School of Cyber Security, University of Chinese Academy of Sciences; Baidu Inc.                          |              arxiv               |                    [FFT: Towards Harmlessness Evaluation and Analysis for LLMs with Factuality Fairness Toxicity](https://arxiv.org/abs/2311.18580)                     |                                 **Harmlessness Evaluation**                                  |
| 23.11 |                                                                    Fudan University&Shanghai Artificial Intelligence Laboratory                                                                     |            NAACL2024             |                                            [Fake Alignment: Are LLMs Really Aligned Well?](https://arxiv.org/abs/2311.05915)                                            |              **Large Language Models**&**Safety Evaluation**&**Fake Alignment**              |
| 23.11 |                                                                                     Kahlert School of Computing                                                                                     |            NAACL2024             |                                    [Whispers of Doubt Amidst Echoes of Triumph in NLP Robustness](https://arxiv.org/abs/2311.09694)                                     |          **NLP Robustness**&**Out-of-Domain Evaluation**&**Adversarial Evaluation**          |
| 23.11 |                                                                                    Shanghai Jiao Tong University                                                                                    |       NAACL2024(findings)        |                                 [CLEAN–EVAL: Clean Evaluation on Contaminated Large Language Models](https://arxiv.org/abs/2311.09154)                                  |            **Clean Evaluation**&**Data Contamination**&**Large Language Models**             |
| 23.12 |                                                                                                Meta                                                                                                 |              arxiv               |                              [Purple Llama CYBERSECEVAL: A Secure Coding Benchmark for Language Models](https://arxiv.org/abs/2312.04724)                               |                   **Safety**&**Cybersecurity**&**Code Security Benchmark**                   |
| 23.12 |                                   University of Illinois Chicago, Bosch Research North America & Bosch Center for Artificial Intelligence (BCAI), UNC Chapel-Hill                                   |              arxiv               |                             [DELUCIONQA: Detecting Hallucinations in Domain-specific Question Answering](https://arxiv.org/abs/2312.05200)                              |       **Hallucination Detection**&**Domain-specific QA**&**Retrieval-augmented LLMs**        |
| 23.12 |                                              University of Science and Technology of China, Hong Kong University of Science and Technology, Microsoft                                               |              arxiv               |                    [Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models](https://arxiv.org/abs/2312.14197)                    |            **Indirect Prompt Injection Attacks**&**BIPIA Benchmark**&**Defense**             |
| 24.01 |                                                                         NewsBreak, University of Illinois Urbana-Champaign                                                                          |              arxiv               |                   [RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models](https://arxiv.org/abs/2401.00396)                   |          **Retrieval-Augmented Generation**&**Hallucination Detection**&**Dataset**          |
| 24.01 | University of Notre Dame, Lehigh University, Illinois Institute of Technology, Institut Polytechnique de Paris, William & Mary, Texas A&M University, Samsung Research America, Stanford University |            ICML 2024             |                                 [TRUSTLLM: TRUSTWORTHINESS IN LARGE LANGUAGE MODELS](https://proceedings.mlr.press/v235/huang24x.html)                                  |                         **Trustworthiness**&**Benchmark Evaluation**                         |
| 24.01 |                                                                                      University College London                                                                                      |              arxiv               |                                    [Hallucination Benchmark in Medical Visual Question Answering](https://arxiv.org/abs/2401.05827)                                     |              **Medical Visual Question Answering**&**Hallucination Benchmark**               |
| 24.01 |                                                                                     Carnegie Mellon University                                                                                      |              arxiv               |                                           [TOFU: A Task of Fictitious Unlearning for LLMs](https://arxiv.org/abs/2401.06121)                                            |                     **Data Privacy**&**Ethical Concerns**&**Unlearning**                     |
| 24.01 |                                                                         IRLab CITIC Research Centre, Universidade da Coruña                                                                         |              arxiv               |                                  [MetaHate: A Dataset for Unifying Efforts on Hate Speech Detection](https://arxiv.org/abs/2401.06526)                                  |                          **Hate Speech Detection**&**Social Media**                          |
| 24.01 |                                                      Northwestern University, New York University, University of Liverpool, Rutgers University                                                      |              arxiv               |                    [AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models](https://arxiv.org/abs/2401.09002)                    |           **Jailbreak Attack**&**Evaluation Frameworks**&**Ground Truth Dataset**            |
| 24.01 |                                                                                    Shanghai Jiao Tong University                                                                                    |              arxiv               |                                     [R-Judge: Benchmarking Safety Risk Awareness for LLM Agents](https://arxiv.org/abs/2401.10019)                                      |                    **LLM Agents**&**Safety Risk Awareness**&**Benchmark**                    |
| 24.02 |                                          University of Illinois Urbana-Champaign, Center for AI Safety, Carnegie Mellon University, UC Berkeley, Microsoft                                          |              arxiv               |                     [HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal](https://arxiv.org/abs/2402.04249)                     |                         **Automated Red Teaming**&**Robust Refusal**                         |
| 24.02 |                            Shanghai Artificial Intelligence Laboratory, Harbin Institute of Technology, Beijing Institute of Technology, Chinese University of Hong Kong                            |              arxiv               |                      [SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models](https://arxiv.org/abs/2402.05044)                       |              **Safety Benchmark**&Safety Evaluation**&**Hierarchical Taxonomy**              |
| 24.02 |                                                                                  Middle East Technical University                                                                                   |              arxiv               |                       [HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination Tendency of LLMs](https://arxiv.org/abs/2402.16211)                        |                          **Hallucination**&**Benchmarking Dataset**                          |
| 24.02 |                                                                              Indian Institute of Technology Kharagpur                                                                               |              arxiv               |  [How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries](https://arxiv.org/abs/2402.15302)   |                **Instruction-centric Responses**&**Ethical Vulnerabilities**                 |
| 24.03 |                                                                                    East China Normal University                                                                                     |              arxiv               |                       [DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2403.00896)                        |       **Dialogue-level Hallucination**&**Benchmarking**&**Human-machine Interaction**        |
| 24.03 |                                      Tianjin University, Tianjin University, Zhengzhou University, China Academy of Information and Communications Technology                                       |              arxiv               |                            [OpenEval: Benchmarking Chinese LLMs across Capability, Alignment, and Safety](https://arxiv.org/abs/2403.12316)                             |                         **Chinese LLMs**&**Benchmarking**&**Safety**                         |
| 24.04 |                                                                        University of Pennsylvania, ETH Zurich, EPFL, Sony AI                                                                        |              arxiv               |                         [JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models](https://arxiv.org/abs/2404.01318)                         |                      **Jailbreaking Attacks**&**Robustness Benchmark**                       |
| 24.04 |                                                                Vector Institute for Artificial Intelligence, University of Limerick                                                                 |              arxiv               |                          [Developing Safe and Responsible Large Language Models - A Comprehensive Framework](https://arxiv.org/abs/2404.01399)                          |                      **Responsible AI**&**AI Safety**&**Generative AI**                      |
| 24.04 |                                              LMU Munich, University of Oxford, Siemens AG, Munich Center for Machine Learning (MCML), Wuhan University                                              |              arxiv               |                           [RED TEAMING GPT-4V: ARE GPT-4V SAFE AGAINST UNI/MULTI-MODAL JAILBREAK ATTACKS?](https://arxiv.org/abs/2404.03411)                            |           **Jailbreak Attacks**&**GPT-4V**&**Evaluation Benchmark**&**Robustness**           |
| 24.04 |                                                                              Bocconi University, University of Oxford                                                                               |              arxiv               |            [SafetyPrompts: a Systematic Review of Open Datasets for Evaluating and Improving Large Language Model Safety](https://arxiv.org/abs/2404.05399)             |                    **LLM Safety**&**Open Datasets**&**Systematic Review**                    |
| 24.04 |                                                                            University of Alberta&The University of Tokyo                                                                            |              arxiv               |                           [Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path Forward](https://arxiv.org/abs/2404.08517)                           |                   **LLM Safety**&**Online Safety Analysis**&**Benchmark**                    |
| 24.04 |                                                                     Technion – Israel Institute of Technology, Google Research                                                                      |              arxiv               |                           [Constructing Benchmarks and Interventions for Combating Hallucinations in LLMs](https://arxiv.org/abs/2404.09971)                            |                              **Hallucinations**&**Benchmarks**                               |
| 24.05 |                                                                                     Carnegie Mellon University                                                                                      |              arxiv               |                [PolygloToxicityPrompts: Multilingual Evaluation of Neural Toxic Degeneration in Large Language Models](https://arxiv.org/abs/2405.09373)                |                           **Multilingual Evaluation**&**Datasets*                            |
| 24.05 |                                                                       Paul G. Allen School of Computer Science & Engineering                                                                        |              arxiv               |              [MASSIVE Multilingual Abstract Meaning Representation: A Dataset and Baselines for Hallucination Detection](https://arxiv.org/abs/2405.19285)              |                 **Hallucination Detection**&**Multilingual AMR**&**Dataset**                 |
| 24.05 |                                                                                 University of California, Riverside                                                                                 |              arxiv               |                                   [Cross-Task Defense: Instruction-Tuning LLMs for Content Safety](https://arxiv.org/abs/2405.15202)                                    |                   **Instruction-Tuning**&**LLM Safety**&**Content Safety**                   |
| 24.06 |                                                                                       University of Waterloo                                                                                        |              arxiv               |                                  [TruthEval: A Dataset to Evaluate LLM Truthfulness and Reliability](https://arxiv.org/abs/2406.01855)                                  |                               **Truthfulness**&**Reliability**                               |
| 24.06 |                                                                                         Rutgers University                                                                                          |              arxiv               |                                                [MoralBench: Moral Evaluation of LLMs](https://arxiv.org/abs/2406.04428)                                                 |                             **Moral Evaluation**&**MoralBench**                              |
| 24.06 |                                                                                         Tsinghua University                                                                                         |              arxiv               |                       [Benchmarking Trustworthiness of Multimodal Large Language Models: A Comprehensive Study](https://arxiv.org/abs/2406.07057)                       |                         **Trustworthiness**&**MLLMs**&**Benchmark**                          |
| 24.06 |                                                                             Beijing Academy of Artificial Intelligence                                                                              |              arxiv               |                      [HalluDial: A Large-Scale Benchmark for Automatic Dialogue-Level Hallucination Evaluation](https://arxiv.org/abs/2406.07070)                       |                **Hallucination Evaluation**&**Dialogue-Level**&**HalluDial**                 |
| 24.06 |                                                                                         Sichuan University                                                                                          |              arxiv               |                   [LEGEND: Leveraging Representation Engineering to Annotate Safety Margin for Preference Datasets](https://arxiv.org/abs/2406.08124)                   |           **Safety Margin**&**Preference Datasets**&**Representation Engineering**           |
| 24.06 |                                                                   The Hong Kong University of Science and Technology (Guangzhou)                                                                    |              arxiv               |                                      [Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs](https://arxiv.org/abs/2406.09324)                                       |                            **Jailbreak Attacks**&**Benchmarking**                            |
| 24.06 |                                                                                 AI Innovation Center, China Unicom                                                                                  |              arxiv               |                          [CHiSafetyBench: A Chinese Hierarchical Safety Benchmark for Large Language Models](https://arxiv.org/abs/2406.10311)                          | **Chinese Hierarchical Safety Benchmark**&**Large Language Models**&**Automatic Evaluation** |
| 24.06 |                                                                                               Google                                                                                                |              arxiv               |                      [Supporting Human Raters with the Detection of Harmful Content using Large Language Models](https://arxiv.org/abs/2406.12800)                      |                        **Harmful Content Detection**&**Hate Speech**                         |
| 24.06 |                                                  South China University of Technology, Pazhou Laboratory, University of Maryland, Baltimore County                                                  |              arxiv               |                        [GenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large Language Models](https://arxiv.org/abs/2406.13925)                        |             **Gender Bias Mitigation**&**Alignment Dataset**&**Bias Categories**             |
| 24.06 |                                                              Center for AI Safety and Governance, Institute for AI, Peking University                                                               |              arxiv               |                     [SAFESORA: Towards Safety Alignment of Text2Video Generation via a Human Preference Dataset](https://arxiv.org/abs/2406.14477)                      |                        **Safety Alignment**&**Text2Video Generation**                        |
| 24.06 |                                                                                          Fudan University                                                                                           |              arxiv               |                                                   [Cross-Modality Safety Alignment](https://arxiv.org/abs/2406.15279)                                                   |          **Multimodal Safety**&**Large Vision-Language Models**&**SIUO Benchmark**           |
| 24.06 |                                                                                                KAIST                                                                                                |              arxiv               |                           [CSRT: Evaluation and Analysis of LLMs using Code-Switching Red-Teaming Dataset](https://arxiv.org/abs/2406.15481)                            |                    **Code-Switching**&**Red-Teaming**&**Multilingualism**                    |
| 24.06 |                                                                                      University College London                                                                                      |              arxiv               |                          [JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models](https://arxiv.org/abs/2406.15484)                          |                       **Gender Bias**&**Hiring Bias**&**Benchmarking**                       |
| 24.06 |                                                                                          Peking University                                                                                          |              arxiv               |                             [PKU-SafeRLHF: A Safety Alignment Preference Dataset for Llama Family Models](https://arxiv.org/abs/2406.15513)                             |                         **Safety Alignment**&**Preference Dataset**                          |
| 24.06 |                                                                                University of California, Los Angeles                                                                                |              arxiv               |                             [MOSSBench: Is Your Multimodal Language Model Oversensitive to Safe Queries?](https://arxiv.org/abs/2406.17806)                             |           **Multimodal Language Models**&**Oversensitivity**&**Safety Mechanisms**           |
| 24.06 |                                                                                       Allen Institute for AI                                                                                        |              arxiv               |                    [WILDGUARD: Open One-stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs](https://arxiv.org/abs/2406.18495)                     |               **Safety Moderation**&**Jailbreak Attacks**&**Moderation Tools**               |
| 24.06 |                                                                                      University of Washington                                                                                       |              arxiv               |                     [WILDTEAMING at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models](https://arxiv.org/abs/2406.18510)                      |                 **Jailbreaking**&**Safety Training**&**Adversarial Attacks**                 |
| 24.07 |                                                                                     Beijing Jiaotong University                                                                                     |              arxiv               |               [KG-FPQ: Evaluating Factuality Hallucination in LLMs with Knowledge Graph-based False Premise Questions](https://arxiv.org/abs/2407.05868)                |         **Factuality Hallucination**&**Knowledge Graph**&**False Premise Questions**         |
| 24.07 |                                                                                     Chinese Academy of Sciences                                                                                     |              arxiv               |                              [T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models](https://arxiv.org/abs/2407.05965)                               |           **Text-to-Video Generation**&**Safety Evaluation**&**Generative Models**           |
| 24.07 |                                                                                             Patronus AI                                                                                             |              arxiv               |                                         [Lynx: An Open Source Hallucination Evaluation Model](https://arxiv.org/abs/2407.08488)                                         |                   **Hallucination Detection**&**RAG**&**Evaluation Model**                   |
| 24.07 |                                                                                            Virginia Tech                                                                                            |              arxiv               |                      [AIR-BENCH 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies](https://arxiv.org/abs/2407.17436)                      |                **AI Safety**&**Regulations**&**Policies**&**Risk Categories**                |
| 24.07 |                                                                                         Columbia University                                                                                         |            ECCV 2024             |                            [HaloQuest: A Visual Hallucination Dataset for Advancing Multimodal Reasoning](https://arxiv.org/abs/2407.15680)                             |                  **Hallucination**&**Vision-Language Models**&**Datasets**                   |
| 24.07 |                                                                                        Center for AI Safety                                                                                         |              arxiv               |                              [Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?](https://arxiv.org/abs/2407.21792)                               |                                 **AI Safety**&**Benchmarks**                                 |
| 24.08 |                                                                                           Walled AI Labs                                                                                            |              arxiv               |                           [WALLEDEVAL: A Comprehensive Safety Evaluation Toolkit for Large Language Models](https://arxiv.org/abs/2408.03837)                           |                              **AI Safety**&**Prompt Injection**                              |
| 24.08 |                                                                                       ShanghaiTech University                                                                                       |              arxiv               |                    [MMJ-Bench: A Comprehensive Study on Jailbreak Attacks and Defenses for Vision Language Models](https://arxiv.org/abs/2408.08464)                    |                **Jailbreak Attacks**&**Vision-Language Models**&**Security**                 |
| 24.08 |                                                                                         Stanford University                                                                                         |              arxiv               |                     [Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risk of Language Models](https://arxiv.org/abs/2408.08926)                      |                            **Cybersecurity**&**Capture the Flag**                            |
| 24.08 |                                                                                         Zhejiang University                                                                                         |              arxiv               |                           [Characterizing and Evaluating the Reliability of LLMs against Jailbreak Attacks](https://arxiv.org/abs/2408.09326)                           |              **Jailbreak Attacks**&**LLM Reliability**&**Evaluation Framework**              |
| 24.08 |                                                                                             Enkrypt AI                                                                                              |              arxiv               |                         [SAGE-RT: Synthetic Alignment Data Generation for Safety Evaluation and Red Teaming](https://arxiv.org/abs/2408.11851)                          |             **Synthetic Data Generation**&**Safety Evaluation**&**Red Teaming**              |
| 24.08 |                                                                                         Tianjin University                                                                                          |       Findings of ACL 2024       |                             [CMoralEval: A Moral Evaluation Benchmark for Chinese Large Language Models](https://arxiv.org/abs/2408.09819)                              |                            **Moral Evaluation**&**Moral Dilemma**                            |
| 24.08 |                                                                                        University of Surrey                                                                                         |            IJCAI 2024            |                                [CodeMirage: Hallucinations in Code Generated by Large Language Models](https://arxiv.org/abs/2408.08333)                                |                        **Code Hallucinations**&**CodeMirage Dataset**                        |
| 24.08 |                                                                                  Chalmers University of Technology                                                                                  |              arxiv               |                                   [LLMSecCode: Evaluating Large Language Models for Secure Coding](https://arxiv.org/abs/2408.16100)                                    |                          **Secure Coding**&**Evaluation Framework**                          |
| 24.09 |                                                                                 The Chinese University of Hong Kong                                                                                 |              arxiv               |                      [Testing and Evaluation of Large Language Models: Correctness, Non-Toxicity, and Fairness](https://arxiv.org/abs/2409.00551)                       |                        **Correctness**&**Non-Toxicity**&**Fairness**                         |
| 24.09 |                                                                                                KAIST                                                                                                |              arxiv               |                         [Evaluating Image Hallucination in Text-to-Image Generation with Question-Answering](https://arxiv.org/abs/2409.12784)                          |         **Image Hallucination**&**Text-to-Image Generation**&**Question-Answering**          |
| 24.09 |                                                                                         Zhejiang University                                                                                         |              arxiv               |               [GenTel-Safe: A Unified Benchmark and Shielding Framework for Defending Against Prompt Injection Attacks](https://arxiv.org/abs/2409.19521)               |                     **Prompt Injection**&**LLM Safety**&**Benchmarking**                     |
| 24.10 |                                                                                         Zhejiang University                                                                                         |              arxiv               |                  [AGENT SECURITY BENCH (ASB): FORMALIZING AND BENCHMARKING ATTACKS AND DEFENSES IN LLM-BASED AGENTS](https://arxiv.org/abs/2410.02644)                  |             **LLM-based Agents**&**Security Benchmarks**&**Adversarial Attacks**             |
| 24.10 |                                                                                Zhejiang University, Duke University                                                                                 |              arxiv               |              [SCISAFEEVAL: A Comprehensive Benchmark for Safety Alignment of Large Language Models in Scientific Tasks](https://arxiv.org/abs/2410.03769)               |                          **Safety Alignment**&**Scientific Tasks**                           |
| 24.10 |                                                                         The Chinese University of Hong Kong, Tencent AI Lab                                                                         |              arxiv               |                           [Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by Step](https://arxiv.org/abs/2410.03869)                            |                **Chain-of-Jailbreak**&**Image Generation Models**&**Safety**                 |
| 24.10 |                                                              University of California, Santa Cruz, University of California, Berkeley                                                               |              arxiv               |                                [Multimodal Situational Safety: A Benchmark for Large Language Models](https://arxiv.org/abs/2410.06172)                                 |               **Multimodal Situational Safety**&**MLLMs**&**Safety Benchmark**               |
| 24.10 |                                                                                            IBM Research                                                                                             |              arxiv               |                        [ST-WebAgentBench: A Benchmark for Evaluating Safety and Trustworthiness in Web Agents](https://arxiv.org/abs/2410.06703)                        |                        **Web Agents**&**Safety**&**Trustworthiness**                         |
| 24.10 |                                                Renmin University of China, Anthropic, University of Oxford, University of Edinburgh, Mila, Tangentic                                                |              arxiv               |                             [POISONBENCH: Assessing Large Language Model Vulnerability to Data Poisoning](https://arxiv.org/abs/2410.08811)                             |               **Data poisoning**&**LLM vulnerability**&**Preference learning**               |
| 24.10 |                                                                                Gray Swan AI, UK AI Safety Institute                                                                                 |              arxiv               |                                   [AGENTHARM: A Benchmark for Measuring Harmfulness of LLM Agents](https://arxiv.org/abs/2410.09024)                                    |                   **Jailbreaking**&**LLM agents**&**Harmful agent tasks**                    |
| 24.10 |                                                                                          Purdue University                                                                                          |              arxiv               |                            [COLLU-BENCH: A Benchmark for Predicting Language Model Hallucinations in Code](https://arxiv.org/abs/2410.09997)                            |           **Code hallucinations**&**Code generation**&**Automated program repair**           |
| 24.10 |                                                The Hong Kong University of Science and Technology (Guangzhou), University of Birmingham, Baidu Inc.                                                 |              arxiv               |           [JAILJUDGE: A Comprehensive Jailbreak Judge Benchmark with Multi-Agent Enhanced Explanation Evaluation Framework](https://arxiv.org/abs/2410.12855)           |                        **Jailbreak judge**&**Multi-agent framework**                         |
| 24.10 |                                                                               University of Notre Dame, IBM Research                                                                                |              arxiv               |                                       [BenchmarkCards: Large Language Model and Risk Reporting](https://arxiv.org/abs/2410.12974)                                       |                           **BenchmarkCards**&**Bias**&**Fairness**                           |
| 24.10 |                  Vectara, Inc., Iowa State University, University of Southern California, Entropy Technologies, University of Waterloo, Funix.io, University of Wisconsin, Madison                  |              arxiv               |                           [FaithBench: A Diverse Hallucination Benchmark for Summarization by Modern LLMs](https://arxiv.org/abs/2410.13210)                            |          **Hallucination detection**&**Human-annotated benchmark**&**Faithfulness**          |
| 24.10 |                                                                            Southern University of Science and Technology                                                                            |              arxiv               |                           [ChineseSafe: A Chinese Benchmark for Evaluating Safety in Large Language Models](https://arxiv.org/abs/2410.18491)                           |                    **ChineseSafe**&**Content Safety**&**LLM Evaluation**                     |
| 24.10 |                                                                                         Beihang University                                                                                          |              arxiv               |                            [SafeBench: A Safety Evaluation Framework for Multimodal Large Language Models](https://arxiv.org/abs/2410.18927)                            |   **Multimodal Large Language Models**&**Safety Evaluation Framework**&**Risk Assessment**   |
| 24.10 |                                                                                  University of Washington-Madison                                                                                   |              arxiv               |                                   [CFSafety: Comprehensive Fine-grained Safety Assessment for LLMs](https://arxiv.org/abs/2410.21695)                                   |               **Safety Assessment**&**LLM Evaluation**&**Instruction Attacks**               |
| 24.10 |                                                                                     University of Pennsylvania                                                                                      |              arxiv               |                                    [Benchmarking LLM Guardrails in Handling Multilingual Toxicity](https://arxiv.org/abs/2410.22153)                                    |         **Multilingual Toxicity Detection**&**Guardrails**&**Jailbreaking Attacks**          |
| 24.10 |                                                                                   University of Wisconsin-Madison                                                                                   |              arxiv               |                      [InjecGuard: Benchmarking and Mitigating Over-defense in Prompt Injection Guardrail Models](https://arxiv.org/abs/2410.22770)                      |         **Prompt Injection Defense**&**Over-defense Detection**&**Guardrail Models**         |
| 24.10 |                                                          National Engineering Research Center for Software Engineering, Peking University                                                           |           NeurIPS 2024           |                      [SG-Bench: Evaluating LLM Safety Generalization Across Diverse Tasks and Prompt Types](https://github.com/MurrayTom/SG-Bench)                      |                 **LLM Safety**&**Prompt Engineering**&**Jailbreak Attacks**                  |
| 24.10 | Alan Turing Institute | arxiv | [AI Cyber Risk Benchmark: Automated Exploitation Capabilities](https://arxiv.org/abs/2410.21939v2) | **Cybersecurity**&**LLMs**&**Automated Exploitation** |
| 24.11 |                                                                                          Fudan University                                                                                           |              arXiv               |                                   [LONGSAFETYBENCH: LONG-CONTEXT LLMS STRUGGLE WITH SAFETY ISSUES](https://arxiv.org/abs/2411.06899)                                    |                **Long-Context Models**&**Safety Evaluation**&**Benchmarking**                |
| 24.11 |                                                                                              Anthropic                                                                                              |              arXiv               |                                    [Rapid Response: Mitigating LLM Jailbreaks with a Few Examples](https://arxiv.org/abs/2411.07494)                                    |                           **Jailbreak Defense**&**Rapid Response**                           |
| 24.11 |                                                                                        Texas A&M University                                                                                         |              arXiv               |            [Responsible AI in Construction Safety: Systematic Evaluation of Large Language Models and Prompt Engineering](https://arxiv.org/abs/2411.08320)             |              **Construction Safety**&**Prompt Engineering**&**LLM Evaluation**               |
| 24.11 |                                                                                         IBM Research Europe                                                                                         | NeurIPS 2024 SafeGenAI Workshop  |                 [HarmLevelBench: Evaluating Harm-Level Compliance and the Impact of Quantization on Model Alignment](https://arxiv.org/abs/2411.06835)                  |          **Jailbreaking Techniques**&**LLM Vulnerability**&**Quantization Impact**           |
| 24.11 |                                                                                          Peking University                                                                                          |              arxiv               |                                    [ChemSafetyBench: Benchmarking LLM Safety on Chemistry Domain](https://arxiv.org/abs/2411.16736)                                     |                     **LLM Safety**&**Chemistry Domain**&**Benchmarking**                     |
| 24.11 |                                                             New York University, JPMorgan Chase, Cornell Tech, Northeastern University                                                              |              arxiv               |                                     [Assessment of LLM Responses to End-user Security Questions](https://arxiv.org/abs/2411.14571)                                      |              **LLM Evaluation**&**End-user Security**&**Information Integrity**              |
| 24.11 |                                                National Library of Medicine, NIH&University of Maryland&University of Virginia&Universidad de Chile                                                 |              arxiv               |                         [Ensuring Safety and Trust: Analyzing the Risks of Large Language Models in Medicine](https://arxiv.org/abs/2411.14487)                         |                     **Medical AI**&**LLM Safety**&**MedGuard Benchmark**                     |
| 24.11 |                                                                              European Commission Joint Research Centre                                                                              |            EMNLP 2024            |                              [GuardBench: A Large-Scale Benchmark for Guardrail Models](https://aclanthology.org/2024.emnlp-main.1022.pdf)                              |                      **guardrail models**&**benchmark**&**evaluation**                       |
| 24.12 |                                                                                           Vizuara AI Labs                                                                                           |              arxiv               |                            [CBEVAL: A Framework for Evaluating and Interpreting Cognitive Biases in LLMs](https://arxiv.org/abs/2412.03605)                             |              **Cognitive Biases**&**LLM Evaluation**&**Reasoning Limitations**               |
| 24.12 |                                                                         Beijing Institute of Technology, Beihang University                                                                         |              arxiv               |                            [REFF: Reinforcing Format Faithfulness in Language Models across Varied Tasks](https://arxiv.org/abs/2412.09173)                             |                            **Format Faithfulness**&**Benchmark**                             |
| 24.12 |                                                                                    UCLA, Salesforce AI Research                                                                                     |           NeurIPS 2024           |                                           [SAFEWORLD: Geo-Diverse Safety Alignment](https://github.com/PlusLabNLP/SafeWorld)                                            |             **Geo-Diverse Alignment**&**Safety Evaluation**&**Legal Compliance**             |
| 24.12 |                                                                                    Shanghai Jiao Tong University                                                                                    |              arxiv               |                              [SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents](https://arxiv.org/abs/2412.13178)                              |         **Safety-Aware Task Planning**&**Embodied LLM Agents**&**Hazard Mitigation**         |
| 24.12 |                                                                                         Tsinghua University                                                                                         |              arxiv               |                                       [AGENT-SAFETYBENCH: Evaluating the Safety of LLM Agents](https://arxiv.org/abs/2412.14470)                                        |                **Agent Safety**&**Risk Awareness**&**Interactive Evaluation**                |
| 24.12 |                                                                                            TU Darmstadt                                                                                             |              arxiv               |                               [LLMs Lost in Translation: M-ALERT Uncovers Cross-Linguistic Safety Gaps](https://arxiv.org/abs/2412.15035)                               |           **Cross-Linguistic Safety**&**Multilingual Benchmark**&**LLM Alignment**           |
| 24.12 |                                                                 Alibaba, China Academy of Information and Communications Technology                                                                 |              arxiv               |                        [Chinese SafetyQA: A Safety Short-form Factuality Benchmark for Large Language Models](https://arxiv.org/abs/2412.15265)                         |                        **Safety Benchmark**&**Factuality Evaluation**                        |
| 24.12 |                                                                             University of Warwick, Cranfield University                                                                             |              arxiv               |                     [MedHallBench: A New Benchmark for Assessing Hallucination in Medical Large Language Models](https://arxiv.org/abs/2412.18947)                      |                      **Medical Hallucinations**&**Benchmark**&**RLHF**                       |
| 24.12 |                                                                                The Hong Kong Polytechnic University                                                                                 |              arxiv               |                     [SecBench: A Comprehensive Multi-Dimensional Benchmarking Dataset for LLMs in Cybersecurity](https://arxiv.org/abs/2412.20787)                      |         **Cybersecurity Benchmark**&**Large Language Models**&**Dataset Evaluation**         |
| 25.01 |                                                                                  KTH Royal Institute of Technology                                                                                  |              arxiv               |             [CySecBench: Generative AI-based CyberSecurity-focused Prompt Dataset for Benchmarking Large Language Models](https://arxiv.org/abs/2501.01335)             |               **Cybersecurity Benchmark**&**Jailbreaking**&**Prompt Dataset**                |
| 25.01 |                                                                           Shahjalal University of Science and Technology                                                                            |              arxiv               |                   [From Scarcity to Capability: Empowering Fake News Detection in Low-Resource Languages with LLMs](https://arxiv.org/abs/2501.09604)                   |                **Fake News Detection**&**Bangla**&**Low-Resource Languages**                 |
| 25.01 |                                                                                               NVIDIA                                                                                                |              arxiv               |                      [AEGIS2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails](https://arxiv.org/abs/2501.09004)                       |              **AI Safety**&**Content Moderation Dataset**&**LLM Risk Taxonomy**              |
| 25.01 |                                                                                   Georgia Institute of Technology                                                                                   |              arxiv               |                 [On The Origin of Cultural Biases in Language Models: From Pre-training Data to Linguistic Phenomena](https://arxiv.org/abs/2501.04662)                 |    **Cultural Bias in LLMs**&**Cross-Linguistic Analysis**&**Arabic-English Benchmarks**     |
| 25.01 |                                                                                         Bocconi University                                                                                          |              arxiv               |                                   [MSTS: A Multimodal Safety Test Suite for Vision-Language Models](https://arxiv.org/abs/2501.10057)                                   |                       **Multimodal Safety**&**Vision-Language Models**                       |
| 25.01 |                                                                                          Fudan University                                                                                           |              arxiv               |                 [You Can't Eat Your Cake and Have It Too: The Performance Degradation of LLMs with Jailbreak Defense](https://arxiv.org/abs/2501.12210)                 |                    **Jailbreak Defense**&**LLM Performance**&**USEBench**                    |
| 25.01 |                                                                                          McGill University                                                                                          |              arxiv               |                  [OnionEval: A Unified Evaluation of Fact-conflicting Hallucination for Small-Large Language Models](https://arxiv.org/abs/2501.12975)                  |   **Fact-conflicting Hallucination**&**Small-Large Language Models (SLLMs)**&**Benchmark**   |
| 25.01 |                                                                                                HKUST                                                                                                |              arxiv               |         [Tune In, Act Up: Exploring the Impact of Audio Modality-Specific Edits on Large Audio Language Models in Jailbreak](https://arxiv.org/abs/2501.13772)          |       **Audio Language Models**&**Jailbreak Vulnerabilities**&**Audio Modality Edits**       |
| 25.01 |                                                                                       University of Cambridge                                                                                       |              arxiv               |                           [CASE-BENCH: Context-Aware Safety Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2501.14940)                           |                 **LLM Safety**&**Context-Aware Evaluation**&**Over-Refusal**                 |
| 25.01 |                                                                           CISPA Helmholtz Center for Information Security                                                                           |       USENIX Security 2025       |                      [HATEBENCH: Benchmarking Hate Speech Detectors on LLM-Generated Content and Hate Campaigns](https://arxiv.org/abs/2501.16750)                      |            **Hate Speech Detection**&**LLM-Generated Content**&**Hate Campaigns**            |
| 25.01 |                                                                   Shanghai Artificial Intelligence Laboratory, Tianjin University                                                                   |              arxiv               |                              [Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models](https://arxiv.org/abs/2501.18533v1)                               |                 **Vision-Language Models (VLMs)**&**Chain-of-Thought (CoT)**                 |
| 25.01 |                                                                             Independent Research Team “Annyeong! Luda”                                                                              |            PACLIC 38             |                                 [RICoTA: Red-teaming of In-the-wild Conversation with Test Attempts](https://arxiv.org/abs/2501.17715)                                  |               **Jailbreaking**&**Conversational AI**&**User Intent Detection**               |
| 25.01 |                                                                                     Renmin University of China                                                                                      |              arxiv               |                      [SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2501.18636)                      |     **Retrieval-Augmented Generation**&**Security Benchmarking**&**Adversarial Attacks**     |
| 25.02 |                                                                                  Rochester Institute of Technology                                                                                  |              arxiv               | [HOPE VS. HATE: UNDERSTANDING USER INTERACTIONS WITH LGBTQ+ NEWS CONTENT IN MAINSTREAM US NEWS MEDIA THROUGH THE LENS OF HOPE SPEECH](https://arxiv.org/abs/2502.09004) |                        **Hope Speech**&**LGBTQ+**&**Political Bias**                         |
| 25.02 |                                                                                         Nanjing University                                                                                          |              arxiv               |   [SafeDialBench: A Fine-Grained Safety Benchmark for Large Language Models in Multi-Turn Dialogues with Diverse Jailbreak Attacks](https://arxiv.org/abs/2502.11090)   |             **Safety Benchmark**&**Jailbreak Attacks**&**Multi-Turn Dialogues**              |
| 25.02 |                                                                                            China Unicom                                                                                             |              arxiv               |                                      [Safety Evaluation of DeepSeek Models in Chinese Contexts](https://arxiv.org/abs/2502.11137)                                       |                **DeepSeek Models**&**Safety Evaluation**&**Chinese Contexts**                |
| 25.02 |                                                                           Hong Kong University of Science and Technology                                                                            |              arxiv               |                  [Can’t See the Forest for the Trees: Benchmarking Multimodal Safety Awareness for Multimodal LLMs](https://arxiv.org/abs/2502.11184)                   |                  **Multimodal LLMs**&**Safety Awareness**&**Benchmarking**                   |
| 25.02 |                                                                                         Columbia University                                                                                         |              arXiv               |                    [TREECUT: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation](https://arxiv.org/abs/2502.13442)                     |              **Math Word Problem**&**LLM Hallucination**&**Synthetic Dataset**               |
| 25.02 |                                                                                Barcelona Supercomputing Center (BSC)                                                                                |              arXiv               |                                     [Efficient Safety Retrofitting Against Jailbreaking for LLMs](https://arxiv.org/abs/2502.13603)                                     |                     **Model Alignment**&**LLM Safety**&**Jailbreaking**                      |
| 25.02 |                                                                         The Hong Kong University of Science and Technology                                                                          |              arxiv               |                                     [GuidedBench: Equipping Jailbreak Evaluation with Guidelines](https://arxiv.org/abs/2502.16903)                                     |                   **Jailbreak Evaluation**&**AI Safety**&**LLM Security**                    |
| 25.02 |                                                                                          Wuhan University                                                                                           |              arxiv               |                       [ELBA-Bench: An Efficient Learning Backdoor Attacks Benchmark for Large Language Models](https://arxiv.org/abs/2502.18511)                        |                    **Backdoor Attacks**&**LLM Security**&**Benchmarking**                    |
| 25.02 |                                                                                         Tsinghua University                                                                                         |              arxiv               |                                 [LongSafety: Evaluating Long-Context Safety of Large Language Models](https://arxiv.org/abs/2502.16971)                                 |               **Long-Context Safety**&**LLM Evaluation**&**Safety Benchmark**                |
| 25.02 |                                                                                                KAIST                                                                                                |              arxiv               |                   [Is Safety Standard Same for Everyone? User-Specific Safety Evaluation of Large Language Models](https://arxiv.org/abs/2502.15086)                    |               **User-Specific Safety**&**LLM Evaluation**&**Safety Benchmark**               |
| 25.02 |                                                                         Beijing University of Posts and Telecommunications                                                                          |              arxiv               |                     [JailBench: A Comprehensive Chinese Security Assessment Benchmark for Large Language Models](https://arxiv.org/abs/2502.18935)                      |              **Chinese Benchmark**&**Security Assessment**&**Jailbreak Attack**              |
| 25.02 |                                                                                            IBM Research                                                                                             | NeurIPS 2024, SafeGenAI Workshop |              [Adversarial Prompt Evaluation: Systematic Benchmarking of Guardrails Against Prompt Input Attacks on LLMs](https://arxiv.org/abs/2502.15427)              |         **Adversarial Prompting**&**Jailbreak Detection**&**Guardrail Benchmarking**         |
| 25.03 |                                                                                     Google DeepMind, ETH Zurich                                                                                     |              arXiv               |                        [AutoAdvExBench: Benchmarking Autonomous Exploitation of Adversarial Example Defenses](https://arxiv.org/abs/2503.01811)                         |        **Adversarial Example Defenses**&**LLM Security**&**Autonomous Exploitation**         |
| 25.03 |                                                                          The Pennsylvania State University, GE Healthcare                                                                           |              arXiv               |               [MedHEval: Benchmarking Hallucinations and Mitigation Strategies in Medical Large Vision-Language Models](https://arxiv.org/abs/2503.02157)               |      **Medical Hallucination**&**Hallucination Mitigation**&**Vision-Language Models**       |
| 25.03 |                                                       Nanyang Technological University, Agency for Science, Technology and Research (A*STAR)                                                        |              arXiv               |                  [Benchmarking LLMs and LLM-based Agents in Practical Vulnerability Detection for Code Repositories](https://arxiv.org/abs/2503.03586)                  |    **Software Vulnerability Detection**&**LLM-based Agents**&**Interprocedural Analysis**    |
| 25.03 |                                                                                         Zhejiang University                                                                                         |              arxiv               |                       [Can Small Language Models Reliably Resist Jailbreak Attacks? A Comprehensive Evaluation](https://arxiv.org/abs/2503.06519)                       |            **Small Language Models**&**Jailbreak Attack**&**Security Evaluation**            |
| 25.03 |                                                                                       University of Virginia                                                                                        |              arxiv               |                                            [Benchmarking Group Fairness in Reward Models](https://arxiv.org/abs/2503.07806)                                             |                    **Group Fairness**&**Reward Models**&**LLM Alignment**                    |
| 25.03 |                                                                                        Texas A&M University                                                                                         |              arxiv               |                          [HalluVerse25: Fine-grained Multilingual Benchmark Dataset for LLM Hallucinations](https://arxiv.org/abs/2503.07833)                           |      **Hallucination Detection**&**Multilingual Benchmark**&**Fine-grained Annotation**      |
| 25.03 |                                                                                         University of Kent                                                                                          |              arxiv               |                  [CyberLLMInstruct: A New Dataset for Analysing Safety of Fine-Tuned LLMs Using Cyber Security Data](https://arxiv.org/abs/2503.09334)                  |              **Cyber Security**&**Fine-Tuning Safety**&**Instruction Dataset**               |
| 25.03 |                                                                                            UC San Diego                                                                                             |              arxiv               |                            [ExtremeAIGC: Benchmarking LMM Vulnerability to AI-Generated Extremist Content](https://arxiv.org/abs/2503.09964)                            |           **Multimodal Jailbreaking**&**Extremist Content**&**Safety Evaluation**            |
| 25.03 |                                                                                         Beihang University                                                                                          |              arxiv               |                       [Towards Understanding the Safety Boundaries of DeepSeek Models: Evaluation and Findings](https://arxiv.org/abs/2503.15092)                       |               **Safety Evaluation**&**Jailbreak Attacks**&**DeepSeek Models**                |
| 25.03 |                                                                                       George Mason University                                                                                       |              arxiv               |                           [Efficient but Vulnerable: Benchmarking and Defending LLM Batch Prompting Attack](https://arxiv.org/abs/2503.15551)                           |               **Batch Prompting**&**Prompt Injection Attack**&**LLM Security**               |
| 25.03 |                                                                                      Arizona State University                                                                                       |              arxiv               |          [Personalized Attacks of Social Engineering in Multi-turn Conversations - LLM Agents for Simulation and Detection](https://arxiv.org/abs/2503.15552)           |              **Social Engineering**&**Multi-turn Conversation**&**LLM Agents**               |
| 25.03 |                                                                               University of Illinois Urbana-Champaign                                                                               |            ICLR 2025             |                            [MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models](https://arxiv.org/abs/2503.14827)                            |     **Multimodal Foundation Models**&**Trustworthiness Evaluation**&**Safety Benchmark**     |
| 25.03 |                                                                                        University of Chicago                                                                                        |              arxiv               |                                [SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning](https://arxiv.org/abs/2503.22738)                                 |                  **LLM Agents**&**Safety Policy Reasoning**&**Guardrails**                   |
| 25.04 |                                                                                      University College London                                                                                      |              arxiv               |                                       [On Benchmarking Code LLMs for Android Malware Analysis](https://arxiv.org/abs/2504.00694)                                        |             **Code LLMs**&**Android Malware Analysis**&**Structured Evaluation**             |
| 25.04 |                                                                                            UC Santa Cruz                                                                                            |              arxiv               |                                       [STAR-1: Safer Alignment of Reasoning LLMs with 1K Data](https://arxiv.org/abs/2504.01903)                                        |               **Reasoning LLMs**&**Safety Alignment**&**High-Quality Dataset**               |
| 25.04 |                                                                                  National University of Singapore                                                                                   |              arxiv               |                                 [SafeMLRM: Demystifying Safety in Multi-modal Large Reasoning Models](https://arxiv.org/abs/2504.08813)                                 |                   **MLRM**&**Safety Evaluation**&**Jailbreaking Attacks**                    |
| 25.04 |                                                                                         Tsinghua University                                                                                         |              arxiv               |                          [RealSafe-R1: Safety-Aligned DeepSeek-R1 without Compromising Reasoning Capability](https://arxiv.org/abs/2504.10081)                          |               **Safety Alignment**&**DeepSeek-R1**&**Large Reasoning Models**                |
| 25.04 |                                                                         The Hong Kong University of Science and Technology                                                                          |              arxiv               |                                [Benchmarking Multi-National Value Alignment for Large Language Models](https://arxiv.org/abs/2504.12911)                                |           **LLM Hallucination**&**Value Alignment**&**Cross-National Evaluation**            |
| 25.04 |                                                                                 Qatar Computing Research Institute                                                                                  |              arxiv               |                                            [AIXAMINE: SIMPLIFIED LLM SAFETY AND SECURITY](https://arxiv.org/abs/2504.14985)                                             |              **LLM Safety**&**Security Evaluation**&**Black-box Benchmarking**               |
| 25.04 |                                                                                 University of California, Berkeley                                                                                  |              arxiv               |                              [JailbreaksOverTime: Detecting Jailbreak Attacks Under Distribution Shift](https://arxiv.org/abs/2504.19440)                               |              **Jailbreak Detection**&**Concept Drift**&**Continuous Learning**               |
| 25.04 |                                                                                               Intuit                                                                                                |              arxiv               |                                                [Security Steerability is All You Need](https://arxiv.org/abs/2504.19521)                                                |              **Security Steerability**&**Prompt Guardrails**&**LLM Robustness**              |
| 25.05 |                                                                                  Rutgers University-New Brunswick                                                                                   |              arxiv               |                 [Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety](https://arxiv.org/abs/2505.04146)                  |              **Jailbreaking**&**Image Generation**&**Multilingual Obfuscation**              |
| 25.05 |                                                                                     Beijing Jiaotong University                                                                                     |              arxiv               |              [Think in Safety: Unveiling and Mitigating Safety Alignment Collapse in Multimodal Large Reasoning Model](https://arxiv.org/abs/2505.06538v1)              |       **Multimodal Large Reasoning Models**&**Safety Alignment**&**Chain-of-Thought**        |
| 25.05 |                                                                             Shanghai Artificial Intelligence Laboratory                                                                             |              arxiv               |      [Benchmarking Ethical and Safety Risks of Healthcare LLMs in China – Toward Systemic Governance under Healthy China 2030](https://arxiv.org/abs/2505.07205v1)      |            **Medical LLMs**&**Ethical Risk Assessment**&**Governance Framework**             |
| 25.05 |                                                                                   University of Technology Sydney                                                                                   |              arxiv               |                        [SecReEvalBench: A Security Resilient Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2505.07584v1)                        |             **Prompt Attack**&**Security Benchmark**&**Adversarial Robustness**              |
| 25.05 |                                                                                          Dartmouth College                                                                                          |              arxiv               |       [FalseReject: A Resource for Improving Contextual Safety and Mitigating Over-Refusals in LLMs via Structured Reasoning](https://arxiv.org/abs/2505.08054v1)       |        **Over-Refusal Mitigation**&**Structured Reasoning**&**LLM Safety Evaluation**        |
| 25.05 |                                                                                  National University of Singapore                                                                                   |              arxiv               |                                    [GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning](https://arxiv.org/abs/2505.11049)                                     |           **VLM Guard Models**&**Reinforcement Learning**&**Reasoning Alignment**            |
| 25.05 |                                                                                          Fudan University                                                                                           |              arxiv               |                       [Think Twice Before You Act: Enhancing Agent Behavioral Safety with Thought Correction](https://arxiv.org/abs/2505.11063v2)                       |                  **LLM Agent**&**Behavioral Safety**&**Thought Correction**                  |
| 25.05 |                                                                                             Giskard AI                                                                                              |              arxiv               |                                           [Phare: A Safety Probe for Large Language Models](https://arxiv.org/abs/2505.11365)                                           |             **Safety Evaluation**&**Hallucination Detection**&**Bias Diagnosis**             |
| 25.05 |                                                                         Beijing University of Posts and Telecommunications                                                                          |              arxiv               |                                 [Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs](https://arxiv.org/abs/2505.11842)                                 |                  **Safety Evaluation**&**Video LVLMs**&**Prompt Injection**                  |
| 25.05 |                                                                                          Yonsei University                                                                                          |              arxiv               |              [Better Safe Than Sorry? Overreaction Problem of Vision Language Models in Visual Emergency Recognition](https://arxiv.org/abs/2505.15367v1)               |     **Visual Emergency Recognition**&**Vision-Language Models**&**Contextual Reasoning**     |
| 25.05 |                                                                                               POSTECH                                                                                               |              arxiv               |                             [Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark Study](https://arxiv.org/abs/2505.15389v1)                             |                  **VLM Safety**&**Meme Benchmark**&**Multimodal Alignment**                  |
| 25.05 |                                                                       Mohamed bin Zayed University of Artificial Intelligence                                                                       |              arxiv               |                   [Audio Jailbreak: An Open Comprehensive Benchmark for Jailbreaking Large Audio-Language Models](https://arxiv.org/abs/2505.15406v1)                   |          **Audio-Language Models**&**Jailbreak Attacks**&**Adversarial Benchmark**           |
| 25.05 |                                                                                     Fujitsu Research of Europe                                                                                      |              arxiv               |                             [MAPS: A Multilingual Benchmark for Global Agent Performance and Security](https://arxiv.org/abs/2505.15935v1)                              |                 **Agentic AI**&**Multilingual Evaluation**&**LLM Security**                  |
| 25.05 |                                                                                  Nanyang Technological University                                                                                   |              arxiv               |                     [AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models](https://arxiv.org/abs/2505.16211v1)                      |             **Audio LLMs**&**Trustworthiness Evaluation**&**Multimodal Safety**              |
| 25.05 |                                                                            University of Science and Technology of China                                                                            |              arxiv               |                            [From Evaluation to Defense: Advancing Safety in Video Large Language Models](https://arxiv.org/abs/2505.16643v1)                            |                  **Video LLMs**&**Safety Benchmark**&**Multimodal Defense**                  |
| 25.05 | Hong Kong University of Science and Technology (Guangzhou)| arxiv | [JALMBench: Benchmarking Jailbreak Vulnerabilities in Audio Language Models](https://arxiv.org/abs/2505.17568v1) | **Jailbreak Benchmark**&**Audio Language Model**&**Adversarial Attack** |
| 25.05 | University of Washington | arxiv | [SOS BENCH: Benchmarking Safety Alignment on Scientific Knowledge](https://arxiv.org/abs/2505.21605) | **Safety Alignment**&**Scientific Knowledge**&**Benchmark** |
| 25.05 | John Hopkins University | arxiv | [Jailbreak Distillation: Renewable Safety Benchmarking](https://arxiv.org/abs/2505.22037) | **Jailbreak Distillation**&**Safety Benchmark**&**Prompt Selection** |


## 📚Resource

- Toxicity - [RealToxicityPrompts datasets](https://toxicdegeneration.allenai.org/)
- Truthfulness - [TruthfulQA datasets](https://github.com/sylinrl/TruthfulQA)
- TRUSTLLM - [TRUSTLLM](https://trustllmbenchmark.github.io/TrustLLM-Website/)
- Protection - [CircleGuardBench](https://github.com/whitecircle-ai/circle-guard-bench)
