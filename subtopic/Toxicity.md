# Toxicity

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating witha the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"


## üìëPapers

| Date  |                                                                                         Institute                                                                                          |          Publication           |                                                                                                                       Paper                                                                                                                        |                                          Keywords                                           |
|:-----:|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:------------------------------:|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------------:|
| 20.10 |                                                                                    Facebook AI Research                                                                                    |             arxiv              |                                                                                   [Recipes for Safety in Open-domain Chatbots](https://arxiv.org/abs/2010.07079)                                                                                   |                             **Toxic Behavior**&**Open-domain**                              |
| 22.11 |                                                                            University of California, Santa Cruz                                                                            |           NAACL2024            |                                                         [Navigation as Attackers Wish? Towards Building Robust Embodied Agents under Federated Learning](https://arxiv.org/abs/2211.14769)                                                         |           **Federated Learning**&**Vision-and-Language Navigation**&**Security**            |
| 23.05 |                                                                Harvard&UCLA&USC&University of Wisconsin, Madison&UC, Davis                                                                 |           NAACL2024            |                                                      [Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models](https://arxiv.org/abs/2305.14710)                                                       |            **Instruction Tuning**&**Large Language Models**&**Backdoor Attacks**            |
| 23.06 |                                                                         University of Illinois at Urbana-Champaign                                                                         |             arxiv              |                                                                   [DECODINGTRUST: A Comprehensive Assessment of Trustworthiness in GPT Models](https://arxiv.org/abs/2306.11698)                                                                   |                     **Robustness**&**Ethics**&**Privacy**&**Toxicity**                      |
| 23.10 |                                                                      CISPA Helmholtz Center for Information Security                                                                       |      NAACL2024(findings)       |                                                                            [Composite Backdoor Attacks Against Large Language Models](https://arxiv.org/abs/2310.07676)                                                                            |                 **Backdoor Attacks**&**Large Language Models**&**Security**                 |
| 23.11 |                                                                               Pennsylvania State University                                                                                |             arxiv              |                                           [A Taxonomy of Rater Disagreements: Surveying Challenges & Opportunities from the Perspective of Annotating Online Toxicity](https://arxiv.org/abs/2311.04345)                                           |                                   **Toxicity**&**Survey**                                   |
| 23.11 |                                                                          University of Maryland, Baltimore County                                                                          |             arxiv              |                                                      [Determination of toxic comments and unintended model bias minimization using Deep learning approach](https://arxiv.org/abs/2311.04789)                                                       |                             **Toxicity**&**Detection**&**Bias**                             |
| 23.11 |                                                                               University of Central Florida                                                                                |             arxiv              |                                                                        [THOS: A Benchmark Dataset for Targeted Hate and Offensive Speech](https://arxiv.org/abs/2311.06446)                                                                        |                      **Hate Speech**&**Offensive Speech**&**Dataset**                       |
| 23.11 |                                                                                         FAIR Meta                                                                                          |             arXiv              |                                                       [Added Toxicity Mitigation at Inference Time for Multimodal and Massively Multilingual Translation](https://arxiv.org/abs/2311.06532)                                                        |       **Multimodal Translation**&**Multilingual Translation**&**Toxicity Mitigation**       |
| 23.11 |                                                                University of Haifa, OriginAI, Braude College of Engineering                                                                |             arXiv              |                                                                        [Generative AI for Hate Speech Detection: Evaluation and Findings](https://arxiv.org/abs/2311.09993)                                                                        |             **Hate Speech Detection**&**Synthetic Data**&&**Data Augmentation**             |
| 23.11 |                                             Seoul National University, Chung-Ang University, NAVER AI Lab, NAVER Cloud, University of Richmond                                             |             arxiv              |                                                                              [LifeTox: Unveiling Implicit Toxicity in Life Advice](https://arxiv.org/abs/2311.09585)                                                                               |            **LifeTox Dataset**&**Toxicity Detection**&**Social Media Analysis**             |
| 23.11 |                                                                                     Amazon Alexa AI-NU                                                                                     |             arXiv              |                                                                            [JAB: Joint Adversarial Prompting and Belief Augmentation](https://arxiv.org/abs/2311.09473)                                                                            |               **Adversarial Prompting**&T**oxicity Reduction**&**Robustness**               |
| 23.11 |                                                                      CISPA Helmholtz Center for Information Security                                                                       |             arxiv              |                                                                                [Comprehensive Assessment of Toxicity in ChatGPT](https://arxiv.org/abs/2311.14685)                                                                                 |                   **Toxicity Assessment**&**Instruction-tuning Datasets**                   |
| 23.11 |                                                                          Tsinghua University, TAL Education Group                                                                          |             arxiv              |                                                                            [Unveiling the Implicit Toxicity in Large Language Models](https://arxiv.org/abs/2311.17391)                                                                            |           **Implicit Toxicity**&**Toxicity Detection**&**Reinforcement Learning**           |
| 23.11 |                                                                                            Meta                                                                                            |           EMNLP2023            |                                                                       [ROBBIE: Robust Bias Evaluation of Large Generative Language Models](https://arxiv.org/abs/2311.18140)                                                                       |                        **Bias Evaluation**&**Fairness**&**Toxicity**                        |
| 23.12 |                                                                                         Anthropic                                                                                          |             arxiv              |                                                                      [Evaluating and Mitigating Discrimination in Language Model Decisions](https://arxiv.org/abs/2312.03689)                                                                      |             **Discrimination**&**High-Stakes Decisions**&**Societal Decisions**             |
| 23.12 |                                                                                      Ajou University                                                                                       |             arXiv              |                                                                         [GTA: Gated Toxicity Avoidance for LM Performance Preservation](https://arxiv.org/abs/2312.06122)                                                                          |    **Toxicity Avoidance**&**Gated Toxicity Avoidance**&**Controllable Text Generation**     |
| 23.12 |                                                      Hong Kong Baptist University; The Hong Kong University of Science and Technology                                                      |             arXiv              |                                                  [Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models](https://arxiv.org/abs/2312.05434)                                                   |                            **Harmful**&**Multimodal Reasoning**                             |
| 23.12 |                                                                     University of Southern California, Amazon.com Inc.                                                                     |             arxiv              |                                                            [Efficient Toxic Content Detection by Bootstrapping and Distilling Large Language Models](https://arxiv.org/abs/2312.08303)                                                             |         **Toxic Content Detection**&**Bootstrapping**&**Decision-Tree-of-Thought**          |
| 23.12 |                                                       University of Texas at San Antonio, University at Buffalo, Clemson University                                                        |            S&P2024             |                                                          [Moderating New Waves of Online Hate with Chain-of-Thought Reasoning in Large Language Models](https://arxiv.org/abs/2312.15099)                                                          |                       **Online Hate**&**Chain-of-Thought Reasoning**                        |
| 23.12 |                                     University of Central Florida, Samsung Research America, University of Pittsburgh, Indiana University Bloomington                                      |           NAACL2024            |                                                                              [TrojFSP: Trojan Insertion in Few-shot Prompt Tuning](https://arxiv.org/abs/2312.10467)                                                                               |                **Prompt Tuning**&**Backdoor Attacks**&**Few-shot Learning**                 |
| 23.12 |                                                                             The Pennsylvania State University                                                                              |           NAACL2024            |                                                              [Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections](https://arxiv.org/abs/2312.00027)                                                              |             **Backdoor Injections**&**LLM Security**&**Persistent Unalignment**             |
| 24.01 |                                                         University of Michigan Ann Arbor, Harvard University, University of Sydney                                                         |             arxiv              |                                                             [A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity](https://arxiv.org/abs/2401.01967)                                                              |       **Alignment Algorithms**&**Toxicity**&**Direct Preference Optimization (DPO)**        |
| 24.01 |                                                                Algorix Convergence Research Office, Centennial High School                                                                 |             arxiv              |                                                               [EFFICACY OF UTILIZING LARGE LANGUAGE MODELS TO DETECT PUBLIC THREAT POSTED ONLINE](https://arxiv.org/abs/2401.02974)                                                                |                          **Content Moderation**&**Public Threat**                           |
| 24.01 |                    University at Buffalo, University of Texas at San Antonio, Union County Magnet High School, East Chapel Hill High School, Minhang Crosspoint Academy                    |           ICMLA2023            |                                                                 [An Investigation of Large Language Models for Real-World Hate Speech Detection](https://arxiv.org/abs/2401.03346)                                                                 |                **Hate Speech**&**Prompt Engineering**&**Few-shot Learning**                 |
| 24.01 |                                                                    IRLab CITIC Research Centre, Universidade da Coru√±a                                                                     |             arxiv              |                                                                       [MetaHate: A Dataset for Unifying Efforts on Hate Speech Detection](https://arxiv.org/abs/2401.06526)                                                                        |                         **Hate Speech Detection**&**Social Media**                          |
| 24.02 |                                                                               University of Brighton, Ofgem                                                                                |             arxiv              |                                                                         [The Use of a Large Language Model for Cyberbullying Detection](https://arxiv.org/abs/2402.04088)                                                                          |                         **Cyberbullying&**Social Media Analytics**                          |
| 24.02 |                                                                                 Seoul National University                                                                                  |             arxiv              |                                                       [Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric](https://arxiv.org/abs/2402.06900)                                                       |                       **Toxicity Detection**&**Semantic Evaluation**                        |
| 24.02 |                                                               University of Pisa&University of Edinburgh&Bocconi University                                                                |             arxiv              |                                                                           [FAIRBELIEF ‚Äì Assessing Harmful Beliefs in Language Models](https://arxiv.org/abs/2402.17389)                                                                            |                             **Beliefs Assessment**&**Fairness**                             |
| 24.02 |                                                                              Nanyang Technological University                                                                              |      NAACL2024(findings)       |                                                            [Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2402.12168)                                                             |                    **Backdoor Attacks**&**Fine-Tuning**&**NLP Security**                    |
| 24.03 |                             Indian Institute of Technology (IIT) Jodhpur, Indraprastha Institute of Information Technology Delhi (IIIT-D), IBM Research India                              | demonstration track of AAAI-24 |                                                                                 [LLMGuard: Guarding against Unsafe LLM Behavior](https://arxiv.org/abs/2403.00826)                                                                                 |                        **Safety**&**Content Flagging**&**Detectors**                        |
| 24.03 |                                                                                       Cohere for AI                                                                                        |             arxiv              |                                                                [From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models](https://arxiv.org/abs/2403.03893)                                                                 | **Multilingual Toxicity Mitigation**&**Translated Data**&**Retrieval-Augmented Techniques** |
| 24.03 |                                                                                  Arizona State University                                                                                  |             arxiv              |                             [Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection](https://arxiv.org/abs/2403.08035)                             |                      **Hate Speech Detection**&**Text Classification**                      |
| 24.03 |                                                           McGill University, University of Montreal, Mila - Quebec AI Institute                                                            |             arxiv              |                                                       [From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards](https://arxiv.org/abs/2403.13213)                                                       |                             **Safety**&**Biases**&**Toxicity**                              |
| 24.03 |     Zhejiang University, Zhejiang University-Ant Group Joint Laboratory of Knowledge Graph, Ant Group, National University of Singapore, Westlake University, Microsoft Research Asia      |             arxiv              |                                                                            [Detoxifying Large Language Models via Knowledge Editing](https://arxiv.org/abs/2403.14472)                                                                             |                 **Knowledge Editing**&**Toxicity Mitigation**&**Benchmark**                 |
| 24.03 |                                                            University of North Texas, Peking University, University of Arizona                                                             |             arxiv              |                                                                      [Outcome-Constrained Large Language Models for Countering Hate Speech](https://arxiv.org/abs/2403.17146)                                                                      |                              **Counterspeech**&**Hate Speech**                              |
| 24.03 | The World Bank, University of Oxford, New York University, Universidade Federal de Goi√°s, Ecole Normale Sup√©rieure, Universiti Sultan Zainal Abidin, Massachusetts Institute of Technology |             arxiv              |                                                           [NAIJAHATE: Evaluating Hate Speech Detection on Nigerian Twitter Using Representative Data](https://arxiv.org/abs/2403.19260)                                                            |           **Hate Speech Detection**&**Nigerian Twitter**&**Representative Data**            |
| 24.05 |                                                                                 Carnegie Mellon University                                                                                 |             arxiv              |                                                     [PolygloToxicityPrompts: Multilingual Evaluation of Neural Toxic Degeneration in Large Language Models](https://arxiv.org/abs/2405.09373)                                                      |                           **Multilingual Evaluation**&**Datasets*                           |
| 24.05 |                                                                                  Johns Hopkins University                                                                                  |      NAACL2024(findings)       | [MICo: Preventative Detoxification of Large Language Models through Inhibition Control](https://assets.amazon.science/e7/94/facc166449eba31223cbc88614a9/mico-preventative-detoxification-of-large-language-models-through-inhibition-control.pdf) |              **Detoxification**&**Inhibition Control**&**Toxicity Reduction**               |
| 24.05 |                                                                                        GSAI POSTECH                                                                                        |      NAACL2024(findings)       |                                       [Adversarial DPO: Harnessing Harmful Data for Reducing Toxicity with Minimal Impact on Coherence and Evasiveness in Dialogue Agents](https://arxiv.org/abs/2405.12900)                                       |            **Adversarial Training**&**Toxicity Reduction**&**Dialogue Systems**             |
| 24.06 |                                                                                      Brown University                                                                                      |             arxiv              |                                                                     [Preference Tuning For Toxicity Mitigation Generalizes Across Languages](https://arxiv.org/abs/2406.16235)                                                                     |       **Toxicity Mitigation**&**Cross-Lingual Generalization**&**Preference Tuning**        |
| 24.07 |                                                                       Huazhong University of Science and Technology                                                                        |             arxiv              |                                                                                     [On the (In)Security of LLM App Stores](https://arxiv.org/abs/2407.08422)                                                                                      |                         **LLM App Stores**&**Security**&**Privacy**                         |
| 24.07 |                                                                                    Tsinghua University                                                                                     |             arxiv              |                                                                     [Model Surgery: Modulating LLM‚Äôs Behavior Via Simple Parameter Editing](https://arxiv.org/abs/2407.08770)                                                                      |                          **Parameter Editing**&**Detoxification**                           |
| 24.07 |                                                                                    Stanford University                                                                                     |             arxiv              |                                                      [ASTPrompter: Weakly Supervised Automated Language Model Red-Teaming to Identify Likely Toxic Prompts](https://arxiv.org/abs/2407.09447)                                                      |         **Red-Teaming**&**Toxic Prompt Identification**&**Reinforcement Learning**          |
| 24.07 |                                                                                  University of Rochester                                                                                   |             arxiv              |                                                       [Evolver: Chain-of-Evolution Prompting to Boost Large Multimodal Models for Hateful Meme Detection](https://arxiv.org/abs/2407.21004)                                                        |            **Large Multimodal Models**&**Hateful Meme Detection**&**Prompting**             |
| 24.08 |                                                                                    University of Ottawa                                                                                    |             arxiv              |                                                  [HateSieve: A Contrastive Learning Framework for Detecting and Segmenting Hateful Content in Multimodal Memes](https://arxiv.org/abs/2408.05794)                                                  |         **Hateful Content Detection**&**Contrastive Learning**&**Multimodal Memes**         |
| 24.08 |                                                                              National University of Singapore                                                                              |          eCrime 2024           |                                                               [Multimodal Large Language Models for Phishing Webpage Detection and Identification](https://arxiv.org/abs/2408.05941)                                                               |             **Phishing Detection**&**Multimodal LLMs**&**Brand Identification**             |
| 24.08 |                                                                              Nanyang Technological University                                                                              |            ASE 2024            |                                                                         [Efficient Detection of Toxic Prompts in Large Language Models](https://arxiv.org/abs/2408.11727)                                                                          |                           **Toxic Prompts**&**Jailbreak Attacks**                           |
| 24.10 |                                                                                   University of A Coru√±a                                                                                   |             arxiv              |                                                                       [Decoding Hate: Exploring Language Models' Reactions to Hate Speech](https://arxiv.org/abs/2410.00775)                                                                       |                          **Hate Speech**&**Mitigation Strategies**                          |
| 24.10 |                                                                              Dalian University of Technology                                                                               |             arxiv              |                                                                            [Towards Comprehensive Detection of Chinese Harmful Memes](https://arxiv.org/abs/2410.02378)                                                                            |            **Chinese Harmful Memes**&**Multimodal Detection**&**Meme Toxicity**             |
| 24.10 |                                                                                     Cornell University                                                                                     |             arxiv              |                                                                     [How Toxicity Classifiers and Large Language Models Respond to Ableism](https://arxiv.org/abs/2410.03448)                                                                      |                            **Toxicity Classifiers**&**Ableism**                             |
| 24.10 |                                                                                     IBM Research, MIT                                                                                      |             arxiv              |                                                                              [Large Language Models Can Be Strong Self-Detoxifiers](https://arxiv.org/abs/2410.03818)                                                                              |                       **Toxicity Reduction**&**Self-Detoxification**                        |
| 24.10 |                                                                  University of California, Los Angeles, Amazon.com, Inc.                                                                   |      EMNLP 2024 Findings       |                                                           [Attribute Controlled Fine-tuning for Large Language Models: A Case Study on Detoxification](https://arxiv.org/abs/2410.05559)                                                           |                          **Attribute Control**&**Detoxification**                           |
| 24.11 |                                                                                         Meta FAIR                                                                                          |             arXiv              |                                                                         [On the Role of Speech Data in Reducing Toxicity Detection Bias](https://arxiv.org/abs/2411.08135)                                                                         |           **Speech Toxicity Detection**&**Bias Reduction**&**Multimodal Models**            |
| 24.11 |                                                                           University of California, Los Angeles                                                                            |             arxiv              |                                                                [Leveraging Large Language Models and Topic Modeling for Toxicity Classification](https://arxiv.org/abs/2411.17876)                                                                 |              **Toxicity Classification**&**BERTweet**&**Bias in Classifiers**               |
| 24.12 |                                                                                      Wuhan University                                                                                      |             arxiv              |                                                                       [Toxicity Detection Towards Adaptability to Changing Perturbations](https://arxiv.org/abs/2412.15267)                                                                        |           **Toxicity Detection**&**Continual Learning**&**Perturbation Patterns**           |
| 25.01 |                                                                                    Amazon Web Services                                                                                     |          NeurIPS 2024          |                                                                                [Risk-Averse Fine-tuning of Large Language Models](https://arxiv.org/abs/2501.06911)                                                                                |       **Risk-Averse Fine-tuning**&**Reinforcement Learning**&**Toxicity Mitigation**        |
| 25.01 |                                                                                  Georgia State University                                                                                  |             arxiv              |                                                        [Playing Devil's Advocate: Unmasking Toxicity and Vulnerabilities in Large Vision-Language Models](https://arxiv.org/abs/2501.09039)                                                        |           **LVLMs Vulnerabilities**&**Toxicity Analysis**&**Adversarial Prompts**           |
| 25.01 |                                                                       University of Science and Technology of China                                                                        |             arxiv              |                                                            [Polarized Patterns of Language Toxicity and Sentiment of Debunking Posts on Social Media](https://arxiv.org/abs/2501.06274)                                                            |         **Social Media Analysis**&**Language Toxicity**&**Political Polarization**          |
| 25.01 |                                                                                   Politecnico di Milano                                                                                    |             arxiv              |                                                                 [How Toxic Can You Get? Search-Based Toxicity Testing for Large Language Models](https://arxiv.org/abs/2501.01741)                                                                 |          **Automated Testing**&**Evolutionary Algorithms**&**Toxicity Detection**           |
| 25.01 |                                                                              University of California, Davis                                                                               |             arxiv              |                                         [Towards Safer Social Media Platforms: Scalable and Performant Few-Shot Harmful Content Moderation Using Large Language Models](https://arxiv.org/abs/2501.13976)                                          |                      **Social Media Moderation**&**Few-Shot Learning**                      |
| 25.02 |                                                                                   Iowa State University                                                                                    |             arxiv              |                                                       [Evaluation of Hate Speech Detection using Large Language Models and Geographical Contextualization](https://arxiv.org/abs/2502.19612)                                                       |            **Hate Speech Detection**&**Geographical Context**&**LLM Robustness**            |
| 25.03 |                                                                                   University of Warwick                                                                                    |           NAACL 2025           |                                                  [SafeSpeech: A Comprehensive and Interactive Tool for Analysing Sexist and Abusive Language in Conversations](https://arxiv.org/abs/2503.06534)                                                   |         **Toxic Speech Detection**&**Conversation Analysis**&**LLM Explainability**         |
| 25.03 |                                                 Aston University, Jamhuriya University, Somali National University, University of Djibouti                                                 |             arxiv              |                                              [Detection of Somali-written Fake News and Toxic Messages on the Social Media Using Transformer-based Language Models](https://arxiv.org/abs/2503.18117)                                              |           **Somali NLP**&**Fake News Detection**&**Toxic Content Classification**           |
| 25.03 |                                                                                     Beihang University                                                                                     |             arxiv              |                                             [Collaborative Evolution: Multi-Round Learning Between Large and Small Language Models for Emergent Fake News Detection](https://arxiv.org/abs/2503.21127)                                             |     **Fake News Detection**&**LLM-SLM Collaboration**&**Retrieval-Augmented Learning**      |
| 25.04 | The University of Queensland | arxiv | [LLM-based Semantic Augmentation for Harmful Content Detection](https://arxiv.org/abs/2504.15548) | **Semantic Augmentation**&**Harmful Content Detection**&**Social Media Classification** |
| 25.04 | Guangxi Police College | arxiv | [Few-Shot Hate Speech Detection Based on the MindSpore Framework](https://arxiv.org/abs/2504.15987) | **Few-shot Learning**&**Hate Speech Detection**&**Prompt-based Models** |
| 25.05 | University of Arkansas | arxiv | [Detecting and Mitigating Hateful Content in Multimodal Memes with Vision-Language Models](https://arxiv.org/abs/2505.00150) | **Hateful Meme Detection**&**Vision-Language Models**&**Multimodal Content Mitigation** |
| 25.05 | Microsoft | arxiv | [Towards Safer Pretraining: Analyzing and Filtering Harmful Content in Webscale datasets for Responsible LLMs](https://arxiv.org/abs/2505.02009) | **Data Filtering**&**Toxicity Detection**&**LLM Pretraining** |


## üíªPresentations & Talks


## üìñTutorials & Workshops

| Date  |          Type          |       Title        |                                          URL                                          |
|:-----:|:----------------------:|:------------------:|:-------------------------------------------------------------------------------------:|
| 22.02 | Toxicity Detection API |  Perspective API   | [link](https://www.perspectiveapi.com/)<br/>[paper](https://arxiv.org/abs/2202.11176) |
| 23.10 |       Tutorials        | Awesome-LLM-Safety |                 [link](https://github.com/ydyjya/Awesome-LLM-Safety)                  |

## üì∞News & Articles

## üßë‚Äçüè´Scholars
