# Defense

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating with the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"


## üìëPapers

| Date  |                                                                          Institute                                                                          |                          Publication                           |                                                                                  Paper                                                                                  |                                                      Keywords                                                      |
|:-----:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------:|:--------------------------------------------------------------:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:------------------------------------------------------------------------------------------------------------------:|
| 21.07 |                                                                       Google Research                                                                       |                            ACL2022                             |                                 [Deduplicating Training Data Makes Language Models Better](https://aclanthology.org/2022.acl-long.577/)                                 |                              **Privacy Protected**&**Deduplication**&**Memorization**                              |
| 23.05 |                                                                  UC Davis, USC, UW-Madison                                                                  |                           NAACL2024                            |                                   [From Shortcuts to Triggers: Backdoor Defense with Denoised PoE](https://arxiv.org/abs/2305.14910)                                    |                      **Backdoor Attacks**&**Defense Methods**&**Natural Language Processing**                      |
| 23.08 |                                                                  Georgia Tech, Intel Labs                                                                   |                             arxiv                              |                               [LLM Self Defense: By Self Examination LLMs Know They Are Being Tricked](https://arxiv.org/abs/2308.07308)                                |                       **Adversarial Attacks**&**Self Defense**&**Harmful Content Detection**                       |
| 23.08 |                                                                   University of Michigan                                                                    |                             arxiv                              |                                         [DETECTING LANGUAGE MODEL ATTACKS WITH PERPLEXITY](https://arxiv.org/abs/2308.14132v3)                                          |                            **Adversarial Suffixes**&**Perplexity**&**Attack Detection**                            |
| 23.09 |                                                                   University of Maryland                                                                    |                             arxiv                              |                                         [Certifying LLM Safety against Adversarial Prompting](https://arxiv.org/abs/2309.02705)                                         |                                     **Safety Filter**&**Adversarial Prompts**                                      |
| 23.09 |                                                                   University of Maryland                                                                    |                             arxiv                              |                              [BASELINE DEFENSES FOR ADVERSARIAL ATTACKS AGAINST ALIGNED LANGUAGE MODELS](https://arxiv.org/abs/2309.00614)                              |                          **Perplexity**&**Input Preprocessing**&**Adversarial Training**                           |
| 23.09 |                                                              The Pennsylvania State University                                                              |                             arxiv                              |                                [DEFENDING AGAINST ALIGNMENT-BREAKING ATTACKS VIA ROBUSTLY ALIGNED LLM](https://arxiv.org/abs/2309.14348)                                |                  **Alignment-Breaking Attacks**&**Adversarial Prompts**&**Jailbreaking Prompts**                   |
| 23.10 |                                                                 University of Pennsylvania                                                                  |                             arxiv                              |                               [SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks](https://arxiv.org/abs/2310.03684)                               |                               **Jailbreak**&**Adversarial Attack**&**Perturbation**                                |
| 23.10 |                                                                  Michigan State University                                                                  |                             arXiv                              |                                [Jailbreaker in Jail: Moving Target Defense for Large Language Models](https://arxiv.org/abs/2310.02417)                                 |                   **Dialogue System**&**Trustworthy Machine Learning**&**Moving Target Defense**                   |
| 23.10 |                                                                      Peking University                                                                      |                             arxiv                              |                         [Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations](https://arxiv.org/abs/2310.06387)                         |                   **In-Context Learning**&**Adversarial Attacks**&**In-Context Demonstrations**                    |
| 23.10 |                                                        The Chinese University of Hong Kong&Microsoft                                                        |                           NAACL2024                            |                                           [SELF-GUARD: Empower the LLM to Safeguard Itself](https://arxiv.org/abs/2310.15851)                                           |                       **Large Language Models**&**Jailbreak Attacks**&**Safety Mechanisms**                        |
| 23.11 |                                                               University of California Irvine                                                               |                             arxiv                              |                            [Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield](https://arxiv.org/abs/2311.00172)                            |                                **Adversarial Prompt Shield**&**Safety Classifier**                                 |
| 23.11 |                                                              Child Health Evaluative Sciences                                                               |                             arxiv                              |                                [Pyclipse, a library for deidentification of free-text clinical notes](https://arxiv.org/abs/2311.02748)                                 |                                    **Clinical Text Data**&**Deidentification**                                     |
| 23.11 |                                                                     Tsinghua University                                                                     |                             arxiv                              |                      [Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization](https://arxiv.org/abs/2311.09096)                       |                            **Jailbreaking Attacks**&**Goal Prioritization**&**Safety**                             |
| 23.11 |                   University of Southern California, Harvard University, University of California Davis, University of Wisconsin-Madison                    |                             arxiv                              |                   [Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations](https://arxiv.org/abs/2311.09763)                   |                      **Backdoor Attacks**&**Defensive Demonstrations**&**Test-Time Defense**                       |
| 23.11 |                                                             University of Maryland College Park                                                             |                             arxiv                              |                  [Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information](https://arxiv.org/abs/2311.11509)                   |                 **Adversarial Prompt Detection**&**Perplexity Measures**&**Token-level Analysis**                  |
| 23.12 |                                                  Rensselaer Polytechnic Institute, Northeastern University                                                  |                             arxiv                              |                            [Combating Adversarial Attacks through a Conscience-Based Alignment Framework](https://arxiv.org/abs/2312.00029)                             |                         **Adversarial Attacks**&**Conscience-Based Alignment**&**Safety**                          |
| 23.12 |                                                     Azure Research, Microsoft Security Response Center                                                      |                             arXiv                              |                                  [Maatphor: Automated Variant Analysis for Prompt Injection Attacks](https://arxiv.org/abs/2312.11513)                                  |                            **Prompt Injection Attacks**&**Automated Variant Analysis**                             |
| 23.12 |                         University of Massachusetts Amherst, Columbia University, Google, Stanford University, New York University                          |                             arxiv                              |                                  [Learning and Forgetting Unsafe Examples in Large Language Models](https://arxiv.org/abs/2312.12736)                                   |                          **Safety Issues**&**ForgetFilter Algorithm**&**Unsafe Content**                           |
| 23.12 |                                                 UC Berkeley, King Abdulaziz City for Science and Technology                                                 |                             arXiv                              |                                     [Jatmo: Prompt Injection Defense by Task-Specific Finetuning](https://arxiv.org/abs/2312.17673)                                     |                                           Prompt Injection&LLM Security                                            |
| 24.01 |                                                                  Arizona State University                                                                   |                             arxiv                              |        [The Art of Defending: A Systematic Evaluation and Analysis of LLM Defense Strategies on Safety and Over-Defensiveness](https://arxiv.org/abs/2401.00287)        |                              **Safety**&**Over-Defensiveness**&**Defense Strategies**                              |
| 24.01 |                                                    Logistic and Supply Chain MultiTech R&D Centre (LSCM)                                                    |                             arxiv                              |                 [Detection and Defense Against Prominent Attacks on Preconditioned LLM-Integrated Virtual Assistants](https://arxiv.org/abs/2401.00994)                 |                                       **Preconditioning**&**Cyber Security**                                       |
| 24.01 |            The Hong Kong University of Science and Technology, University of Illinois at Urbana-Champaign, The Hong Kong Polytechnic University             |                             arxiv                              |                                 [MLLM-Protector: Ensuring MLLM‚Äôs Safety without Hurting Performance](https://arxiv.org/abs/2401.02906)                                  |                   **Multimodal Large Language Models (MLLMs)**&**Safety**&**Malicious Attacks**                    |
| 24.01 |                                                                 Carnegie Mellon University                                                                  |                             arxiv                              |                                           [TOFU: A Task of Fictitious Unlearning for LLMs](https://arxiv.org/abs/2401.06121)                                            |                                **Data Privacy**&**Ethical Concerns**&**Unlearning**                                |
| 24.01 |                                                         Wuhan University, The University of Sydney                                                          |                             arxiv                              |                         [Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender](https://arxiv.org/abs/2401.06561)                          |                              **Intention Analysis**&**Jailbreak Defense**&**Safety**                               |
| 24.01 |                                                            The Hong Kong Polytechnic University                                                             |                             arxiv                              |                [Signed-Prompt: A New Approach to Prevent Prompt Injection Attacks Against LLM-Integrated Applications](https://arxiv.org/abs/2401.07612)                |                                    **AI Security**&**Prompt Injection Attacks**                                    |
| 24.01 |                                              University of Illinois at Urbana-Champaign, University of Chicago                                              |                             arxiv                              |                     [Robust Prompt Optimization for Defending Large Language Models Against Jailbreaking Attacks](https://arxiv.org/abs/2401.17263)                     |                          **AI Alignment**&**Jailbreaking**&**Robust Prompt Optimization**                          |
| 24.02 |                                                                  Arizona State University                                                                   |                             arxiv                              |                             [Adversarial Text Purification: A Large Language Model Approach for Defense](https://arxiv.org/abs/2402.06655)                              |                           **Textual Adversarial Defenses**&**Adversarial Purification**                            |
| 24.02 |                                                             Peking University, Wuhan University                                                             |                             arxiv                              |                                    [Fight Back Against Jailbreaking via Prompt Adversarial Tuning](https://arxiv.org/abs/2402.06255)                                    |                   **Jailbreaking Attacks**&**Prompt Adversarial Tuning**&**Defense Mechanisms**                    |
| 24.02 |                                     University of Washington, The Pennsylvania State University, Allen Institute for AI                                     |                             arxiv                              |                             [SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding](https://arxiv.org/abs/2402.08983)                             |                                  **Jailbreak Attacks**&**Safety-Aware Decoding**                                   |
| 24.02 |                                                         Shanghai Artificial Intelligence Laboratory                                                         |                             arxiv                              |                               [Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey](https://arxiv.org/abs/2402.09283)                               |                                **LLM Conversation Safety**&**Attacks**&**Defenses**                                |
| 24.02 |                                     University of Notre Dame, INRIA&King Abdullah University of Science and Technology                                      |                             arxiv                              |                                     [Defending Jailbreak Prompts via In-Context Adversarial Game](https://arxiv.org/abs/2402.13148)                                     |                                   **Adversarial Training**&**Jailbreak Defense**                                   |
| 24.02 |             University of New South Wales Australia, Delft University of Technology The Netherlands&Nanyang Technological University Singapore              |                             arxiv                              |                               [LLM Jailbreak Attack versus Defense Techniques - A Comprehensive Study](https://arxiv.org/abs/2402.13457)                                |                                    **Jailbreak Attacks**&**Defense Techniques**                                    |
| 24.02 |                                             The Hong Kong University of Science and Technology, Duke University                                             |                             arxiv                              |                          [GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis](https://arxiv.org/abs/2402.13494)                          |                         **Safety-Critical Gradient Analysis**&**Unsafe Prompt Detection**                          |
| 24.02 |                                                                 The University of Melbourne                                                                 |                             arxiv                              |                          [Round Trip Translation Defence against Large Language Model Jailbreaking Attacks](https://arxiv.org/abs/2402.13517)                           |                              **Social-Engineered Attacks**&**Round Trip Translation**                              |
| 24.02 |                                                              Nanyang Technological University                                                               |                             arxiv                              |                        [LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A Vision Paper](https://arxiv.org/abs/2402.15727)                        |                                     **Jailbreaking Attacks**&**Self-Defense**                                      |
| 24.02 |                                                                       Ajou University                                                                       |                             arxiv                              |                      [Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement](https://arxiv.org/abs/2402.15180)                      |                                     **Jailbreak Attacks**&**Self-Refinement**                                      |
| 24.02 |                                                                            UCLA                                                                             |                             arxiv                              |                                   [Defending LLMs against Jailbreaking Attacks via Backtranslation](https://arxiv.org/abs/2402.16459)                                   |                                    **Backtranslation**&**Jailbreaking Attacks**                                    |
| 24.02 |                                                           University of California Santa Barbara                                                            |                             arxiv                              |                          [Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing](https://arxiv.org/abs/2402.16192)                           |                                    **Semantic Smoothing**&**Jailbreak Attacks**                                    |
| 24.02 |                                                               University of Wisconsin-Madison                                                               |                             arxiv                              |                              [Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment](https://arxiv.org/abs/2402.14968)                               |                         **Fine-tuning Attack**&**Backdoor Alignment**&**Safety Examples**                          |
| 24.02 |                                                                    University of Exeter                                                                     |                             arxiv                              |                           [Is the System Message Really Important to Jailbreaks in Large Language Models?](https://arxiv.org/abs/2402.14857)                            |                                         **Jailbreak**&**System Messages**                                          |
| 24.03 |                                                      The Chinese University of Hong Kong, IBM Research                                                      |                             arxiv                              |              [Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes](https://arxiv.org/abs/2403.00867)               |                              **Jailbreak Attacks**&**Refusal Loss**&**Gradient Cuff**                              |
| 24.03 |                           Oregon State University, Pennsylvania State University, CISPA Helmholtz Center for Information Security                           |                             arxiv                              |                                   [AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks](https://arxiv.org/abs/2403.04783)                                    |                                    **Defense Mechanisms**&**Jailbreak Attacks**                                    |
| 24.03 |                  Peking University, University of Wisconsin‚ÄìMadison, International Digital Economy Academy, University of California Davis                  |                             arxiv                              |         [AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting](https://arxiv.org/abs/2403.09513)          |                          **Multimodal Large Language Models Safety**&**Defense Strategy**                          |
| 24.03 |         Southern University of Science and Technology, Hong Kong University of Science and Technology, Huawei Noah‚Äôs Ark Lab, Peng Cheng Laboratory         |                             arxiv                              |                         [Eyes Closed Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation](https://arxiv.org/abs/2403.09572)                          |                                           **Multimodal LLMs**&**Safety**                                           |
| 24.03 |                                    UIUC, Virginia Tech, Salesforce Research, University of California Berkeley, UChicago                                    |                             arxiv                              |                         [RIGORLLM: RESILIENT GUARDRAILS FOR LARGE LANGUAGE MODELS AGAINST UNDESIRED CONTENT](https://arxiv.org/abs/2403.13031)                          |                              **Biases**&**Harmful Content**&**Resilient Guardrails**                               |
| 24.03 |                                                                          Microsoft                                                                          |                             arxiv                              |                                [Defending Against Indirect Prompt Injection Attacks With Spotlighting](https://arxiv.org/abs/2403.14720)                                |                                   **Indirect Prompt Injection**&**Spotlighting**                                   |
| 24.03 |                     XiaMen University, Yanshan University, IDEA Research, Inner Mongolia University, Microsoft, Microsoft Research Asia                     |                           NAACL2024                            |                      [Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models](https://arxiv.org/abs/2403.11838)                       |                           **Language Models**&**Safety Guidelines**&**Model Alignment**                            |
| 24.03 |                                                                          IIT Delhi                                                                          |                           NAACL2024                            |              [Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task Instruction Tuning with RLAIF](https://arxiv.org/abs/2403.10088)               | **Counterspeech Generation**&**Multi-Task Instruction Tuning**&**Reinforcement Learning from AI Feedback (RLAIF)** |
| 24.03 |                                                                   Stony Brook University                                                                    |                      NAACL2024(findings)                       |                                     [Task-Agnostic Detector for Insertion-Based Backdoor Attacks](https://arxiv.org/abs/2403.17155)                                     |                             **Backdoor Detection**&**Logit Features**&**NLP Security**                             |
| 24.03 |                                                                    Chung-Ang University                                                                     |                      NAACL2024(findings)                       |             [Don‚Äôt be a Fool: Pooling Strategies in Offensive Language Detection from User-Intended Adversarial Attacks](https://arxiv.org/abs/2403.15467)              |                  **Offensive Language Detection**&**Adversarial Attacks**&**Pooling Strategies**                   |
| 24.04 |                                                   South China University of Technology&Pazhou Laboratory                                                    |                             arxiv                              |                       [Eraser: Jailbreaking Defense in Large Language Models via Unlearning Harmful Knowledge](https://arxiv.org/abs/2404.05880)                        |                                          **Jailbreaking**&**Unlearning**                                           |
| 24.04 |                                                        Zhejiang University, Johns Hopkins University                                                        |                             arxiv                              |                                [SAFEGEN: Mitigating Unsafe Content Generation in Text-to-Image Models](https://arxiv.org/abs/2404.06666)                                |                         **Text-to-Image Models**&**Unsafe Content**&**Content Mitigation**                         |
| 24.04 |                                            Hong Kong University of Science and Technology, University of Oxford                                             |                             arxiv                              |                                    [Latent Guard: A Safety Framework for Text-to-Image Generation](https://arxiv.org/abs/2404.08031)                                    |                           **Text-to-Image Models**&**Safety Framework**&**Latent Guard**                           |
| 24.04 | Nanjing University, Microsoft Research Asia, Tsinghua University, Queen Mary University of London, Pennsylvania State University,  NEC Laboratories America |                             arxiv                              |                                          [Protecting Your LLMs with Information Bottleneck](https://arxiv.org/abs/2404.13968)                                           |                                 **Information Bottleneck**&**Adversarial Defense**                                 |
| 24.04 |                                      Centre for Software Excellence Huawei, University of Manitoba, Queen‚Äôs University                                      |                             arxiv                              |                         [A Framework for Real-time Safeguarding the Text Generation of Large Language Models](https://arxiv.org/abs/2404.19048)                         |                               **Text Generation Safety**&**Real-time Safeguarding**                                |
| 24.04 |                                                              Princeton University&UC Davis&USC                                                              |                           NAACL2024                            |                        [Two Heads are Better than One: Nested PoE for Robust Defense Against Multi-Backdoors](https://arxiv.org/abs/2404.02356)                         |                       **Nested Product of Experts**&**Data Poisoning**&**Backdoor Defense**                        |
| 24.05 |                                                                     Tsinghua University                                                                     |                             arxiv                              |                    [Learnable Linguistic Watermarks for Tracing Model Extraction Attacks on Large Language Models](https://arxiv.org/abs/2405.01509)                    |                                   **Model Extraction Attacks**&**Watermarking**                                    |
| 24.05 |                                                                     Tsinghua University                                                                     |                             arxiv                              |                                    [Adaptive and Robust Watermark against Model Extraction Attack](https://arxiv.org/abs/2405.02365)                                    |                                   **Model Extraction Attacks**&**Watermarking**                                    |
| 24.05 |                                                                  Johns Hopkins University                                                                   |                           ICML 2024                            |                              [PARDEN: Can You Repeat That? Defending against Jailbreaks via Repetition](https://arxiv.org/abs/2405.07932)                               |                                             **Jailbreaks**&**Defense**                                             |
| 24.05 |                                                                   University of Edinburgh                                                                   |                             arxiv                              |                                 [Spectral Editing of Activations for Large Language Model Alignment](https://arxiv.org/abs/2405.09719)                                  |                       **Bias Mitigation**&**Truthfulness Enhancement**&**Spectral Editing**                        |
| 24.05 |                                                                East China Normal University                                                                 |                             arxiv                              |                     [A Safety Realignment Framework via Subspace-Oriented Model Fusion for Large Language Models](https://arxiv.org/abs/2405.09055)                     |                                      **Model Fusion**&**Safeguard Strategy**                                       |
| 24.05 |                                                     The Hong Kong University of Science and Technology                                                      |                             arxiv                              |                                        [Backdoor Removal for Generative Large Language Models](https://arxiv.org/abs/2405.07667)                                        |                           **Backdoor Attacks**&**Generative Models**&**Safety Training**                           |
| 24.05 |                                                                   Stony Brook University                                                                    |                             arxiv                              |                            [Robustifying Safety-Aligned Large Language Models through Clean Data Curation](https://arxiv.org/abs/2405.19358)                            |                     **Jailbreaking Attacks**&**Clean Data Curation**&**LLM Safety Alignment**                      |
| 24.05 |                                                                 University of Pennsylvania                                                                  |                             arxiv                              |                             [One-Shot Safety Alignment for Large Language Models via Optimal Dualization](https://arxiv.org/abs/2405.19544)                             |                         **Safety Alignment**&**Constrained RLHF**&**Optimal Dualization**                          |
| 24.05 |                                                                            Naver                                                                            |                             arxiv                              |                                  [SLM as Guardian: Pioneering AI Safety with Small Language Models](https://arxiv.org/abs/2405.19795)                                   |                        **AI Safety**&**Small Language Models**&**Harmful Query Detection**                         |
| 24.05 |                                                             The Chinese University of Hong Kong                                                             |                             arxiv                              |                    [Defensive Prompt Patch: A Robust and Interpretable Defense of LLMs against Jailbreak Attacks](https://arxiv.org/abs/2405.20099)                     |                         **Jailbreak Attacks**&**Defensive Prompt Patch**&**LLM Security**                          |
| 24.05 |                                                       Tokyo University of Agriculture and Technology                                                        |                             arxiv                              |                            [Large Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent](https://arxiv.org/abs/2405.20770)                             |                            **Adversarial Robustness**&**LLM Agent**&**Textual Attacks**                            |
| 24.06 |                                                             University of California, Riverside                                                             |                             arxiv                              |                                  [Cross-Modal Safety Alignment: Is Textual Unlearning All You Need?](https://arxiv.org/abs/2406.02575)                                  |               **Cross-Modality Safety Alignment**&**Textual Unlearning**&**Vision-Language Models**                |
| 24.06 |                                                                   University of Liverpool                                                                   | IEEE Transactions on Pattern Analysis and Machine Intelligence |                                            [Safeguarding Large Language Models: A Survey](https://arxiv.org/abs/2406.02622)                                             |                                    **Survey**&**Safeguards**&**Trustworthy AI**                                    |
| 24.06 |                                                                   Oregon State University                                                                   |                             arxiv                              |                      [Defending Large Language Models Against Attacks With Residual Stream Activation Analysis](https://arxiv.org/abs/2406.03230)                       |                               **Adversarial Machine Learning**&**Machine Learning**                                |
| 24.06 |                                                        Huazhong University of Science and Technology                                                        |                             arxiv                              |                          [AutoJailbreak: Exploring Jailbreak Attacks and Defenses through a Dependency Lens](https://arxiv.org/abs/2406.03805)                          |                                   **Jailbreak Attacks**&**Dependency Analysis**                                    |
| 24.06 |                                                     The Hong Kong University of Science and Technology                                                      |                             arxiv                              |                          [SELFDEFEND: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner](https://arxiv.org/abs/2406.05498)                          |                                      **Jailbreaking Defense**&**SELFDEFEND**                                       |
| 24.06 |                                                                    Princeton University                                                                     |                             arxiv                              |                                  [Safety Alignment Should Be Made More Than Just a Few Tokens Deep](https://arxiv.org/abs/2406.05946)                                   |                                    **Safety Alignment**&**Adversarial Attacks**                                    |
| 24.06 |                                                           The University of Alabama at Birmingham                                                           |                             arxiv                              |                               [Chain-of-Scrutiny: Detecting Backdoor Attacks for Large Language Models](https://arxiv.org/abs/2406.05948)                               |                                     **Backdoor Attacks**&**Chain-of-Scrutiny**                                     |
| 24.06 |                                               The Hong Kong University of Science and Technology (Guangzhou)                                                |                             arxiv                              |                                  [Adversarial Tuning: Defending Against Jailbreak Attacks for LLMs](https://arxiv.org/abs/2406.06622)                                   |                                    **Jailbreak Attacks**&**Adversarial Tuning**                                    |
| 24.06 |                                                                         Komorebi AI                                                                         |                             arxiv                              |                                      [Merging Improves Self-Critique Against Jailbreak Attacks](https://arxiv.org/abs/2406.07188)                                       |                                      **Jailbreak Attacks**&**Self-Critique**                                       |
| 24.06 |                                                       Tsinghua Shenzhen International Graduate School                                                       |                             arxiv                              |                     [MLLMGUARD: A Multi-dimensional Safety Evaluation Suite for Multimodal Large Language Models](https://arxiv.org/abs/2406.07594)                     |                               **Safety Evaluation**&**MLLMs**&**Multi-dimensional**                                |
| 24.06 |                                                                      Purdue University                                                                      |                             arxiv                              |                         [RL-JACK: Reinforcement Learning-powered Black-box Jailbreaking Attack against LLMs](https://arxiv.org/abs/2406.08725)                          |                                **Jailbreaking Attacks**&**Reinforcement Learning**                                 |
| 24.06 |                                                   Mohamed bin Zayed University of Artificial Intelligence                                                   |                             arxiv                              |                                [MirrorCheck: Efficient Adversarial Defense for Vision-Language Models](https://arxiv.org/abs/2406.09250)                                |                         **Adversarial Defense**&**Vision-Language Models**&**MirrorCheck**                         |
| 24.06 |                                                                     Teesside University                                                                     |                             arxiv                              |                       [Threat Modelling and Risk Analysis for Large Language Model (LLM)-Powered Applications](https://arxiv.org/abs/2406.11007)                        |                        **Threat Modelling**&**Risk Analysis**&**LLM-Powered Applications**                         |
| 24.06 |                                                                     NVIDIA Corporation                                                                      |                             arxiv                              |                                    [garak: A Framework for Security Probing Large Language Models](https://arxiv.org/abs/2406.11036)                                    |                                           **garak**&**Security Probing**                                           |
| 24.06 |                                                        University of Science and Technology of China                                                        |                             arxiv                              |                     [Self and Cross-Model Distillation for LLMs: Effective Methods for Refusal Pattern Alignment](https://arxiv.org/abs/2406.11285)                     |                  **Self Distillation**&**Cross-Model Distillation**&**Refusal Pattern Alignment**                  |
| 24.06 |                                                                  University of Washington                                                                   |                             arxiv                              |                         [CLEANGEN: Mitigating Backdoor Attacks for Generation Tasks in Large Language Models](https://arxiv.org/abs/2406.12257)                         |                               **CLEANGEN**&**Backdoor Attacks**&**Generation Tasks**                               |
| 24.06 |                                                                     Columbia University                                                                     |                             arxiv                              |                                   [Defending Against Social Engineering Attacks in the Age of LLMs](https://arxiv.org/abs/2406.12263)                                   |                                      **Social Engineering**&**CSE Detection**                                      |
| 24.06 |                                                          Indian Institute of Technology Kharagpur                                                           |                             arxiv                              |                        [SafeInfer: Context Adaptive Decoding Time Safety Alignment for Large Language Models](https://arxiv.org/abs/2406.12274)                         |                          **SafeInfer**&**Context Adaptive Decoding**&**Safety Alignment**                          |
| 24.06 |                                                                 Chinese Academy of Sciences                                                                 |                             arxiv                              |        [Adversarial Contrastive Decoding: Boosting Safety Alignment of Large Language Models via Opposite Prompt Optimization](https://arxiv.org/abs/2406.16743)        |                                   **Safety Alignment**&**Contrastive Decoding**                                    |
| 24.07 |                                                                    University of Toronto                                                                    |                             arxiv                              |                             [A False Sense of Safety: Unsafe Information Leakage in ‚ÄòSafe‚Äô AI Responses](https://arxiv.org/abs/2407.02551)                              |                                   **Jailbreak Attacks**&**Information Leakage**                                    |
| 24.07 |                                                                     Tsinghua University                                                                     |                             arxiv                              |              [Safe Unlearning: A Surprisingly Effective and Generalizable Solution to Defend Against Jailbreak Attacks](https://arxiv.org/abs/2407.02855)               |                                        **Jailbreak Attacks**&**Unlearning**                                        |
| 24.07 |                                                              National University of Singapore                                                               |                             arxiv                              |                                  [Self-Evaluation as a Defense Against Adversarial Attacks on LLMs](https://arxiv.org/abs/2407.03234)                                   |                                    **Adversarial Attacks**&**Self-Evaluation**                                     |
| 24.07 |                                                                     Tianjin University                                                                      |                             arxiv                              |                                     [DART: Deep Adversarial Automated Red Teaming for LLM Safety](https://arxiv.org/abs/2407.03876)                                     |                         **Automated Red Teaming**&**Adversarial Training**&**LLM Safety**                          |
| 24.07 |                                                                  Seoul National University                                                                  |                       ACL 2024 Workshop                        |                              [Safe-Embed: Unveiling the Safety-Critical Knowledge of Sentence Encoders](https://arxiv.org/abs/2407.06851)                               |                       **Sentence Encoders**&**Safety-Critical Knowledge**&**Unsafe Prompts**                       |
| 24.07 |                                                                       Duke University                                                                       |                             arxiv                              |                                     [Refusing Safe Prompts for Multi-modal Large Language Models](https://arxiv.org/abs/2407.09050)                                     |                        **Multimodal LLMs**&**Safe Prompt Refusal**&**Adversarial Attacks**                         |
| 24.07 |                                                             The Chinese University of Hong Kong                                                             |                             arxiv                              |                      [Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training](https://arxiv.org/abs/2407.09121)                       |                    **Safety Training**&**Refusal Position Bias**&**Decoupled Refusal Training**                    |
| 24.07 |                                                                      GovTech Singapore                                                                      |                             arxiv                              |                    [LionGuard: Building a Contextualized Moderation Classifier to Tackle Localized Unsafe Content](https://arxiv.org/abs/2407.10995)                    |                             **Moderation Classifier**&**Localized Content**&**Safety**                             |
| 24.07 |                                                               Georgia Institute of Technology                                                               |                             arxiv                              |                  [Targeted Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs](https://arxiv.org/abs/2407.15549)                   |                                   **Latent Adversarial Training**&**Robustness**                                   |
| 24.07 |                                                               Georgia Institute of Technology                                                               |                             arxiv                              |                 [A Voter-Based Stochastic Rejection-Method Framework for Asymptotically Safe Language Model Outputs](https://arxiv.org/abs/2407.16994)                  |                                     **Stochastic Rejection-Method**&**Safety**                                     |
| 24.07 |                                                                Shanghai Jiao Tong University                                                                |                             arxiv                              |                                              [SAFETY-J: Evaluating Safety with Critique](https://arxiv.org/abs/2407.17075)                                              |                     **Safety Evaluation**&**Critique-based Judgment**&**Bilingual Evaluator**                      |
| 24.07 |                                                                          Dynamo AI                                                                          |                       ICML 2024 Workshop                       |                                    [PrimeGuard: Safe and Helpful LLMs through Tuning-Free Routing](https://arxiv.org/abs/2407.16318)                                    |                              **Inference-Time Guardrails**&**Safety**&**Helpfulness**                              |
| 24.07 |                                                                   ShanghaiTech University                                                                   |                             arxiv                              |                             [Defending Jailbreak Attack in VLMs via Cross-modality Information Detector](https://arxiv.org/abs/2407.21659)                              |               **Jailbreak Attacks**&**Vision Language Models (VLMs)**&**Cross-modality Information**               |
| 24.08 |                                                           University of Illinois Urbana-Champaign                                                           |                             arxiv                              |                                          [Tamper-Resistant Safeguards for Open-Weight LLMs](https://arxiv.org/abs/2408.00761)                                           |                             **Tamper Resistance**&**Security**&**Adversarial Attacks**                             |
| 24.08 |                                                     Beijing University of Posts and Telecommunications                                                      |                             arxiv                              |                            [SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models](https://arxiv.org/abs/2408.02632)                            |                                 **Self-Evolving Framework**&**Adversarial Safety**                                 |
| 24.08 |                                                             University of Texas at San Antonio                                                              |                       KDD 2024 AI4Cyber                        |                       [Can Reinforcement Learning Unlock the Hidden Dangers in Aligned Large Language Models?](https://arxiv.org/abs/2408.02651)                        |                                       &**Adversarial Attacks**&**Alignment**                                       |
| 24.08 |                                                              University of Texas at Arlington                                                               |                             arxiv                              |                  [Utilizing Large Language Models to Optimize the Detection and Explainability of Phishing Websites](https://arxiv.org/abs/2408.05667)                  |                                     **Phishing Detection**&**Explainability**                                      |
| 24.08 |                                                                     New York University                                                                     |                             arxiv                              |                          [Towards Robust and Cost-Efficient Knowledge Unlearning for Large Language Models](https://arxiv.org/abs/2408.06621)                           |                                    **Knowledge Unlearning**&**Cost-Efficiency**                                    |
| 24.08 |                                                     Beijing University of Posts and Telecommunications                                                      |                             arxiv                              |                [Alignment-Enhanced Decoding: Defending via Token-Level Adaptive Refining of Probability Distributions](https://arxiv.org/abs/2408.07663)                |                               **Alignment-Enhanced Decoding**&**Jailbreak Defense**                                |
| 24.08 |                                                        University of Science and Technology of China                                                        |                             arxiv                              |                   [Prefix Guidance: A Steering Wheel for Large Language Models to Defend Against Jailbreak Attacks](https://arxiv.org/abs/2408.08924)                   |                                     **Jailbreak Defense**&**Prefix Guidance**                                      |
| 24.08 |                                                                   University of Liverpool                                                                   |                             arxiv                              |                      [Adaptive Guardrails for Large Language Models via Trust Modeling and In-Context Learning](https://arxiv.org/abs/2408.08959)                       |                         **Adaptive Guardrails**&**Trust Modeling**&**In-Context Learning**                         |
| 24.08 |                                                              National University of Singapore                                                               |                             arxiv                              | [BaThe: Defense against the Jailbreak Attack in Multimodal Large Language Models by Treating Harmful Instruction as Backdoor Trigger](https://arxiv.org/abs/2408.09093) |                  **Jailbreak Defense**&**Multimodal Large Language Models**&**Backdoor Trigger**                   |
| 24.08 |                                                               Georgia Institute of Technology                                                               |                             arxiv                              |                  [Antidote: Post-fine-tuning Safety Alignment for Large Language Models against Harmful Fine-tuning](https://arxiv.org/abs/2408.09600)                  |                     **Safety Alignment**&**Harmful Fine-tuning**&**Post-fine-tuning Defense**                      |
| 24.08 |                                                                       LG Electronics                                                                        |                             arxiv                              |                                   [ATHENA: Safe Autonomous Agents with Verbal Contrastive Learning](https://arxiv.org/abs/2408.11021)                                   |                    **Autonomous Agents**&**Verbal Contrastive Learning**&**Safety Evaluation**                     |
| 24.08 |                                                                   Duke Kunshan University                                                                   |                             arxiv                              |                  [EEG-Defender: Defending against Jailbreak through Early Exit Generation of Large Language Models](https://arxiv.org/abs/2408.11308)                   |                                  **Jailbreak Defense**&**Early Exit Generation**                                   |
| 24.08 |                                                       Hong Kong University of Science and Technology                                                        |                             arxiv                              |                        [AgentMonitor: A Plug-and-Play Framework for Predictive and Secure Multi-Agent Systems](https://arxiv.org/abs/2408.14972)                        |                          **Multi-Agent Systems**&**Predictive Modeling**&**AgentMonitor**                          |
| 24.09 |                                                            University of Maryland, College Park                                                             |                             CoLM24                             |                  [Automatic Pseudo-Harmful Prompt Generation for Evaluating False Refusals in Large Language Models](https://arxiv.org/abs/2409.00598)                  |                         **False Refusals**&**Pseudo-Harmful Prompts**&**Safety Alignment**                         |
| 24.09 |                                                               Georgia Institute of Technology                                                               |                             arxiv                              |                [BOOSTER: Tackling Harmful Fine-Tuning for Large Language Models via Attenuating Harmful Perturbation](https://arxiv.org/abs/2409.01586)                 |                       **Harmful Fine-Tuning**&**LLM Alignment**&**Perturbation Attenuation**                       |
| 24.09 |                                                          University of Chinese Academy of Sciences                                                          |                             arxiv                              |                              [Recent Advances in Attack and Defense Approaches of Large Language Models](https://arxiv.org/abs/2409.03274)                              |                                    **Attack Approaches**&**Defense Mechanisms**                                    |
| 24.09 |                    Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing, Institute of Artificial Intelligence                     |                             arxiv                              |                                [HSF: Defending against Jailbreak Attacks with Hidden State Filtering](https://arxiv.org/abs/2409.03788)                                 |                       **Hidden State Filtering**&**Jailbreak Attacks**&**Defense Mechanism**                       |
| 24.09 |                                                           Mahindra International School, SuperAGI                                                           |                             arxiv                              |                                [Safeguarding AI Agents: Developing and Analyzing Safety Architectures](https://arxiv.org/abs/2409.03793)                                |                             **AI Agents**&**Safety Architectures**&**Risk Mitigation**                             |
| 24.09 |                                                                Southern Illinois University                                                                 |                             arxiv                              |                   [Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks](https://arxiv.org/abs/2409.07353)                   |                    **Vision-Language Models**&**Jailbreak Attacks**&**Adversarial Fine-Tuning**                    |
| 24.09 |                                                                       Noah's Ark Lab                                                                        |                           COLM 2024                            |                [CoCA: Regaining Safety-awareness of Multimodal Large Language Models with Constitutional Calibration](https://arxiv.org/abs/2409.11365)                 |          **Multimodal Large Language Models (MLLMs)**&**Safety Awareness**&**Constitutional Calibration**          |
| 24.09 |                                                                     Rutgers University                                                                      |                             arxiv                              |                                   [Data-centric NLP Backdoor Defense from the Lens of Memorization](https://arxiv.org/abs/2409.14200)                                   |                         **NLP Backdoor Defense**&**Memorization**&**Data-centric Defense**                         |
| 24.09 |                                                                   Northwestern University                                                                   |                             arxiv                              |                      [PROMPTFUZZ: Harnessing Fuzzing Techniques for Robust Testing of Prompt Injection in LLMs](https://arxiv.org/abs/2409.14729)                       |                                 **Prompt Injection**&**Fuzzing**&**LLM Security**                                  |
| 24.09 |                                                                   University of Minnesota                                                                   |                             arxiv                              |           [On the Vulnerability of Applying Retrieval-Augmented Generation within Knowledge-Intensive Application Domains](https://arxiv.org/abs/2409.17275)            |           **Retrieval-Augmented Generation**&**Poisoning Attacks**&**Knowledge-Intensive Applications**            |
| 24.09 |                                                                     IBM Research Europe                                                                     |                             arxiv                              |                      [MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard for Prompt Attacks](https://arxiv.org/abs/2409.17699)                      |                        **Jailbreak Attacks**&**Mixture of Experts**&**Tabular Classifiers**                        |
| 24.09 |                                              Institute of Information Engineering, Chinese Academy of Sciences                                              |                           EMNLP 2024                           |      [Holistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction](https://arxiv.org/abs/2409.16783)      |                        **Red Teaming**&**Multi-turn Interaction**&**Automated Red Teaming**                        |
| 24.09 |                                                                         Chegg Inc.                                                                          |                             arxiv                              |                                         [Overriding Safety Protections of Open-Source Models](https://arxiv.org/abs/2409.19476)                                         |                             **Harmfulness**&**Knowledge Drift**&**Model Uncertainty**                              |
| 24.09 |                                                                    University of Toronto                                                                    |                             arxiv                              |                                  [ROBUST LLM SAFEGUARDING VIA REFUSAL FEATURE ADVERSARIAL TRAINING](https://arxiv.org/abs/2409.20089)                                   |                         **Adversarial Training**&**LLM Safeguarding**&**Refusal Feature**                          |
| 24.10 |                                                                   University of A Coru√±a                                                                    |                             arxiv                              |                                 [Decoding Hate: Exploring Language Models' Reactions to Hate Speech](https://arxiv.org/abs/2410.00775)                                  |                                 **Hate Speech**&**LLMs**&**Mitigation Strategies**                                 |
| 24.10 |                                                                     Tel Aviv University                                                                     |                             arxiv                              |                                 [MITIGATING COPY BIAS IN IN-CONTEXT LEARNING THROUGH NEURON PRUNING](https://arxiv.org/abs/2410.01288)                                  |                              **Copy Bias**&**In-Context Learning**&**Neuron Pruning**                              |
| 24.10 |                                                                 Chinese Academy of Sciences                                                                 |                             arxiv                              |          [JAILBREAK ANTIDOTE: RUNTIME SAFETY-UTILITY BALANCE VIA SPARSE REPRESENTATION ADJUSTMENT IN LARGE LANGUAGE MODELS](https://arxiv.org/abs/2410.02298)           |                             **Jailbreaking**&**LLM Safety**&**Sparse Representation**                              |
| 24.10 |                                                               CAS Key Laboratory of AI Safety                                                               |                             arxiv                              |                          [HIDDENGUARD: FINE-GRAINED SAFE GENERATION WITH SPECIALIZED REPRESENTATION ROUTER](https://arxiv.org/abs/2410.02684)                           |                      **Safe Generation**&**Representation Router**&**Token-level Moderation**                      |
| 24.10 |                                                                          HydroX AI                                                                          |                             arxiv                              |                               [Precision Knowledge Editing: Enhancing Safety in Large Language Models](https://arxiv.org/abs/2410.03772)                                |                                          **Knowledge Editing**&**Safety**                                          |
| 24.10 |                                                                      IBM Research, MIT                                                                      |                             arxiv                              |                                        [Large Language Models Can Be Strong Self-Detoxifiers](https://arxiv.org/abs/2410.03818)                                         |                                   **Toxicity Reduction**&**Self-Detoxification**                                   |
| 24.10 |                                                                   UC Berkeley, Meta, FAIR                                                                   |                             arxiv                              |                                         [Aligning LLMs to Be Robust Against Prompt Injection](https://arxiv.org/abs/2410.05451)                                         |                                         **Prompt Injection**&**Security**                                          |
| 24.10 |                                                                      Purdue University                                                                      |                             arxiv                              |                          [ETA: Evaluating Then Aligning Safety of Vision Language Models at Inference Time](https://arxiv.org/abs/2410.06625)                           |                         **Vision Language Models**&**Safety Alignment**&**Inference Time**                         |
| 24.10 |                                            Aerospace Information Research Institute, Chinese Academy of Sciences                                            |                             arxiv                              |                                [Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level](https://arxiv.org/abs/2410.06809)                                |                                **Decoding-Level Defense**&**Jailbreak Prevention**                                 |
| 24.10 |                                                       Rensselaer Polytechnic Institute, IBM Research                                                        |                             arxiv                              |                              [SEAL: Safety-enhanced Aligned LLM Fine-tuning via Bilevel Data Selection](https://arxiv.org/abs/2410.07471)                               |                      **Safety-enhanced Aligned LLM**&**Fine-tuning**&**Bilevel Optimization**                      |
| 24.10 |                                                                  KAIST AI, Naver Cloud AI                                                                   |                             arxiv                              |                          [HOW DOES VISION-LANGUAGE ADAPTATION IMPACT THE SAFETY OF VISION LANGUAGE MODELS?](https://arxiv.org/abs/2410.07571)                           |                                **Vision-Language Adaptation**&**Safety**&**LVLMs**                                 |
| 24.10 |                                                 Johns Hopkins University, Microsoft Responsible AI Research                                                 |                             arxiv                              |                       [Controllable Safety Alignment: Inference-Time Adaptation to Diverse Safety Requirements](https://arxiv.org/abs/2410.08968)                       |                    **Safety alignment**&**Inference-time adaptation**&**LLMs controllability**                     |
| 24.10 |                                                       Princeton University, Zoom Video Communications                                                       |                             arxiv                              |                          [Instructional Segment Embedding: Improving LLM Safety with Instruction Hierarchy](https://arxiv.org/abs/2410.09102)                           |                    **Instruction hierarchy**&**LLM safety**&**Instructional segment embedding**                    |
| 24.10 |                                                Zhejiang University, Westlake University, Beihang University                                                 |                             arxiv                              |                                               [LOCKING DOWN THE FINETUNED LLMS SAFETY](https://arxiv.org/abs/2410.10343)                                                |                            **Safety alignment**&**Fine-tuned LLMs**&**Meta-SafetyLock**                            |
| 24.10 |                                                                 Carnegie Mellon University                                                                  |                             arxiv                              |                                             [Data Defenses Against Large Language Models](https://arxiv.org/abs/2410.13138)                                             |                     **Data defenses**&**Adversarial prompt injections**&**Privacy protection**                     |
| 24.10 |                                                               University of Wisconsin-Madison                                                               |            NeurIPS 2024 Safe Generative AI Workshop            |                                          [Safety-Aware Fine-Tuning of Large Language Models](https://arxiv.org/abs/2410.10014)                                          |                   **Safety-aware fine-tuning**&**Harmful data detection**&**Data contamination**                   |
| 24.10 |                                                                     Zhejiang University                                                                     |                             arxiv                              |                 [CoreGuard: Safeguarding Foundational Capabilities of LLMs Against Model Stealing in Edge Deployment](https://arxiv.org/abs/2410.13903)                 |                                       **Model Stealing**&**Edge Deployment**                                       |
| 24.10 |                                                              Nanyang Technological University                                                               |                             arxiv                              |                           [Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge Distillation](https://arxiv.org/abs/2410.14425)                           |                **Backdoor Defense**&**Knowledge Distillation**&**Parameter-Efficient Fine-Tuning**                 |
| 24.10 |                                                                   Stony Brook University                                                                    |                             arxiv                              |                         [RobustKV: Defending Large Language Models Against Jailbreak Attacks via KV Eviction](https://arxiv.org/abs/2410.19937)                         |                          **Jailbreak Attacks**&**LLM Defense**&**KV Cache Optimization**                           |
| 24.10 |                                                                         UW-Madison                                                                          |                             arxiv                              |                       [FATH: Authentication-based Test-time Defense against Indirect Prompt Injection Attacks](https://arxiv.org/abs/2410.21492)                        |                      **Prompt Injection Defense**&**LLM Security**&**Authentication System**                       |
| 24.10 |                                                            Vanderbilt University Medical Center                                                             |                             arxiv                              |                                   [Embedding-based Classifiers Can Detect Prompt Injection Attacks](https://arxiv.org/abs/2410.22284)                                   |             **Prompt Injection Detection**&**Embedding-based Classification**&**Adversarial Attacks**              |
| 24.10 |                                                               University of Wisconsin-Madison                                                               |                             arxiv                              |                      [InjecGuard: Benchmarking and Mitigating Over-defense in Prompt Injection Guardrail Models](https://arxiv.org/abs/2410.22770)                      |                    **Prompt Injection Defense**&**Over-defense Detection**&**Guardrail Models**                    |
| 24.11 |                                                                 National Taiwan University                                                                  |                             arxiv                              |                                    [Attention Tracker: Detecting Prompt Injection Attacks in LLMs](https://arxiv.org/abs/2411.00348)                                    |                                    **Prompt Injection**&**Attention Mechanism**                                    |
| 24.11 |                                                              National University of Singapore                                                               |                             arxiv                              |                               [Defense Against Prompt Injection Attack by Leveraging Attack Techniques](https://arxiv.org/abs/2411.00459)                               |                                 **Prompt Injection Defense**&**Attack Techniques**                                 |
| 24.11 |                                                               Georgia Institute of Technology                                                               |                             arxiv                              |               [UNIGUARD: Towards Universal Safety Guardrails for Jailbreak Attacks on Multimodal Large Language Models](https://arxiv.org/abs/2411.01703)               |                 **Jailbreak Attacks**&**Multimodal Safety Guardrails**&**Adversarial Robustness**                  |
| 24.11 |                                                                 Mila ‚Äì Quebec AI Institute                                                                  |                             arXiv                              |                    [Combining Domain and Alignment Vectors to Achieve Better Knowledge-Safety Trade-offs in LLMs](https://arxiv.org/abs/2411.06824)                     |                            **Domain Expertise**&**Safety Alignment**&**Model Merging**                             |
| 24.11 |                                                                          Anthropic                                                                          |                             arXiv                              |                                    [Rapid Response: Mitigating LLM Jailbreaks with a Few Examples](https://arxiv.org/abs/2411.07494)                                    |                                      **Jailbreak Defense**&**Rapid Response**                                      |
| 24.11 |                                                              National University of Singapore                                                               |                             arXiv                              |                                 [The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense](https://arxiv.org/abs/2411.08410)                                  |                        **Vision-Language Models**&**Jailbreak Defense**&**Model Alignment**                        |
| 24.11 |                                                     Beijing University of Posts and Telecommunications                                                      |                             arXiv                              |                            [Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey](https://arxiv.org/abs/2411.09259)                            |                   **Jailbreak Attacks**&**Multimodal Generative Models**&**Security Challenges**                   |
| 24.11 |                                                               Singapore Management University                                                               |                             arxiv                              |                   [CROW: Eliminating Backdoors from Large Language Models via Internal Consistency Regularization](https://arxiv.org/abs/2411.12768)                    |                                **Backdoor Defense**&**Consistency Regularization**                                 |
| 24.11 |                                                                         UC Berkeley                                                                         |                          COLING 2025                           |                                   [Lightweight Safety Guardrails Using Fine-tuned BERT Embeddings](https://arxiv.org/abs/2411.14398)                                    |                           **Safety Guardrails**&**Fine-tuned BERT**&**Prompt Filtering**                           |
| 24.11 |                                     The Hong Kong University of Science and Technology (Guangzhou), Tsinghua University                                     |                             arxiv                              |                            [PEFTGuard: Detecting Backdoor Attacks Against Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2411.17453)                            |                                      **Backdoor Detection**&**PEFT**&**LoRA**                                      |
| 24.12 |                                                                    MIT,Speechmatics,MATS                                                                    |                  NeurIPS 2024 SoLaR workshops                  |           [Jailbreak Defense in a Narrow Domain: Limitations of Existing Methods and a New Transcript-Classifier Approach](https://arxiv.org/abs/2412.02159)            |                          **Jailbreak Defense**&**LLM Security**&**Transcript Classifier**                          |
| 24.12 |                                                                Shanghai Jiao Tong University                                                                |                          COLING 2025                           |                    [Gracefully Filtering Backdoor Samples for Generative Large Language Models without Retraining](https://arxiv.org/abs/2412.02454)                    |                            **Backdoor Attacks**&**Generative LLMs**&**Frequency Space**                            |
| 24.12 |                                                                     NVIDIA Corporation                                                                      |                             arxiv                              |                             [Improved Large Language Model Jailbreak Detection via Pretrained Embeddings](https://arxiv.org/abs/2412.01547)                             |                         **Jailbreak Detection**&**Pretrained Embeddings**&**LLM Security**                         |
| 24.11 |                                                      Fudan University, Worcester Polytechnic Institute                                                      |                             arxiv                              |                      [Knowledge Database or Poison Base? Detecting RAG Poisoning Attack through LLM Activations](https://arxiv.org/abs/2411.18948)                      |                             **RAG Poisoning Attack**&**LLM Activations**&**Security**                              |
| 24.11 |                              University of Maryland-College Park, Indian Institute of Technology Bombay, Princeton University                               |                             arxiv                              |                    [Immune: Improving Safety Against Jailbreaks in Multi-modal LLMs via Inference-Time Alignment](https://arxiv.org/abs/2411.18688)                     |                       **Safety Alignment**&**Jailbreak Attack**&**Inference-Time Alignment**                       |
| 24.12 |                                                                        Not specified                                                                        |                             arxiv                              |                                       [Enhancing Adversarial Resistance in LLMs with Recursion](https://arxiv.org/abs/2412.06181)                                       |                       **Adversarial Resistance**&**LLM Security**&**Prompt Simplification**                        |
| 24.12 |                                                  University of Maryland, Capital One, New York University                                                   |                             arxiv                              |                             [Refusal Tokens: A Simple Way to Calibrate Refusals in Large Language Models](https://arxiv.org/abs/2412.06748)                             |                                      **Refusal Tokens**&**Model Calibration**                                      |
| 24.12 |                                                                    Dalhousie University                                                                     |                             arxiv                              |                                               [Classifier-free Guidance in LLMs Safety](https://arxiv.org/abs/2412.06846)                                               |                       **Unlearning**&**Reinforcement Learning**&**Classifier-free Guidance**                       |
| 24.12 |                                                                Princeton University, Google                                                                 |                             arxiv                              |                                    [Evaluating the Durability of Safeguards for Open-Weight LLMs](https://arxiv.org/abs/2412.07097)                                     |                         **LLM Safeguards**&**Open-Weight Models**&**Adversarial Defense**                          |
| 24.12 |                                                  Michigan State University, University of Hawaii at Manoa                                                   |                             arxiv                              |             [FlexLLM: Exploring LLM Customization for Moving Target Defense on Black-Box LLMs Against Jailbreak Attacks](https://arxiv.org/abs/2412.07672)              |                       **LLM Customization**&**Moving Target Defense**&**Jailbreak Attacks**                        |
| 24.12 |                                                                          Neudesic                                                                           |                             arxiv                              |                                   [Lightweight Safety Classification Using Pruned Language Models](https://arxiv.org/abs/2412.13435)                                    |                        **Model Pruning**&**Content Safety**&**Prompt Injection Detection**                         |
| 24.12 |                                                                     Asan Medical Center                                                                     |                             arxiv                              |                             [Mitigating Adversarial Attacks in LLMs through Defensive Suffix Generation](https://arxiv.org/abs/2412.13705)                              |                    **Adversarial Defense**&**Gradient-Based Optimization**&**Defensive Suffix**                    |
| 24.12 |                                                      The Chinese University of Hong Kong, IBM Research                                                      |                           AAAI 2025                            |                      [Token Highlighter: Inspecting and Mitigating Jailbreak Prompts for Large Language Models](https://arxiv.org/abs/2412.18171)                       |                         **Jailbreak Attacks**&**Affirmation Loss**&**Token-Level Defense**                         |
| 24.12 |                                       MBZUAI, Huazhong University of Science and Technology, University of Notre Dame                                       |                             arxiv                              |               [Shaping the Safety Boundaries: Understanding and Defending Against Jailbreaks in Large Language Models](https://arxiv.org/abs/2412.17034)                |                    **Jailbreak Attacks**&**Activation Boundary Defense (ABD)**&**LLM Security**                    |
| 24.12 |                                                              The Pennsylvania State University                                                              |                             arxiv                              |                 [The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt Injection in LLM Agents](https://arxiv.org/abs/2412.16682)                 |                         **Indirect Prompt Injection**&**Task Alignment**&**LLM Security**                          |
| 24.12 |                                                                           OpenAI                                                                            |                             arxiv                              |                                   [Deliberative Alignment: Reasoning Enables Safer Language Models](https://arxiv.org/abs/2412.16339)                                   |                       **Safety Alignment**&**Reasoning-Based Training**&**Chain-of-Thought**                       |
| 24.12 |                                                         MMLab, The Chinese University of Hong Kong                                                          |                             arxiv                              |                   [RapGuard: Safeguarding Multimodal Large Language Models via Rationale-aware Defensive Prompting](https://arxiv.org/abs/2412.18826)                   |                 **Multimodal Large Language Models**&**Defensive Prompting**&**Safety Mechanisms**                 |
| 24.12 |                                                                 National Taiwan University                                                                  |                             arxiv                              |                                [Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging](https://arxiv.org/abs/2412.19512)                                 |                                       **Safety Alignment**&**Model Merging**                                       |
| 24.12 |                                       MBZUAI, Huazhong University of Science and Technology, University of Notre Dame                                       |                            ACL 2025                            | [Shaping the Safety Boundaries: Understanding and Defending Against Jailbreaks in Large Language Models](https://arxiv.org/abs/2412.17034) | **Jailbreak**&**LLMs**&**Safety Boundary** |
| 25.01 |                                                                           Lakera                                                                            |                             arxiv                              |                                             [Gandalf the Red: Adaptive Security for LLMs](https://arxiv.org/abs/2501.07927)                                             |                              **Adaptive Security**&**Prompt Attacks**&**Red-Teaming**                              |
| 25.01 |                                                                       IIIT Hyderabad                                                                        |                             arxiv                              |                                     [Enhancing AI Safety Through the Fusion of Low-Rank Adapters](https://arxiv.org/abs/2501.06208)                                     |                        **Low-Rank Adapters**&**Safety Alignment**&**Jailbreak Mitigation**                         |
| 25.01 |                                                                      Xidian University                                                                      |                           AAAI 2025                            |                      [Backdoor Token Unlearning: Exposing and Defending Backdoors in Pretrained Language Models](https://arxiv.org/abs/2501.03272)                      |                                     **Backdoor Defense**&**Token Unlearning**                                      |
| 25.01 |                                                               Case Western Reserve University                                                               |                             arxiv                              |                         [Navigating the Designs of Privacy-Preserving Fine-tuning for Large Language Models](https://arxiv.org/abs/2501.04323)                          |            **Privacy-Preserving Fine-tuning**&**Data Reconstruction Attack Defense**&**GuardedTuning**             |
| 25.01 |                                                               North Carolina State University                                                               |                             arxiv                              |                   [Layer-Level Self-Exposure and Patch: Affirmative Token Mitigation for Jailbreak Attack Defense](https://arxiv.org/abs/2501.02629)                    |                      **Layer-Specific Editing**&**Machine Unlearning**&**Jailbreak Defense**                       |
| 25.01 |                                                                  Xi‚Äôan Jiaotong University                                                                  |                             arxiv                              |                    [Spot Risks Before Speaking! Unraveling Safety Attention Heads in Large Vision-Language Models](https://arxiv.org/abs/2501.02029)                    |                        **Vision-Language Models**&**Safety Mechanisms**&**Attention Heads**                        |
| 25.01 |                                                                     New York University                                                                     |                             arxiv                              |                     [Safeguarding Large Language Models in Real-time with Tunable Safety-Performance Trade-offs](https://arxiv.org/abs/2501.02018)                      |            **Large Language Model Safeguards**&**Controlled Text Generation**&**Jailbreak Mitigation**             |
| 25.01 |                                                                East China Normal University                                                                 |                             arxiv                              |     [Latent-space adversarial training with post-aware calibration for defending large language models against jailbreak attacks](https://arxiv.org/abs/2501.10639)     |                                   **Jailbreak Attacks**&**Adversarial Training**                                   |
| 25.01 |                                                                         Capital One                                                                         |                             arxiv                              |          [Refining Input Guardrails: Enhancing LLM-as-a-Judge Efficiency Through Chain-of-Thought Fine-Tuning and Alignment](https://arxiv.org/abs/2501.13080)          |                       **Input Guardrails**&**Chain-of-Thought Fine-Tuning**&**LLM Defense**                        |
| 25.01 |                                                                      Xidian University                                                                      |                             arxiv                              |                              [HumorReject: Decoupling LLM Safety from Refusal Prefix via A Little Humor](https://arxiv.org/abs/2501.13677)                              |                       **LLM Safety**&**Prefix Injection Defense**&**Humor-based Alignment**                        |
| 25.01 |                                                                      Hainan University                                                                      |                             arxiv                              |             [FDLLM: A Text Fingerprint Detection Method for LLMs in Multi-Language, Multi-Domain Black-Box Environments](https://arxiv.org/abs/2501.16029)              |                   **LLM Fingerprinting**&**Black-Box Detection**&**Multi-Language AI Security**                    |
| 25.01 |                                                               University of Wisconsin-Madison                                                               |                             arXiv                              |                                 [Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs](https://arxiv.org/abs/2501.16534)                                  |                           **LLM Alignment**&**Jailbreak Attacks**&**Safety Classifiers**                           |
| 25.01 |                                                                          AIShield                                                                           |                             arxiv                              |           [CHALLENGES IN ENSURING AI SAFETY IN DEEPSEEK-R1 MODELS: THE SHORTCOMINGS OF REINFORCEMENT LEARNING STRATEGIES](https://arxiv.org/abs/2501.17030v1)           |                        **AI Safety**&**Supervised Fine-Tuning**&**Harmlessness Reduction**                         |
| 25.01 |                                                         Mondragon University, University of Seville                                                         |                             arxiv                              |                                     [ASTRAL: Automated Safety Testing of Large Language Models](https://arxiv.org/abs/2501.17132v1)                                     |                                  **LLM Safety Testing**&**Automated Evaluation**                                   |
| 25.01 |                                                          UK AI Safety Institute, Redwood Research                                                           |                             arxiv                              |                                               [A sketch of an AI control safety case](https://arxiv.org/abs/2501.17315v1)                                               |                               **Safety Case**&**Data Exfiltration**&**LLM Security**                               |
| 25.01 |                        Tsinghua University, Shenzhen Campus of Sun Yat-sen University, Didichuxing, Nanyang Technological University                        |                             arxiv                              |                [Panacea: Mitigating Harmful Fine-tuning for Large Language Models via Post-fine-tuning Perturbation](https://arxiv.org/abs/2501.18100v1)                |                               **Harmful Fine-tuning Attack**&**Defensive Alignment**                               |
| 25.01 |                     National University of Singapore, University of Chinese Academy of Sciences, Peking University, Westlake University                     |                             arxiv                              |                                       [GuardReasoner: Towards Reasoning-based LLM Safeguards](https://arxiv.org/abs/2501.18492v1)                                       |                **LLM Safeguards**&**Reasoning-based Moderation**&**Direct Preference Optimization**                |
| 25.01 |                                                                          Anthropic                                                                          |                             arxiv                              |             [Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming](https://arxiv.org/abs/2501.18837)             |                       **Universal Jailbreaks**&**LLM Security**&**Classifier-based Defense**                       |
| 25.01 |                                                              National University of Singapore                                                               |                             arxiv                              |                             [Enhancing Model Defense Against Jailbreaks with Proactive Safety Reasoning](https://arxiv.org/abs/2501.19180)                              |                         **Jailbreak Defense**&**Safety Chain-of-Thought**&**LLM Security**                         |
| 25.02 |                                                                         Aligned AI                                                                          |                             arxiv                              |                     [Defense Against the Dark Prompts: Mitigating Best-of-N Jailbreaking with Prompt Evaluation](https://arxiv.org/abs/2502.00580)                      |                          **Jailbreaking Defense**&**Prompt Evaluation**&**LLM Security**                           |
| 25.02 |                                                              The Pennsylvania State University                                                              |                             arxiv                              |                              [Towards Robust Multimodal Large Language Models Against Jailbreak Attacks](https://arxiv.org/abs/2502.00653)                              |                         **Multimodal LLMs**&**Jailbreak Defense**&**Adversarial Training**                         |
| 25.02 |                                                                      Purdue University                                                                      |                             arxiv                              |                                      [LLM Safety Alignment is Divergence Estimation in Disguise](https://arxiv.org/abs/2502.00657)                                      |                          **LLM Alignment**&**Divergence Estimation**&**Safety Training**                           |
| 25.02 |                                                                    University of Oxford                                                                     |                             arxiv                              |                               [AgentBreeder: Mitigating the AI Safety Impact of Multi-Agent Scaffolds](https://arxiv.org/abs/2502.00757)                                |                        **Multi-Agent Systems**&**LLM Safety**&**Evolutionary Optimization**                        |
| 25.02 |                                                                    Huawei Noah‚Äôs Ark Lab                                                                    |                             arxiv                              |                               [Almost Surely Safe Alignment of Large Language Models at Inference-Time](https://arxiv.org/abs/2502.01208)                               |                   **LLM Safety Alignment**&**Inference-Time Optimization**&**Constrained MDPs**                    |
| 25.02 |                                                                     Tsinghua University                                                                     |                             arxiv                              |                                   [STAIR: Improving Safety Alignment with Introspective Reasoning](https://arxiv.org/abs/2502.02384)                                    |                  **LLM Safety Alignment**&**Introspective Reasoning**&**Monte Carlo Tree Search**                  |
| 25.02 |                                                            The Hong Kong Polytechnic University                                                             |                             arxiv                              |                   ["It Warned Me Just at the Right Moment": Exploring LLM-based Real-time Detection of Phone Scams](https://arxiv.org/abs/2502.03964)                   |                     **Phone Scam Detection**&**LLM-based Security**&**Real-time Intervention**                     |
| 25.02 |                                                                     Tsinghua University                                                                     |                             arxiv                              |                 [Leveraging Reasoning with Guidelines to Elicit and Utilize Knowledge for Enhancing Safety Alignment](https://arxiv.org/abs/2502.04040)                 |            **LLM Safety Alignment**&**Reasoning-based Training**&**Out-of-Distribution Generalization**            |
| 25.02 |                                                     King Abdullah University of Science and Technology                                                      |                             arxiv                              |      ["Short-length" Adversarial Training Helps LLMs Defend "Long-length" Jailbreak Attacks: Theoretical and Empirical Evidence](https://arxiv.org/abs/2502.04204)      |                          **Adversarial Training**&**Jailbreak Attacks**&**LLM Security**                           |
| 25.02 |                                                            University of California, Los Angeles                                                            |                             arxiv                              |                             [DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails](https://arxiv.org/abs/2502.05163)                              |                    **Multilingual LLM Safety**&**Reinforcement Learning**&**Guardrail Models**                     |
| 25.02 |                                                           University of California, Santa Barbara                                                           |                             arxiv                              |                        [MELON: Indirect Prompt Injection Defense via Masked Re-execution and Tool Comparison](https://arxiv.org/abs/2502.05174)                         |                **Indirect Prompt Injection Defense**&**Large Language Models (LLMs)**&**Security**                 |
| 25.02 |                                                              University of California, Irvine                                                               |                             arxiv                              |                            [Bridging the Safety Gap: A Guardrail Pipeline for Trustworthy LLM Inferences](https://arxiv.org/abs/2502.08142)                             |                         **LLM Safety**&**Guardrail Pipeline**&**Hallucination Detection**                          |
| 25.02 |                                                     Beijing University of Posts and Telecommunications                                                      |                             arxiv                              |             [Refining Positive and Toxic Samples for Dual Safety Self-Alignment of LLMs with Minimal Human Interventions](https://arxiv.org/abs/2502.08657)             |                        **Safety Alignment**&**Self-Supervised Learning**&**LLM Robustness**                        |
| 25.02 |                                                                 Carnegie Mellon University                                                                  |                             arxiv                              |                              [RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage](https://arxiv.org/abs/2502.08966)                               |                       **Prompt Injection**&**Privacy Leakage**&**Tool-Based Agent Systems**                        |
| 25.02 |                                                               Georgia Institute of Technology                                                               |                             arxiv                              |                      [AGENTGUARD: Repurposing Agentic Orchestrator for Safety Evaluation of Tool Orchestration](https://arxiv.org/abs/2502.09809)                       |                                **LLM Agents**&**Tool Orchestration**&**AI Safety**                                 |
| 25.02 |                                                                Shanghai Jiao Tong University                                                                |                             arxiv                              |       [X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from Multi-Turn Jailbreaks without Compromising Usability](https://arxiv.org/abs/2502.09990)       |                           **Jailbreaking Defense**&**Multi-Turn Attacks**&**LLM Safety**                           |
| 25.02 |                                                                          KuaiShou                                                                           |                             arxiv                              |                                     [MM-RLHF: The Next Step Forward in Multimodal LLM Alignment](https://arxiv.org/abs/2502.10391)                                      |                              **Multimodal RLHF**&**LLM Alignment**&**Reward Models**                               |
| 25.02 |                                                          Indian Institute of Technology Kharagpur                                                           |                             arxiv                              |                     [Soteria: Language-Specific Functional Parameter Steering for Multilingual Safety Alignment](https://arxiv.org/abs/2502.11244)                      |                             **Multilingual Safety**&**Functional Parameter Steering**                              |
| 25.02 |                                                                  The Ohio State University                                                                  |                             arxiv                              |                           [AGrail: A Lifelong Agent Guardrail with Effective and Adaptive Safety Detection](https://arxiv.org/abs/2502.11448)                           |                          **LLM Agent Safety**&**Risk Detection**&**Adaptive Guardrails**                           |
| 25.02 |                                                                        Alibaba Group                                                                        |                             arxiv                              |                      [Equilibrate RLHF: Towards Balancing Helpfulness-Safety Trade-off in Large Language Models](https://arxiv.org/abs/2502.11555)                      |                        **RLHF**&**Helpfulness-Safety Trade-off**&**Large Language Models**                         |
| 25.02 |                                                                   ShanghaiTech University                                                                   |                             arxiv                              |                        [DELMAN: Dynamic Defense Against Large Language Model Jailbreaking with Model Editing](https://arxiv.org/abs/2502.11647)                         |                              **Jailbreak Defense**&**Model Editing**&**LLM Security**                              |
| 25.02 |                                                                            KAIST                                                                            |                             arxiv                              |              [SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models](https://arxiv.org/abs/2502.12464)              |                                 **LLM Safety**&**Model Selection**&**Guardrails**                                  |
| 25.02 |                                                                      GovTech Singapore                                                                      |                             arxiv                              |        [Safe at the Margins: A General Approach to Safety Alignment in Low-Resource English Languages ‚Äì A Singlish Case Study](https://arxiv.org/abs/2502.12485)        |                                  **Safety Alignment**&**Low-Resource Languages**                                   |
| 25.02 |                                                               Beihang University, Baidu Inc.                                                                |                             arxiv                              |                   [Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking](https://arxiv.org/abs/2502.12970)                    |                        **Jailbreaking Defense**&**LLM Safety**&**Reasoning-based Security**                        |
| 25.02 |                                                              The Pennsylvania State University                                                              |                             arXiv                              |                                  [Understanding and Rectifying Safety Perception Distortion in VLMs](https://arxiv.org/abs/2502.13095)                                  |                        **Vision-Language Models**&**Safety Alignment**&**Activation Shift**                        |
| 25.02 |                                                              Rochester Institute of Technology                                                              |                             arXiv                              |  [UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor Attacks and Adversarial Attacks in Large Language Models](https://arxiv.org/abs/2502.13141)   |                         **Prompt Injection**&**Backdoor Attacks**&**Adversarial Attacks**                          |
| 25.02 |                                                    Institute of Automation, Chinese Academy of Sciences                                                     |                             arXiv                              |                                 [ShieldLearner: A New Paradigm for Jailbreak Attack Defense in LLMs](https://arxiv.org/abs/2502.13162)                                  |                                    **Jailbreak Attack**&**Adversarial Defense**                                    |
| 25.02 |                                                            The Hong Kong Polytechnic University                                                             |                             arXiv                              |   [Why Safeguarded Ships Run Aground? Aligned Large Language Models‚Äô Safety Mechanisms Tend to Be Anchored in The Template Region](https://arxiv.org/abs/2502.13946)    |                                       **Safety Alignment**&**Jailbreaking**                                        |
| 25.02 |                                                                      Fudan University                                                                       |                             arXiv                              |                                [How Jailbreak Defenses Work and Ensemble? A Mechanistic Investigation](https://arxiv.org/abs/2502.14486)                                |                  **Jailbreak Defenses**&**Large Vision-Language Models**&**Ensemble Strategies**                   |
| 25.02 |                                                             The Chinese University of Hong Kong                                                             |                             arXiv                              |             [HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States](https://arxiv.org/abs/2502.14744)             |               **Jailbreak Detection**&**Large Vision-Language Models**&**Hidden States Monitoring**                |
| 25.02 |                                                                      Wuhan University                                                                       |                      USENIX Security 2025                      |        [JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept Analysis and Manipulation](https://arxiv.org/abs/2502.07557)         |                            **Jailbreak Defense**&**LLM Security**&**Concept Analysis**                             |
| 25.02 |                                                                   North South University                                                                    |                             arxiv                              |                        [Guardians of the Agentic System: Preventing Many-Shots Jailbreak with Agentic System](https://arxiv.org/abs/2502.16750)                         |                               **AI Agents**&**Jailbreaking**&**Adversarial Attacks**                               |
| 25.02 |                                                               Warsaw University of Technology                                                               |                             arxiv                              |                        [MAYBE I SHOULD NOT ANSWER THAT, BUT... DO LLMs UNDERSTAND THE SAFETY OF THEIR INPUTS?](https://arxiv.org/abs/2502.16174)                        |                         **LLM Safety**&**Prompt Classification**&**Adversarial Detection**                         |
| 25.02 |                                                                     Sichuan University                                                                      |                             arxiv                              |                [Beyond Surface-Level Patterns: An Essence-Driven Defense Framework Against Jailbreak Attacks in LLMs](https://arxiv.org/abs/2502.19041)                 |                            **Jailbreak Attacks**&**LLM Security**&**Defense Framework**                            |
| 25.02 |                                                                     Samsung Electronics                                                                     |                             arxiv                              |                [Toward Responsible Federated Large Language Models: Leveraging a Safety Filter and Constitutional AI](https://arxiv.org/abs/2502.16691)                 |                            **Federated Learning**&**LLM Safety**&**Constitutional AI**                             |
| 25.02 |                                                                Mila, Universit√© de Montr√©al                                                                 |                             arxiv                              |                           [A Generative Approach to LLM Harmfulness Detection with Special Red Flag Tokens](https://arxiv.org/abs/2502.16366)                           |                        **Harmfulness Detection**&**Safety Fine-Tuning**&**Red Flag Token**                         |
| 25.02 |                                                                         LMU Munich                                                                          |                             arxiv                              |                            [Soft Token Attacks Cannot Reliably Audit Unlearning in Large Language Models](https://arxiv.org/abs/2502.15836)                             |                          **Unlearning in LLMs**&**Soft Token Attacks**&**Model Auditing**                          |
| 25.02 |                                                                         PRISM Eval                                                                          |                           AAAI 2025                            |                [Multi-Agent Security Tax: Trading Off Security and Collaboration Capabilities in Multi-Agent Systems](https://arxiv.org/abs/2502.19145)                 |                       **Multi-Agent Systems**&**Security Trade-Offs**&**Jailbreak Defense**                        |
| 25.02 |                                                                  NEC Laboratories America                                                                   |                GenAI4Health Workshop AAAI 2025                 |           [Reducing Hallucinations of Medical Multimodal Large Language Models with Visual Retrieval-Augmented Generation](https://arxiv.org/abs/2502.15040)            |         **Hallucination Reduction**&**Medical Multimodal LLMs**&**Visual Retrieval-Augmented Generation**          |
| 25.02 |                                                          √âcole Polytechnique F√©d√©rale de Lausanne                                                           |                           TMLR 2025                            |                           [Single-pass Detection of Jailbreaking Input in Large Language Models](https://openreview.net/forum?id=42v6I5Ut9a)                            |                   **Jailbreak Detection**&**Single-pass Defense**&**Logit-based Classification**                   |
| 25.02 |                                                    Harbin Institute of Technology, Shenzhen, Baidu Inc.                                                     |                             arXiv                              |                           [The Rise of Darkness: Safety-Utility Trade-Offs in Role-Playing Dialogue Agents](https://arxiv.org/abs/2502.20757)                           |         **Role-Playing Dialogue Agents**&**Safety-Utility Trade-Off**&**Adaptive Preference Optimization**         |
| 25.02 |                                              Harbin Institute of Technology & Singapore Management University                                               |                             arXiv                              |                    [Beware of Your Po! Measuring and Mitigating AI Safety Risks in Role-Play Fine-Tuning of LLMs](https://arxiv.org/abs/2502.20968)                     |                             **Role-Playing LLMs**&**AI Safety**&**Fine-Tuning Risks**                              |
| 25.03 |                                                               Singapore Management University                                                               |                             arXiv                              |                          [Zero-Shot Defense Against Toxic Images via Inherent Multimodal Alignment in LVLMs](https://arxiv.org/abs/2503.00037)                          |                     **Toxic Image Detection**&**Vision-Language Models**&**Multimodal Safety**                     |
| 25.03 |                                                             ShanghaiTech University, Quantstamp                                                             |                             arXiv                              |               [Breaking the Loop: Detecting and Mitigating Denial-of-Service Vulnerabilities in Large Language Models](https://arxiv.org/abs/2503.00416)                |                      **Denial-of-Service**&**Large Language Models**&**Recurrent Generation**                      |
| 25.03 |                                                               Georgia Institute of Technology                                                               |                             arXiv                              |                           [Safety Tax: Safety Alignment Makes Your Large Reasoning Models Less Reasonable](https://arxiv.org/abs/2503.00555)                            |                      **Large Reasoning Models**&**Safety Alignment**&**Reasoning Trade-off**                       |
| 25.03 |                                                       Appier AI Research, National Taiwan University                                                        |                             arXiv                              |                        [Answer, Refuse, or Guess? Investigating Risk-Aware Decision Making in Language Models](https://arxiv.org/abs/2503.01332)                        |                   **Risk-Aware Decision Making**&**Language Models**&**Uncertainty Calibration**                   |
| 25.03 |                                                Technical University of Munich, Mila, Universit√© de Montr√©al                                                 |                             arXiv                              |                                               [LLM-Safety Evaluations Lack Robustness](https://arxiv.org/abs/2503.02574)                                                |                            **LLM Safety Evaluation**&**Robustness**&**Bias Mitigation**                            |
| 25.03 |                                                                      Peking University                                                                      |                             arXiv                              |                  [SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning](https://arxiv.org/abs/2503.03480)                  |                 **Vision-Language-Action Models**&**Safe Reinforcement Learning**&**Robot Safety**                 |
| 25.03 |                                                             University of California, Berkeley                                                              |                             arXiv                              |                                   [Improving LLM Safety Alignment with Dual-Objective Optimization](https://arxiv.org/abs/2503.03710)                                   |                   **LLM Safety Alignment**&**Dual-Objective Optimization**&**Jailbreak Defense**                   |
| 25.03 |                                       University of Science and Technology of China, Nanyang Technological University                                       |                             arXiv                              |               [AgentSafe: Safeguarding Large Language Model-based Multi-agent Systems via Hierarchical Data Management](https://arxiv.org/abs/2503.04392)               |              **Multi-agent Systems Security**&**Hierarchical Data Management**&**Memory Protection**               |
| 25.03 |                                                        Nanjing University of Science and Technology                                                         |                             arxiv                              |                         [Adversarial Training for Multimodal Large Language Models against Jailbreak Attacks](https://arxiv.org/abs/2503.04833)                         |                **Multimodal Large Language Models**&**Adversarial Training**&**Jailbreak Defense**                 |
| 25.03 |                                                                 Carnegie Mellon University                                                                  |                             arxiv                              |                    [Safety is Not Only About Refusal: Reasoning-Enhanced Fine-tuning for Interpretable LLM Safety](https://arxiv.org/abs/2503.05021)                    |                           **LLM Safety**&**Reasoning Fine-Tuning**&**Jailbreak Defense**                           |
| 25.03 |                                                                           Google                                                                            |                             arxiv                              |                                                 [BSAFE: (B)acktracking for (SAFE)ty](https://arxiv.org/abs/2503.08919)                                                  |                          **LLM Safety**&**Backtracking Alignment**&**Jailbreak Defense**                           |
| 25.03 |                                                              National University of Singapore                                                               |                            S&P 2025                            |                          [Prompt Inversion Attack against Collaborative Inference of Large Language Models](https://arxiv.org/abs/2503.09022)                           |                      **Prompt Inversion**&**Collaborative Inference**&**LLM Privacy Attack**                       |
| 25.03 |                                              Institute of Information Engineering, Chinese Academy of Sciences                                              |                             arxiv                              |                          [Align in Depth: Defending Jailbreak Attacks via Progressive Answer Detoxification](https://arxiv.org/abs/2503.11185)                          |                          **Jailbreak Defense**&**Safety Fine-tuning**&**LLM Robustness**                           |
| 25.03 |                                                                           XCALLY                                                                            |                             arxiv                              |                             [Prompt Injection Detection and Mitigation via AI Multi-Agent NLP Frameworks](https://arxiv.org/abs/2503.11517)                             |                            **Prompt Injection**&**Multi-Agent Systems**&**AI Security**                            |
| 25.03 |                                                                  Michigan State University                                                                  |                             arxiv                              |                              [Safety Mirage: How Spurious Correlations Undermine VLM Safety Fine-tuning](https://arxiv.org/abs/2503.11832)                              |                     **Vision-Language Models**&**Safety Fine-tuning**&**Spurious Correlation**                     |
| 25.03 |                                                     Beijing University of Posts and Telecommunications                                                      |                             arxiv                              |                         [MirrorGuard: Adaptive Defense Against Jailbreaks via Entropy-Guided Mirror Crafting](https://arxiv.org/abs/2503.12931)                         |                         **Jailbreak Defense**&**Entropy Analysis**&**Prompt Calibration**                          |
| 25.03 |                                                            The Hong Kong Polytechnic University                                                             |                             arxiv                              |                              [Towards Harmless Multimodal Assistants with Blind Preference Optimization](https://arxiv.org/abs/2503.14189)                              |                        **Multimodal LLMs**&**Safety Alignment**&**Preference Optimization**                        |
| 25.03 |                                                           University of Modena and Reggio Emilia                                                            |                           CVPR 2025                            |                                           [Hyperbolic Safety-Aware Vision-Language Models](https://arxiv.org/abs/2503.12127)                                            |                   **Safety-Aware Retrieval**&**Vision-Language Models**&**Hyperbolic Embedding**                   |
| 25.03 |                                                                           OpenAI                                                                            |                             arxiv                              |                                 [OpenAI‚Äôs Approach to External Red Teaming for AI Models and Systems](https://arxiv.org/abs/2503.16431)                                 |                            **Red Teaming**&**AI Risk Assessment**&**Safety Evaluation**                            |
| 25.03 |                                                           Unicom Data Intelligence, China Unicom                                                            |                             arxiv                              |                              [Safety Evaluation and Enhancement of DeepSeek Models in Chinese Contexts](https://arxiv.org/abs/2503.16529)                               |                              **Safety Evaluation**&**Distillation**&**Chinese LLMs**                               |
| 25.03 |                                                                     Zhejiang University                                                                     |                             arxiv                              |                                      [Towards LLM Guardrails via Sparse Representation Steering](https://arxiv.org/abs/2503.16851)                                      |                        **Representation Engineering**&**Sparse Autoencoder**&**LLM Safety**                        |
| 25.03 |                                           Technical University of Munich, Ludwig Maximilian University of Munich                                            |                             arxiv                              |                    [THINK BEFORE REFUSAL: Triggering Safety Reflection in LLMs to Mitigate False Refusal Behavior](https://arxiv.org/abs/2503.17882)                    |                              **False Refusal**&**Safety Alignment**&**LLM Reasoning**                              |
| 25.03 |                                                     The Hong Kong University of Science and Technology                                                      |                             arxiv                              |                     [STShield: Single-Token Sentinel for Real-Time Jailbreak Detection in Large Language Models](https://arxiv.org/abs/2503.17932)                      |                          **Jailbreak Detection**&**LLM Safety**&**Adversarial Training**                           |
| 25.03 |                                                               Zhejiang University, Ant Group                                                                |                             arxiv                              |                                 [LookAhead Tuning: Safer Language Models via Partial Answer Previews](https://arxiv.org/abs/2503.19041)                                 |                               **Safe Fine-tuning**&**Answer Prefix**&**LLM Safety**                                |
| 25.03 |                                                       IBM Research, Rensselaer Polytechnic Institute                                                        |                             arxiv                              |                            [Fundamental Safety-Capability Trade-offs in Fine-tuning Large Language Models](https://arxiv.org/abs/2503.20807)                            |                 **Safety-Capability Trade-off**&**Fine-tuning Strategy**&**Theoretical Analysis**                  |
| 25.03 |                                                          Technical University Munich, IBM Research                                                          |                  ICLR 2025 Workshop on BTLMA                   |          [SAFEMERGE: Preserving Safety Alignment in Fine-Tuned Large Language Models via Selective Layer-Wise Model Merging](https://arxiv.org/abs/2503.17239)          |                        **Safety Alignment**&**Model Merging**&**Post-Fine-Tuning Defense**                         |
| 25.03 |                                                                          HydroX AI                                                                          |                             arxiv                              |                          [Optimizing Safe and Aligned Language Generation: A Multi-Objective GRPO Approach](https://arxiv.org/abs/2503.21819)                           |        **Language Model Alignment**&**Multi-Objective Optimization**&**Group Relative Policy Optimization**        |
| 25.04 |                                                        University of Science and Technology of China                                                        |                             arxiv                              |              [LightDefense: A Lightweight Uncertainty-Driven Defense against Jailbreaks via Shifted Token Distribution](https://arxiv.org/abs/2504.01533)               |                       **LLM Safety**&**Jailbreak Defense**&**Token Distribution Adjustment**                       |
| 25.04 |                                                                  Seoul National University                                                                  |                             arxiv                              |                                       [Representation Bending for Large Language Model Safety](https://arxiv.org/abs/2504.01550)                                        |                      **LLM Safety**&**Representation Engineering**&**Adversarial Robustness**                      |
| 25.04 |                                                                 LG Toronto AI Research Lab                                                                  |                             arxiv                              |                       [The Hidden Space of Safety: Understanding Preference-Tuned LLMs in Multilingual Context](https://arxiv.org/abs/2504.02708)                       |                       **Multilingual Alignment**&**LLM Safety**&**Representation Analysis**                        |
| 25.04 |                                                                     Zhejiang University                                                                     |                             arxiv                              |                           [ERPO: Advancing Safety Alignment via Ex-Ante Reasoning Preference Optimization](https://arxiv.org/abs/2504.02725)                            |                     **LLM Safety Alignment**&**Ex-Ante Reasoning**&**Preference Optimization**                     |
| 25.04 |                                                                     Carleton University                                                                     |                             arxiv                              |                    [Safety Modulation: Enhancing Safety in Reinforcement Learning through Cost-Modulated Rewards](https://arxiv.org/abs/2504.03040)                     |                     **Safe Reinforcement Learning**&**Constrained MDP**&**Reward Modulation**                      |
| 25.04 |                                                                       VMware Research                                                                       |                             arxiv                              |                                           [Bypassing Safety Guardrails in LLMs Using Humor](https://arxiv.org/abs/2504.06577)                                           |                           **LLM Jailbreaking**&**Prompt Engineering**&**Humor Attacks**                            |
| 25.04 |                                                                    University of Warwick                                                                    |                             arxiv                              |                                    [Detecting Malicious AI Agents Through Simulated Interactions](https://arxiv.org/abs/2504.03726)                                     |                **Malicious intent detection**&**Human-AI interaction**&**Manipulation techniques**                 |
| 25.04 |                                                                 Chinese Academy of Sciences                                                                 |                             arxiv                              |                        [SINCon: Mitigate LLM-Generated Malicious Message Injection Attack for Rumor Detection](https://arxiv.org/abs/2504.07135)                        |                     **Rumor Detection**&**Message Injection Attack**&**Contrastive Learning**                      |
| 25.04 |                                                                University of South Carolina                                                                 |                             arxiv                              |             [SafeChat: A Framework for Building Trustworthy Collaborative Assistants and a Case Study of its Usefulness](https://arxiv.org/abs/2504.07995)              |                     **Collaborative Assistant**&**Trustworthy Chatbot**&**SafeChat Framework**                     |
| 25.04 |                                                                 Carnegie Mellon University                                                                  |                             arxiv                              |                 [SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs](https://arxiv.org/abs/2504.08192)                 |                            **Machine Unlearning**&**Sparse Autoencoder**&**LLM Safety**                            |
| 25.04 |                                                               Harbin Institute of Technology                                                                |                             arxiv                              |                               [AdaSteer: Your Aligned LLM is Inherently an Adaptive Jailbreak Defender](https://arxiv.org/abs/2504.09466)                               |                         **Jailbreak Defense**&**Activation Steering**&**Adaptive Safety**                          |
| 25.04 |                                                                City University of Hong Kong                                                                 |                             arxiv                              |                                           [ControlNet: A Firewall for RAG-based LLM System](https://arxiv.org/abs/2504.09593)                                           |                               **RAG Security**&**AI Firewall**&**Activation Shift**                                |
| 25.04 |                                                                   Independent Researchers                                                                   |                             arxiv                              |                                                  [Mitigating Many-Shot Jailbreaking](https://arxiv.org/abs/2504.09604)                                                  |                     **Many-Shot Jailbreaking**&**Fine-Tuning Defense**&**Input Sanitization**                      |
| 25.04 |                                                                        Georgia Tech                                                                         |                             arxiv                              |                                            [The Structural Safety Generalization Problem](https://arxiv.org/abs/2504.09712)                                             |                             **Safety Generalization**&**Jailbreaking**&**Guardrails**                              |
| 25.04 |                                                                     University of Utah                                                                      |                             arxiv                              |                                     [Alleviating the Fear of Losing Alignment in LLM Fine-tuning](https://arxiv.org/abs/2504.09757)                                     |                    **LLM Alignment**&**Fine-tuning Recovery**&**Harmful Direction Restoration**                    |
| 25.04 |                                                                         UC Berkeley                                                                         |                             arxiv                              |                                       [Progent: Programmable Privilege Control for LLM Agents](https://arxiv.org/abs/2504.11703)                                        |                            **LLM Agents**&**Privilege Control**&**Policy Programming**                             |
| 25.04 |                                                                          Microsoft                                                                          |                             arxiv                              |                [AttentionDefense: Leveraging System Prompt Attention for Explainable Defense Against Novel Jailbreaks](https://arxiv.org/abs/2504.12321)                |                       **Jailbreak Detection**&**Explainable AI**&**System Prompt Attention**                       |
| 25.04 |                                                                Shanghai Jiao Tong University                                                                |                             arxiv                              |                      [VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven Prompt Optimization](https://arxiv.org/abs/2504.12661)                      |                      **Vision-Language Model**&**Safety Alignment**&**Multimodal Reasoning**                       |
| 25.04 |                                                                        Sporo Health                                                                         |                             arxiv                              |                              [MCP Guardian: A Security-First Layer for Safeguarding MCP-Based AI System](https://arxiv.org/abs/2504.12757)                              |                             **Model Context Protocol**&**Agentic AI**&**AI Security**                              |
| 25.04 |                                                          University of Chinese Academy of Sciences                                                          |                           CVPR 2025                            |                 [Do We Really Need Curated Malicious Data for Safety Alignment in Multi-modal Large Language Models?](https://arxiv.org/abs/2504.13052)                 |                              **MLLM Safety**&**Rejection Tuning**&**Compliance Bias**                              |
| 25.04 |                                                                      Fudan University                                                                       |                             arxiv                              |        [Concept Enhancement Engineering: A Lightweight and Efficient Robust Defense Against Jailbreak Attacks in Embodied AI](https://arxiv.org/abs/2504.13201)         |                   **Embodied Intelligence**&**Jailbreak Attacks**&**Representation Engineering**                   |
| 25.04 |                                                                      Tongji University                                                                      |                             arxiv                              |                         [DETAM: Defending LLMs Against Jailbreak Attacks via Targeted Attention Modification](https://arxiv.org/abs/2504.13562)                         |                  **LLM Jailbreak Defense**&**Attention Modification**&**Inference-Time Strategy**                  |
| 25.04 |                                                               Quebec University at Chicoutimi                                                               |                             arxiv                              |               [A DATA-CENTRIC APPROACH FOR SAFE AND SECURE LARGE LANGUAGE MODELS AGAINST THREATENING AND TOXIC CONTENT](https://arxiv.org/abs/2504.16120)               |                       **Toxicity Mitigation**&**Post-generation Correction**&**LLM Safety**                        |
| 25.04 |                                                                           Intuit                                                                            |                             arxiv                              |                              [Building A Secure Agentic AI Application Leveraging Google‚Äôs A2A Protocol](https://arxiv.org/abs/2504.16902)                              |                            **Agentic AI**&**A2A Protocol**&**MAESTRO Threat Modeling**                             |
| 25.04 |                                                                     Zhejiang University                                                                     |                             arxiv                              |               [DUALBREACH: Efficient Dual-Jailbreaking via Target-Driven Initialization and Multi-Target Optimization](https://arxiv.org/abs/2504.18564)                |                               **Jailbreaking**&**Guardrails**&**Adversarial Attack**                               |
| 25.04 |                                                                       Microsoft India                                                                       |                             arxiv                              |                                         [SAGE: A Generic Framework for LLM Safety Evaluation](https://arxiv.org/abs/2504.19674)                                         |                     **LLM Safety Evaluation**&**Adversarial Testing**&**Multi-turn Dialogue**                      |
| 25.04 |                                                                    Chongqing University                                                                     |                             arxiv                              |                [Evaluate-and-Purify: Fortifying Code Language Models Against Adversarial Attacks Using LLM-as-a-Judge](https://arxiv.org/abs/2504.19730)                |                         **Adversarial Attack**&**Code Language Model**&**LLM-as-a-Judge**                          |
| 25.04 |                                                              National University of Singapore                                                               |                             arxiv                              |           [Robustness via Referencing: Defending against Prompt Injection Attacks by Referencing the Executed Instruction](https://arxiv.org/abs/2504.20472)            |                        **Prompt Injection**&**Instruction Referencing**&**LLM Robustness**                         |
| 25.04 |                                                               Beijing Institute of Technology                                                               |                           ACM MM‚Äô25                            |                          [AGATE: Stealthy Black-box Watermarking for Multimodal Model Copyright Protection](https://arxiv.org/abs/2504.21044)                           |                **Black-box Watermarking**&**Model Copyright Protection**&**Watermarking Security**                 |
| 25.05 |                                                                  University of Cincinnati                                                                   |                             arxiv                              |                         [Jailbreak Detection in Clinical Training LLMs Using Feature-Based Predictive Models](https://arxiv.org/abs/2505.00010)                         |                      **Jailbreak Detection**&**Clinical Simulation**&**Fuzzy Decision Tree**                       |
| 25.05 |                                                            University of California, Santa Cruz                                                             |                             arxiv                              |                                        [Large Language Models are Autonomous Cyber Defenders](https://arxiv.org/abs/2505.04843)                                         |                        **Autonomous Cyber Defense**&**LLM Agents**&**Multi-Agent Systems**                         |
| 25.05 |                                                                     Nanjing University                                                                      |                             arxiv                              |                               [Defending against Indirect Prompt Injection by Instruction Detection](https://arxiv.org/abs/2505.06311v1)                                |               **Indirect Prompt Injection**&**Instruction Detection**&**Behavioral State Analysis**                |
| 25.05 |                                                                   University of Cambridge                                                                   |                             arxiv                              |                                     [Adversarial Suffix Filtering: a Defense Pipeline for LLMs](https://arxiv.org/abs/2505.09602v1)                                     |                     **Adversarial Suffix**&**Jailbreak Defense**&**Model-Agnostic Filtering**                      |
| 25.05 |                                                                     Saarland University                                                                     |                             arxiv                              |                               [ProxyPrompt: Securing System Prompts against Prompt Extraction Attacks](https://arxiv.org/abs/2505.11459)                                |                        **Prompt Extraction**&**System Prompt Protection**&**LLM Security**                         |
| 25.05 |                                                                      Fudan University                                                                       |                             arxiv                              |                                    [SAFEVID: Toward Safety Aligned Video Large Multimodal Models](https://arxiv.org/abs/2505.11926)                                     |                    **Video Multimodal Models**&**Safety Alignment**&**Preference Optimization**                    |
| 25.05 |                                                                   George Mason University                                                                   |                             arxiv                              |                          [Web IP at Risk: Prevent Unauthorized Real-Time Retrieval by Large Language Models](https://arxiv.org/abs/2505.12655)                          |                         **Web IP Protection**&**LLM Anti-Retrieval**&**Semantic Defense**                          |
| 25.05 |                                                                         ARIMLABS.AI                                                                         |                             arxiv                              |                                              [The Hidden Dangers of Browsing AI Agents](https://arxiv.org/abs/2505.13076)                                               |                         **Browsing Agents**&**Prompt Injection**&**Multi-Layer Security**                          |
| 25.05 |                                              Institute of Information Engineering, Chinese Academy of Sciences                                              |                             arxiv                              |                      [EcoSafeRAG: Efficient Security through Context Analysis in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.13506)                      |                  **RAG Security**&**Contextual Diversity Detection**&**Corpus Poisoning Defense**                  |
| 25.05 |                                                        Beijing Institute of AI Safety and Governance                                                        |                             arxiv                              |                            [PANDAGUARD: Systematic Evaluation of LLM Safety against Jailbreaking Attacks](https://arxiv.org/abs/2505.13862)                             |                      **Jailbreak Evaluation**&**Multi-Agent System**&**LLM Safety Benchmark**                      |
| 25.05 |                                                                    University of Oxford                                                                     |                             arxiv                              |                     [SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors](https://arxiv.org/abs/2505.14300)                     |                   **Harmful Output Detection**&**Backdoor Behavior**&**Unsupervised Monitoring**                   |
| 25.05 |                                                                            HKUST                                                                            |                             arxiv                              |                                 [MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol](https://arxiv.org/abs/2505.14590)                                 |                               **MCP**&**Contextual Integrity**&**Safety Benchmark**                                |
| 25.05 |                                                                      Yonsei University                                                                      |                             arxiv                              |                           [SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment](https://arxiv.org/abs/2505.14667)                            |                     **Safety Alignment**&**Chain-of-Thought Reasoning**&**Early Intervention**                     |
| 25.05 |                                                                     Tsinghua University                                                                     |                             arxiv                              |                          [How Should We Enhance the Safety of Large Reasoning Models: An Empirical Study](https://arxiv.org/abs/2505.15404v1)                           |                       **Large Reasoning Models**&**Safety Fine-Tuning**&**Chain-of-Thought**                       |
| 25.05 |                                                                      Peking University                                                                      |                             arxiv                              |                                  [Advancing LLM Safe Alignment with Safety Representation Ranking](https://arxiv.org/abs/2505.15710v1)                                  |                       **Safety Alignment**&**Representation Learning**&**Response Ranking**                        |
| 25.05 |                                                                      Peking University                                                                      |                             arxiv                              |                      [Scalable Defense against In-the-wild Jailbreaking Attacks with Safety Context Retrieval](https://arxiv.org/abs/2505.15753v1)                      |                       **Jailbreaking Defense**&**Safety Context Retrieval**&**LLM Security**                       |
| 25.05 |                                                                        UC Santa Cruz                                                                        |                             arxiv                              |                                   [SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning](https://arxiv.org/abs/2505.16186v1)                                    |                          **Safety Alignment**&**Jailbreak Defense**&**Reasoning Models**                           |
| 25.05 |                                                                      Nankai University                                                                      |                             arxiv                              |                    [CTRAP: Embedding Collapse Trap to Safeguard Large Language Models from Harmful Fine-Tuning](https://arxiv.org/abs/2505.16559v1)                     |                        **Fine-tuning Security**&**Collapse Trap**&**LLM Alignment Defense**                        |
| 25.05 |                                                                      Peking University                                                                      |                             arxiv                              |                            [Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization](https://arxiv.org/abs/2505.16737v1)                             |                         **Fine-tuning Risk**&**Safety Optimization**&**Gradient Probing**                          |
| 25.05 |                                                                      Wuhan University                                                                       |                             arxiv                              |                                  [Backdoor Cleaning without External Guidance in MLLM Fine-tuning](https://arxiv.org/abs/2505.16916v1)                                  |                           **Backdoor Defense**&**Multimodal LLMs**&**Attention Entropy**                           |
| 25.05 |                                                                East China Normal University                                                                 |                       ACL 2025 Findings                        |             [Hierarchical Safety Realignment: Lightweight Restoration of Safety in Pruned Large Vision-Language Models](https://arxiv.org/abs/2505.16104v1)             |                              **Model Pruning**&**LVLM Safety**&**Neuron Restoration**                              |
| 25.05 |                                                                     Nanjing University                                                                      |                       ACL 2025 Findings                        |                [Why Not Act on What You Know? Unleashing Safety Potential of LLMs via Self-Aware Guard Enhancement](https://arxiv.org/abs/2505.12060v1)                 |                       **LLM Safety**&**Jailbreak Defense**&**Discrimination-Generation Gap**                       |
| 25.05 |                                                               Harbin Institute of Technology                                                                |                            ACL 2025                            |                                  [MPO: Multilingual Safety Alignment via Reward Gap Optimization](https://arxiv.org/abs/2505.16869v1)                                   |                       **Multilingual Safety**&**Reward Gap Optimization**&**LLM Alignment**                        |
| 25.05 |                                                        Southern University of Science and Technology                                                        |                           ICML 2025                            |                       [Safe Delta: Consistently Preserving Safety when Fine-Tuning LLMs on Diverse Datasets](https://arxiv.org/abs/2505.12038v1)                        |                           **Safety Fine-Tuning**&**Delta Optimization**&**LLM Defense**                            |
| 25.05 |                                                               North Carolina State University                                                               |                           ICML 2025                            | [Safety Alignment Can Be Not Superficial With Explicit Safety Signals](https://arxiv.org/abs/2505.17072) | **Safety Alignment**&**LLMs**&**Binaray Classification** |
| 25.05 |                                                                            Impel                                                                            |                             arxiv                              | [Improving LLM Outputs Against Jailbreak Attacks with Expert Model Integration](https://arxiv.org/abs/2505.17066v1) | **Jailbreak Attacks**&**Expert Model Integration**&**Prompt Injection** |
| 25.05 |                                                               Beijing Institute of Technology                                                               |                             arxiv                              | [RRTL: Red Teaming Reasoning Large Language Models in Tool Learning](https://arxiv.org/abs/2505.17106v1) | **Red Teaming**&**Reasoning LLMs**&**Tool Learning** |
| 25.05 |                                                                     Tsinghua University                                                                     |                             arxiv                              | [Does Chain-of-Thought Reasoning Really Reduce Harmfulness from Jailbreaking?](https://arxiv.org/abs/2505.17650v1) | **Chain-of-Thought Reasoning**&**Jailbreaking**&**Harmfulness Modeling** |
| 25.05 |                                                                       Duke University                                                                       |                             arxiv                              | [A Critical Evaluation of Defenses against Prompt Injection Attacks](https://arxiv.org/abs/2505.18333v1) | **Prompt Injection**&**Defense Evaluation**&**Adaptive Attack** |
| 25.05 |                                                          Harbin Institute of Technology (Shenzhen)                                                          |                             arxiv                              | [Safety Alignment via Constrained Knowledge Unlearning](https://arxiv.org/abs/2505.18588) | **Safety Alignment**&**Knowledge Unlearning**&**Jailbreak Defense** |
| 25.05 |                                                     Beijing University of Posts and Telecommunications                                                      |                             arxiv                              | [PD3F: A Pluggable and Dynamic DoS-Defense Framework Against Resource Consumption Attacks Targeting Large Language Models](https://arxiv.org/abs/2505.18680) | **DoS Defense**&**Resource Consumption Attack**&**Large Language Models** |
| 25.05 |                                                 King Abdullah University of Science and Technology (KAUST)                                                  |                             arxiv                              | [An Embarrassingly Simple Defense Against LLM Abliteration Attacks](https://arxiv.org/abs/2505.19056v1) | **Abliteration Attack**&**Extended Refusal**&**LLM Alignment** |
| 25.05 |                                                                    King‚Äôs College London                                                                    |                             arxiv                              | [GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling](https://arxiv.org/abs/2505.19234v1) | **Multi-Agent Collaboration**&**Safety Detection**&**Temporal Graph Modeling** |
| 25.05 |                                                                     Sichuan University                                                                      |                             arxiv                              | [ALRPHFS: Adversarially Learned Risk Patterns with Hierarchical Fast & Slow Reasoning for Robust Agent Defense](https://arxiv.org/abs/2505.19260) | **Agent Defense**&**Adversarial Learning**&**Hierarchical Reasoning** |
| 25.05 |                                                            University of Maryland, College Park                                                             |                             arxiv                              | [CoTGuard: Using Chain-of-Thought Triggering for Copyright Protection in Multi-Agent LLM Systems](https://arxiv.org/abs/2505.19405) | **Copyright Protection**&**Chain-of-Thought**&**Multi-Agent LLM** |
| 25.05 |                                                                           NVIDIA                                                                            |                             arxiv                              | [Safety Through Reasoning: An Empirical Study of Reasoning Guardrail Models](https://arxiv.org/abs/2505.20087v1) | **Reasoning Guardrails**&**LLM Safety**&**Data Efficiency** |
| 25.05 |                                                                         Sea AI Lab                                                                          |                             arxiv                              | [Lifelong Safety Alignment for Language Models](https://arxiv.org/abs/2505.20259) | **Lifelong Safety Alignment**&**Jailbreak Attack**&**Meta-Attacker** |
| 25.05 |                                                                   University of Melbourne                                                                   |                           ICLR 2025                            | [MULTI-LEVEL CERTIFIED DEFENSE AGAINST POISONING ATTACKS IN OFFLINE REINFORCEMENT LEARNING](https://arxiv.org/abs/2505.20621) | **Certified Defense**&**Poisoning Attack**&**Offline Reinforcement Learning** |
| 25.05 |                                              NLPR & MAIS, Institute of Automation, Chinese Academy of Sciences                                              |                             arxiv                              | [Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models](https://arxiv.org/abs/2505.22271v1) | **Jailbreak Defense**&**Test-Time Learning**&**Multimodal LLM** |
| 25.05 |                                                                         SentinelAI                                                                          |                             arxiv                              | [Operationalizing CaMeL: Strengthening LLM Defenses for Enterprise Deployment](https://arxiv.org/abs/2505.22852v1) | **LLM Defense**&**Enterprise Security**&**Prompt Injection** |
| 25.05 |                                                                     Beihang University                                                                      |                             arxiv                              | [Detecting Stealthy Backdoor Samples based on Intra-class Distance for Large Language Models](https://arxiv.org/abs/2505.23015v1) | **Backdoor Detection**&**LLM Security**&**TF-IDF Clustering** |
| 25.05 |                                                                           Leidos                                                                            |                             arxiv                              | [MCP Safety Training: Learning to Refuse Falsely Benign MCP Exploits using Improved Preference Alignment](https://arxiv.org/abs/2505.23634v1) | **MCP Safety**&**Falsely Benign Attack**&**Preference Alignment** |
| 25.05 |                                                                          Microsoft                                                                          |                             arxiv                              | [Securing AI Agents with Information-Flow Control](https://arxiv.org/abs/2505.23643v1) | **Information-Flow Control**&**Agent Security**&**Prompt Injection Defense** |
| 25.05 |                                              Institute of Information Engineering, Chinese Academy of Sciences                                              |                            ACL 2025                            | [AgentAlign: Navigating Safety Alignment in the Shift from Informative to Agentic Large Language Models](https://arxiv.org/abs/2505.23020v1) | **Agent Alignment**&**Safety Alignment**&**Agentic LLM** |
| 25.05 |                                                          Harbin Institute of Technology, Shenzhen                                                           |                            ACL 2025                            | [MTSA: Multi-turn Safety Alignment for LLMs through Multi-round Red-teaming](https://arxiv.org/abs/2505.17147v1) | **Multi-turn Alignment**&**Red-teaming**&**LLM Safety** |
| 25.05 | University of California, Los Angeles |                             arxiv                              | [Bootstrapping LLM Robustness for VLM Safety via Reducing the Pretraining Modality Gap](https://arxiv.org/abs/2505.24208v1) | **Vision-Language Model**&**Safety Alignment**&**Modality Gap Regularization** |
| 25.05 | ETH Z√ºrich |                           ICML 2025                            | [Learning Safety Constraints for Large Language Models](https://arxiv.org/abs/2505.24445v1) | **LLM Safety**&**Geometric Constraint**&**Adversarial Defense** |
| 25.06 | Technical University of Munich |                             arxiv                              | [SafeCOMM: What about Safety Alignment in Fine-Tuned Telecom Large Language Models?](https://arxiv.org/abs/2506.00062v1) | **Telecom LLM**&**Safety Alignment**&**Fine-Tuning Defense** |
| 25.06 | Duke University |                             arxiv                              | [SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues](https://arxiv.org/abs/2506.00668v1) | **LLM Safety**&**Multi-Turn Attack**&**Safety Reasoning Moderator** |
| 25.06 | Critical ML Lab, University of Toronto |                             arxiv                              | [SafeTuneBed: A Toolkit for Benchmarking LLM Safety Alignment in Fine-Tuning](https://arxiv.org/abs/2506.00676v1) | **LLM Safety**&**Fine-Tuning Defense**&**Benchmark Toolkit** |
| 25.06 | Peking University |                             arxiv                              | [ReGA: Representation-Guided Abstraction for Model-based Safeguarding of LLMs](https://arxiv.org/abs/2506.01770v1) | **LLM Safety**&**Model-Based Analysis**&**Representation Engineering** |
| 25.06 | Michigan State University |                             arxiv                              | [Attention Knows Whom to Trust: Attention-based Trust Management for LLM Multi-Agent Systems](https://arxiv.org/abs/2506.02546v1) | **LLM Multi-Agent**&**Trustworthiness**&**Attention-based Defense** |
| 25.06 | University of Wisconsin-Madison |                             arxiv                              | [Through the Stealth Lens: Rethinking Attacks and Defenses in RAG](https://arxiv.org/abs/2506.04390v1) | **RAG**&**Stealthy Attack**&**Attention-Variance Filter** |
| 25.06 | Washington University in St. Louis |                       ACL 2025 Findings                        | [COSMIC: Generalized Refusal Direction Identification in LLM Activations](https://arxiv.org/abs/2506.00085v1) | **LLM Safety**&**Refusal Direction**&**Mechanistic Interpretability** |
| 25.06 | Institute of Information Engineering, Chinese Academy of Sciences; Nanyang Technological University; Sun Yat-sen University-Shenzhen |                             arxiv                              | [Robust Anti-Backdoor Instruction Tuning in LVLMs](https://arxiv.org/abs/2506.05401) | **LVLM**&**Backdoor Defense**&**Instruction Tuning** |
| 25.06 | Qualifire |                             arxiv                              | [SENTINEL: SOTA Model to Protect Against Prompt Injections](https://arxiv.org/abs/2506.05446) | **Prompt Injection**&**LLM Security**&**ModernBERT**&**Jailbreak** |
| 25.06 | Luxembourg Tech School |                             arxiv                              | [From Rogue to Safe AI: The Role of Explicit Refusals in Aligning LLMs with International Humanitarian Law](https://arxiv.org/abs/2506.06391v1) | **Generative AI Ethics**&**Harmful Content Mitigation**&**International Humanitarian Law**&**Prompt Engineering**&**Refusal Behaviour** |
| 25.06 | University of Southern California, University of California Berkeley, Peking University |                             arxiv                              | [Auditing Black-Box LLM APIs with a Rank-Based Uniformity Test](https://arxiv.org/abs/2506.06975) | **LLM Auditing**&**API Security**&**Model Equality Testing**&**Adversarial Detection** |
| 25.06 | National University of Singapore, University of Science and Technology of China, Harbin Institute of Technology |                             arxiv                              | [AlphaSteer: Learning Refusal Steering with Principled Null-Space Constraint](https://arxiv.org/abs/2506.07022) | **Refusal Steering**&**Activation Steering**&**Null-Space Constraint**&**LLM Safety** |
| 25.06 | Korea Advanced Institute of Science and Technology (KAIST) |                             arxiv                              | [Refusal-Feature-guided Teacher for Safe Finetuning via Data Filtering and Alignment Distillation](https://arxiv.org/abs/2506.07356v1) | **Safe Finetuning**&**Refusal Feature**&**Data Filtering**&**Alignment Distillation**&**LLM Safety** |
| 25.06 | Massachusetts Institute of Technology |                             arxiv                              | [When Style Breaks Safety: Defending Language Models Against Superficial Style Alignment](https://arxiv.org/abs/2506.07452) | **LLM Safety**&**Style Alignment**&**Jailbreak**&**Fine-Tuning Defense** |
| 25.06 | Texas A&M University, UC San Diego, UC Irvine, University of Wisconsin‚ÄìMadison, Carnegie Mellon University, University of Michigan, Columbia University, Meta |                             arxiv                              | [SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems](https://arxiv.org/abs/2506.07564v3) | **Agent System**&**LLM/VLM Safety**&**Information Flow Control**&**Concurrency**&**Benchmark** |
| 25.06 | National University of Singapore, Cornell University, University of Electronic Science and Technology, Harbin Institute of Technology, Nanyang Technological University |                             arxiv                              | [RSafe: Incentivizing proactive reasoning to build robust and adaptive LLM safeguards](https://arxiv.org/abs/2506.07736) | **LLM Safeguard**&**Reasoning Alignment**&**Guard Model**&**RL**&**Safety Generalization** |
| 25.06 | Stony Brook University, Penn State University |                             arxiv                              | [Your Agent Can Defend Itself against Backdoor Attacks](https://arxiv.org/abs/2506.08336v2) | **LLM Agent**&**Backdoor Attack**&**Self-Defense**&**Consistency Check**&**Agent Security** |
| 25.06 | Shanghai AI Laboratory, Fudan University |                             arxiv                              | [SAFECOT: Improving VLM Safety with Minimal Reasoning](https://arxiv.org/abs/2506.08399v2) | **VLM Safety**&**Chain-of-Thought**&**Refusal Behavior**&**Minimal Supervision** |
| 25.06 | UC Santa Barbara, UC San Francisco |                             arxiv                              | [Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations](https://arxiv.org/abs/2506.09067v1) | **Medical Vision-Language Model**&**Safety Alignment**&**Synthetic Demonstration**&**Jailbreak Defense**&**Over-Defense** |
| 25.06 | Tsinghua University, Beihang University, Peking University |                             arxiv                              | [DAVSP: Safety Alignment for Large Vision-Language Models via Deep Aligned Visual Safety Prompt](https://arxiv.org/abs/2506.09353v1) | **LVLM Safety**&**Visual Safety Prompt**&**Deep Alignment**&**Adversarial Defense** |
| 25.06 | The Hong Kong University of Science and Technology, Renmin University of China |                             arxiv                              | [SoK: Evaluating Jailbreak Guardrails for Large Language Models](https://arxiv.org/abs/2506.10597v1) | **Jailbreak Guardrail**&**LLM Security**&**Defense Evaluation**&**Taxonomy**&**Benchmark** |
| 25.06 | New York University, √âcole Polytechnique F√©d√©rale de Lausanne |                             arxiv                              | [Monitoring Decomposition Attacks in LLMs with Lightweight Sequential Monitors](https://arxiv.org/abs/2506.10949v1) | **Decomposition Attack**&**Sequential Monitor**&**LLM Safety**&**Prompt Obfuscation**&**Defense Evaluation** |
| 25.06 | Bytedance; Northern Arizona University; Pennsylvania State University |                            DSN 2025                            | [To Protect the LLM Agent Against the Prompt Injection Attack with Polymorphic Prompt](https://arxiv.org/abs/2506.05739v1) | **LLM**&**Prompt Injection**&**Agent Defense**&**Polymorphic Prompt**&**Separator Randomization** |
| 25.06 | University of Surrey |                             arxiv                              | [SecurityLingua: Efficient Defense of LLM Jailbreak Attacks via Security-Aware Prompt Compression](https://arxiv.org/abs/2506.12707v1) | **Jailbreak Defense**&**Prompt Compression**&**Security-Aware LLM**&**Token Efficiency**&**LLM Safety** |
| 25.06 | Tel Aviv University |                             arxiv                              | [Universal Jailbreak Suffixes Are Strong Attention Hijackers](https://arxiv.org/abs/2506.12880v1) | **Jailbreak**&**Universal Suffix**&**Attention Hijacking**&**LLM Security**&**Prompt Engineering** |
| 25.06 | University of S√£o Paulo |                             arxiv                              | [LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning](https://arxiv.org/abs/2506.15606v1) | **LLM Safety**&**Fine-tuning Robustness**&**Low-Rank Extrapolation**&**Alignment**&**Safety Subspace** |
| 25.06 | FnGuide Inc. |                         ACL 2025 WOAH                          | [QGuard: Question-based Zero-shot Guard for Multi-modal LLM Safety](https://arxiv.org/abs/2506.12299v1) | **Harmful Prompt Detection**&**Multi-modal LLM**&**Zero-shot Guard**&**Question Prompting**&**White-box Analysis** |
| 25.06 | Xi‚Äôan Jiaotong University |                             arxiv                              | [The Safety Reminder: A Soft Prompt to Reactivate Delayed Safety Awareness in Vision-Language Models](https://arxiv.org/abs/2506.15734v1) | **Vision-Language Model**&**Safety**&**Prompt Tuning** |
| 25.06 | Queen's University |                             arxiv                              | [Security Assessment of DeepSeek and GPT Series Models against Jailbreak Attacks](https://arxiv.org/abs/2506.18543) | **Large Language Models**&**Jailbreak Attacks**&**AI Security** |
| 25.06 | Beijing University of Posts and Telecommunications |                             arxiv                              | [MemeMind: A Large-Scale Multimodal Dataset with Chain-of-Thought Reasoning for Harmful Meme Detection](https://arxiv.org/abs/2506.18919) | **Harmful Meme Detection**&**Multimodal Dataset**&**Chain-of-Thought** |
| 25.06 | Nankai University |                           ICLR 2025                            | [Probe before You Talk: Towards Black-box Defense against Backdoor Unalignment for Large Language Models](https://arxiv.org/abs/2506.16447v1) | **Backdoor Defense**&**Black-box LLM**&**Alignment** |
| 25.06 | University of Luxembourg |                             arxiv                              | [Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large Language Models](https://arxiv.org/abs/2506.23576v1) | **Jailbreaking Defense**&**Multi-Agent LLM**&**Evaluation** |
| 25.06 | FAR.AI, UK AISI, OATML (University of Oxford) |                             arxiv                              | [STACK: Adversarial Attacks on LLM Safeguard Pipelines](https://arxiv.org/abs/2506.24068v1) | **Jailbreak**&**Defense Pipeline**&**Adversarial Attack** |
| 25.07 | Nanyang Technological University, Institute of Software (Chinese Academy of Sciences), Beihang University, Sun Yat-sen University, Beijing Institute of Technology, National University of Singapore |                             arxiv                              | [SafeMobile: Chain-level Jailbreak Detection and Automated Evaluation for Multimodal Mobile Agents](https://arxiv.org/abs/2507.00841v1) | **Mobile Agent**&**Jailbreak Defense**&**Multimodal** |
| 25.07 | Carnegie Mellon University |                             arxiv                              | [Reasoning as an Adaptive Defense for Safety](https://arxiv.org/abs/2507.00971v1) | **Reasoning**&**Safety Defense**&**RL** |
| 25.07 | Southwestern University of Finance and Economics, University of Electronic Science and Technology of China, Center for Future Media (UESTC), Tongji University, Engineering Research Center of Intelligent Finance (Ministry of Education) |                             arxiv                              | [SafePTR: Token-Level Jailbreak Defense in Multimodal LLMs via Prune-then-Restore Mechanism](https://arxiv.org/abs/2507.01513v1) | **Jailbreak Defense**&**Multimodal LLM**&**Token Pruning** |
| 25.07 | Meta FAIR, Universit√© Paris-Saclay, Inria, CNRS, INSA Rouen Normandy, LITIS, NYU Courant Institute and CDS |                             arxiv                              | [Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training](https://arxiv.org/abs/2507.01752v1) | **Black-Box Optimization**&**Privacy**&**Generalization** |
| 25.07 | Meta FAIR, UC Berkeley |                             arxiv                              | [Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks](https://arxiv.org/abs/2507.02735v1) | **Prompt Injection**&**Model-level Defense**&**Instruction Hierarchy** |
| 25.07 | KAIST |                       ACL 2025 Findings                        | [Enhancing LLM Agent Safety via Causal Influence Prompting](https://arxiv.org/abs/2507.00979v1) | **Safety Prompting**&**Causal Influence**&**Agent** |
| 25.07 | BUPT, HKBU, NUS, HKUST |                            ACL 2025                            | [AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness](https://arxiv.org/abs/2507.01702v1) | **Meme Harmfulness**&**Reasoning Probe**&**Multimodal LLM** |
| 25.07 | Carnegie Mellon University |                           ICCV 2025                            | [ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2507.00898v1) | **Hallucination**&**Vision-Language Model**&**Decoding** |
| 25.07 | Nanyang Technological University, Beihang University |                           ICML 2025                            | [ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks](https://arxiv.org/abs/2507.01321v1) | **In-Context Learning**&**Backdoor Attack**&**Defense** |
| 25.07 | The Chinese University of Hong Kong |                             arxiv                              | [Attention Slipping: A Mechanistic Understanding of Jailbreak Attacks and Defenses in LLMs](https://arxiv.org/abs/2507.04365v1) | **Jailbreak**&**Attention Mechanism**&**Defense** |
| 25.07 | University of Wisconsin-Madison |                             arxiv                              | [How Not to Detect Prompt Injections with an LLM](https://arxiv.org/abs/2507.05630v1) | **Prompt Injection**&**Detection**&**LLM** |
| 25.07 | Peking University |                             arxiv                              | [Enhancing LLM Watermark Resilience Against Both Scrubbing and Spoofing Attacks](https://arxiv.org/abs/2507.06274v1) | **Watermarking**&**Scrubbing Attack**&**Spoofing** |
| 25.07 | University of North Texas |                           COLM 2025                            | [Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation](https://arxiv.org/abs/2507.07307v1) | **Counterspeech**&**RAG**&**Multi-Agent** |
| 25.07 | University of California, Berkeley |                             arxiv                              | [Detection is Not Enough: Defending Against Jailbreaks with Restrictive Generation](https://arxiv.org/abs/2507.07735v1) | **Jailbreak Defense**&**Restrictive Decoding**&**LLM Alignment** |
| 25.07 | Beihang University |                             arxiv                              | [From Red to Green: Aligning Large Language Models via Jailbreak Sample Reuse](https://arxiv.org/abs/2507.07810v1) | **Jailbreak Defense**&**Alignment**&**Sample Reuse** |
| 25.07 | Tsinghua University |                             arxiv                              | [Ctrl-Z for AI Models: Stealthy Backdoor Removal from the Lens of Data Attribution](https://arxiv.org/abs/2507.07916v1) | **Backdoor Removal**&**Data Attribution**&**Stealth** |
| 25.07 | Stanford University |                             arxiv                              | [Beyond Prompt Injection: Jailbreaking via Adversarial Instruction Extraction](https://arxiv.org/abs/2507.07974v1) | **Jailbreaking**&**Instruction Extraction**&**LLM Security** |
| 25.05 | University of Southern California |                           ICML 2025                            | [LLM Auto Repair Shop: Repurposing Jailbreak Attacks for Improving LLM Safety](https://arxiv.org/abs/2505.06943) | **Jailbreaking**&**Safety Training**&**Repurposing** |
| 25.07 | Ant Group |                             arxiv                              | [Agent Safety Alignment via Reinforcement Learning](https://arxiv.org/abs/2507.08270) | **Agent**&**Safety**&**RL** |
| 25.07 | Apple |                             arxiv                              | [Lightweight Safety Guardrails via Synthetic Data and RL-guided Adversarial Training](https://arxiv.org/abs/2507.08284) | **Safety**&**Guardrail**&**Synthetic** |
| 25.07 | University of Science and Technology of China |                             arxiv                              | [BURN: Backdoor Unlearning via Adversarial Boundary Analysis](https://arxiv.org/abs/2507.10491v1) | **Backdoor**&**Unlearning**&**Defense** |
| 25.07 | University of Wisconsin-Madison |                             arxiv                              | [ARMOR: Aligning Secure and Safe Large Language Models via Meticulous Reasoning](https://arxiv.org/abs/2507.11500v1) | **LLM**&**Alignment**&**Jailbreak** |
| 25.07 | Brown University |                             arxiv                              | [Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models](https://arxiv.org/abs/2507.12428v1) | **Alignment**&**Monitoring**&**CoT** |
| 25.07 | Zhejiang University |                             arxiv                              | [Automating Steering for Safe Multimodal Large Language Models](https://arxiv.org/abs/2507.13255v1) | **MLLM**&**Safety**&**Steering** |
| 25.07 | University of Science and Technology of China |                             arxiv                              | [AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning](https://arxiv.org/abs/2507.14987v1) | **Safety Alignment**&**Reinforcement Learning**&**LLM** |
| 25.07 | Modulabs |                             arxiv                              | [SIA: Enhancing Safety via Intent Awareness for Vision-Language Models](https://arxiv.org/abs/2507.16856v1) | **Safety**&**Intent Awareness**&**VLM** |
| 25.07 | University of California, Los Angeles |                             arxiv                              | [LoRA is All You Need for Safety Alignment of Reasoning LLMs](https://arxiv.org/abs/2507.17075v1) | **Safety Alignment**&**LoRA**&**Reasoning** |
| 25.07 | Xidian University |                             arxiv                              | [LLM Meets the Sky: Heuristic Multi-Agent Reinforcement Learning for Secure Heterogeneous UAV Networks](https://arxiv.org/abs/2507.17188v1) | **UAV**&**Security**&**Reinforcement Learning** |
| 25.07 | Shanghai Artificial Intelligence Laboratory |                             arxiv                              | [Layer-Aware Representation Filtering: Purifying Finetuning Data to Preserve LLM Safety Alignment](https://arxiv.org/abs/2507.18631v1) | **Safety Alignment**&**Data Filtering**&**LLM** |
| 25.07 | POSTECH |                       ACL 2025 Findings                        | [Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token Probability Method for Poisoned Document Detection](https://arxiv.org/abs/2507.18202v1) | **RAG**&**Poisoning Defense**&**Detection** |
| 25.07 | Universiti Sains Malaysia |                        EAI ICDF2C 2025                         | [PhishIntentionLLM: Uncovering Phishing Website Intentions through Multi-Agent Retrieval-Augmented Generation](https://arxiv.org/abs/2507.15419v1) | **Phishing**&**LLM**&**RAG** |
| 25.07 | Shanghai Normal University |                             arxiv                              | [Self-Aware Safety Augmentation: Leveraging Internal Semantic Understanding to Enhance Safety in Vision-Language Models](https://arxiv.org/abs/2507.21637) | **Safety**&**LVLM**&**Projection** |
| 25.07 | Swarthmore College |                             arxiv                              | [Promoting Online Safety by Simulating Unsafe Conversations with LLMs](https://arxiv.org/abs/2507.22267) | **OnlineSafety**&**Simulation**&**LLM** |
| 25.07 | The University of Manchester |                             arxiv                              | [Adversarial Defence without Adversarial Defence: Enhancing Language Model Robustness via Instance-level Principal Component Removal](https://arxiv.org/abs/2507.21750) | **AdversarialRobustness**&**PCA**&**Isotropy** |
| 25.07 | Mohammed VI Polytechnic University |                             arxiv                              | [Strategic Deflection: Defending LLMs from Logit Manipulation](https://arxiv.org/abs/2507.22160v1) | **LogitManipulation**&**Defense**&**Deflection** |
| 25.07 | South China University of Technology |                            ACL 2025                            | [SDD: Self-Degraded Defense against Malicious Fine-tuning](https://arxiv.org/abs/2507.21182v1) | **MaliciousFinetuning**&**Defense**&**Safety** |
| 25.08 |                                         Fudan University, East China University of Science and Technology                                          |      arxiv      |      [ReasoningGuard: Safeguarding Large Reasoning Models with Inference-time Safety Aha Moments](https://arxiv.org/abs/2508.04204)      |      **Large Reasoning Models**&**Jailbreak Defense**&**Inference-Time Intervention** |
| 25.08 |                                         Amazon Web Services                                          |      EAI @ KDD      |      [Defend LLMs Through Self-Consciousness](https://arxiv.org/abs/2508.02961)      |      **Large Language Model**&**Prompt Injection**&**Self-Consciousness Defense**&**Black-Box Defense**&**Ethical Al** |
| 25.08 | KAIST | arxiv | [R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge](https://arxiv.org/abs/2508.00324) | **Large Reasoning Models**&**Safety Alignment**&**Knowledge Activation**|
| 25.08 | Politecnico di Milano, ML cube | arxiv | [LEAKSEALER: A SEMISUPERVISED DEFENSE FOR LLMS AGAINST PROMPT INJECTION AND LEAKAGE ATTACKS](https://arxiv.org/abs/2508.00602) | **Prompt Injection**&**Privacy**&**Data Leakage**&**Defense**&**Jailbreaking**&**Large Language Models**|
| 25.08 | University of Electronic Science and Technology of China, City University of Hong Kong | arxiv | [ConfGuard: A Simple and Effective Backdoor Detection for Large Language Models](https://arxiv.org/abs/2508.01365) | **Backdoor Detection**&**Large Language Models**&**Sequence Lock**&**Real-Time Defense**|
| 25.08 | Beijing Electronic Science and Technology Institute, Nanyang Technological University | arxiv | [DUP: Detection-guided Unlearning for Backdoor Purification in Language Models](https://arxiv.org/abs/2508.01647) | **Backdoor Defense**&**Language Models**&**Backdoor Detection**&**Unlearning**&**Purification**|
| 25.08 | NVIDIA | arxiv | [CultureGuard: Towards Culturally-Aware Dataset and Guard Model for Multilingual Safety Applications](https://arxiv.org/abs/2508.01710) | **Multilingual Safety**&**Culturally-Aware Datasets**&**Guard Models**&**Synthetic Data Generation**|
| 25.08 | Microsoft Security Response Center | arxiv | [Highlight & Summarize: RAG without the jailbreaks](https://arxiv.org/abs/2508.02872) | **Retrieval-Augmented Generation (RAG)**&**Jailbreaking**&**Model Hijacking**&**System Design**&**LLM Security**|
| 25.08 | Zhejiang University, Xiamen University, Shanghai Jiao Tong University | arxiv | [HARMONYGUARD: TOWARD SAFETY AND UTILITY IN WEB AGENTS VIA ADAPTIVE POLICY ENHANCEMENT AND DUAL-OBJECTIVE OPTIMIZATION](https://arxiv.org/abs/2508.04010) | **Web Agents**&**LLM Safety**&**Dual-Objective Optimization**&**Adaptive Policy**&**Multi-Agent Systems**|
| 25.08 | IBM | arxiv | [A Framework for Inherently Safer AGI through Language-Mediated Active Inference](https://arxiv.org/abs/2508.05766v1) | **Active Inference**&**AGI Safety**&**Large Language Models** |
| 25.08 | University of Bonn | arxiv | [In-Training Defenses against Emergent Misalignment in Language Models](https://arxiv.org/abs/2508.06249v1) | **Emergent Misalignment**&**Fine-Tuning Safety**&**Regularization** |
| 25.08 | EleutherAI | arxiv | [Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs](https://arxiv.org/abs/2508.06601v1) | **Open-Weight LLMs**&**Tamper-Resistance**&**Data Filtering** |
| 25.08 | Nankai University | arxiv | [SafeGrad: Gradient Surgery for Safe LLM Fine-Tuning](https://arxiv.org/abs/2508.07172v1) | **Safe Fine-Tuning**&**Gradient Surgery**&**LLM Safety** |
| 25.08 | Jilin University | arxiv | [BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks](https://arxiv.org/abs/2508.08127v1) | **Multi-Agent Systems Security**&**Unsupervised Defense**&**Graph Anomaly Detection** |
| 25.08 | Chinese Academy of Sciences | arxiv | [Fine-Grained Safety Neurons with Training-Free Continual Projection to Reduce LLM Fine Tuning Risks](https://arxiv.org/abs/2508.09190v1) | **Fine-Grained Safety Neurons**&**Continual Projection**&**LLM Fine-Tuning Risks** |
| 25.08 | Renmin University of China | arxiv | [Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models: A Unified and Accurate Approach](https://arxiv.org/abs/2508.09201v1) | **Jailbreak Detection**&**Vision-Language Models**&**Anomaly Detection** |
| 25.08 | OpenAI | arxiv | [From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training](https://arxiv.org/abs/2508.09224v1) | **Safe-Completions**&**Output-Centric Safety**&**Dual-Use Prompts** |
| 25.08 | Wuhan University | arxiv | [NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs](https://arxiv.org/abs/2508.09473v1) | **Safety-Utility Alignment**&**Neuron Modulation**&**Meta-Learning** |
| 25.08 | Southeast University | arxiv | [Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought Distillation](https://arxiv.org/abs/2508.09666v1) | **Chain-of-Thought Distillation**&**Safety Alignment**&**Small Language Models** |
| 25.08 | University of California, Irvine | arxiv | [Context Misleads LLMs: The Role of Context Filtering in Maintaining Safe Alignment of LLMs](https://arxiv.org/abs/2508.10031v1) | **Context Filtering**&**Jailbreak Defense**&**Safety-Helpfulness Trade-off** |
| 25.08 | Macquarie University | arxiv | [SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through Task-Specific Trajectory](https://arxiv.org/abs/2508.11290v1) | **LLM Safety**&**Over-Refusal Mitigation**&**Representation Steering** |
| 25.08 | National University of Singapore | arxiv | [SafeSieve: From Heuristics to Experience in Progressive Pruning for LLM-based Multi-Agent Communication](https://arxiv.org/abs/2508.11733v1) | **Multi-Agent Systems**&**Progressive Pruning**&**LLM Communication Efficiency** |
| 25.08 | Nanyang Technological University | arxiv | [Mitigating Jailbreaks with Intent-Aware LLMs](https://arxiv.org/abs/2508.12072v1) | **LLM Safety**&**Jailbreak Defense**&**Intent-Aware Fine-Tuning** |
| 25.08 | Microsoft Research | COLM 2025 | [Rethinking Safety in LLM Fine-tuning: An Optimization Perspective](https://arxiv.org/abs/2508.12531v1) | **LLM Safety**&**Fine-tuning Optimization**&**Exponential Moving Average** |
| 25.08 | University College London | arxiv | [CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection](https://arxiv.org/abs/2508.12535v1) | **Sparse Autoencoders**&**LLM Steering**&**Correlation-based Feature Selection** |
| 25.08 | Wuhan University | arxiv | [FuSaR: A Fuzzification-Based Method for LRM Safety-Reasoning Balance](https://arxiv.org/abs/2508.12897v1) | **Large Reasoning Models**&**Safety-Reasoning Trade-off**&**Fuzzification Alignment** |
| 25.08 | Zhejiang University, Antgroup, Quantstamp | arxiv | [ORFUZZ: Fuzzing the "Other Side" of LLM Safety](https://arxiv.org/abs/2508.11222) | **Over-Refusal**&**Fuzzing**&**LLM Safety** |
| 25.08 | Qiyuan Tech | arxiv | [EFFICIENT SWITCHABLE | 25.08 | Nanjing University, Meituan Inc., Dalian University of Technology | arxiv | [SDGO: Self-Discrimination-Guided Optimization for Consistent Safety in Large Language Models](https://arxiv.org/abs/2508.15648) | **LLM Safety**&**Reinforcement Learning**&**Self-Improvement** |SAFETY CONTROL IN LLMS VIA MAGIC-TOKEN-GUIDED CO-TRAINING](https://arxiv.org/abs/2508.14904) | **LLM Safety**&**Controllable Behavior**&**Magic Token** |
| 25.08 | University of Southampton | arxiv | [S¬≥LORA: Safe Spectral Sharpness-Guided Pruning in Adaptation of Agent Planner](https://arxiv.org/abs/2508.15068) | **LLM Agent Safety**&**LoRA**&**Model Pruning** |
| 25.08 | Nanjing University, Meituan Inc., Dalian University of Technology | arxiv | [SDGO: Self-Discrimination-Guided Optimization for Consistent Safety in Large Language Models](https://arxiv.org/abs/2508.15648) | **LLM Safety**&**Reinforcement Learning**&**Self-Improvement** |
| 25.08 | Zhejiang University, University of California, Los Angeles, Westlake University | arxiv | [IPIGUARD: A Novel Tool Dependency Graph-Based Defense Against Indirect Prompt Injection in LLM Agents](https://arxiv.org/abs/2508.15310) | **LLM Agents**&**Indirect Prompt Injection**&**Cybersecurity** |




## üíªPresentations & Talk


## üìñTutorials & Workshops

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |

## üì∞News & Articles

## üßë‚Äçüè´Scholars
