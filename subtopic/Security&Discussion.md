# Security

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating with the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"

## üìëPapers

| Date  |                                                                                                                      Institute                                                                                                                       |                     Publication                     |                                                                                                                                                                                Paper                                                                                                                                                                                 |                                             Keywords                                              |
|:-----:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------:|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------------------:|
| 20.10 |                                                                                                                 Facebook AI Research                                                                                                                 |                        arxiv                        |                                                                                                                                            [Recipes for Safety in Open-domain Chatbots](https://arxiv.org/abs/2010.07079)                                                                                                                                            |                                **Toxic Behavior**&**Open-domain**                                 |
| 22.02 |                                                                                                                       DeepMind                                                                                                                       |                      EMNLP2022                      |                                                                                                                                   [Red Teaming Language Models with Language Model](https://aclanthology.org/2022.emnlp-main.225/)                                                                                                                                   |                                   **Red Teaming**&**Harm Test**                                   |
| 22.03 |                                                                                                                        OpenAI                                                                                                                        |                      NIPS2022                       |                                                                                     [Training language models to follow instructions with human feedback](https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html)                                                                                      |                               **InstructGPT**&**RLHF**&**Harmless**                               |
| 22.04 |                                                                                                                      Anthropic                                                                                                                       |                        arxiv                        |                                                                                                                    [Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2204.05862)                                                                                                                     |                                     **Helpful**&**Harmless**                                      |
| 22.05 |                                                                                                                         UCSD                                                                                                                         |                      EMNLP2022                      |                                                                                                                 [An Empirical Analysis of Memorization in Fine-tuned Autoregressive Language Models](https://aclanthology.org/2022.emnlp-main.119/)                                                                                                                  |                                **Privacy Risks**&**Memorization**                                 |
| 22.09 |                                                                                                                      Anthropic                                                                                                                       |                        arxiv                        |                                                                                                                   [Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned](https://arxiv.org/abs/2209.07858)                                                                                                                   |                             **Red Teaming**&**Harmless**&**Helpful**                              |
| 22.12 |                                                                                                                      Anthropic                                                                                                                       |                        arxiv                        |                                                                                                                                         [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073)                                                                                                                                         |                            **Harmless**&**Self-improvement**&**RLAIF**                            |
| 23.07 |                                                                                                                     UC Berkeley                                                                                                                      |                      NIPS2023                       |                                                                                                                                          [Jailbroken: How Does LLM Safety Training Fail?](https://arxiv.org/abs/2307.02483)                                                                                                                                          |               **Jailbreak**&**Competing Objectives**&**Mismatched Generalization**                |
| 23.08 |                                                                       The Chinese University of Hong Kong Shenzhen China, Tencent AI Lab, The Chinese University of Hong Kong                                                                        |                        arxiv                        |                                                                                                                                [GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs Via Cipher](https://arxiv.org/abs/2308.06463)                                                                                                                                 |                            **Safety Alignment**&**Adversarial Attack**                            |
| 23.08 |                                                                                       University College London, University College London, Tilburg University                                                                                       |                        arxiv                        |                                                                                                                       [Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities](https://arxiv.org/abs/2308.12833)                                                                                                                        |                                   **Security**&**AI Alignment**                                   |
| 23.09 |                                                                                                                  Peking University                                                                                                                   |                        arxiv                        |                                                                                                                                [RAIN: Your Language Models Can Align Themselves without Finetuning](https://arxiv.org/abs/2309.07124)                                                                                                                                |                              **Self-boosting**&**Rewind Mechanisms**                              |
| 23.10 |                                                                                        Princeton University, Virginia Tech, IBM Research, Stanford University                                                                                        |                        arxiv                        |                                                                                                                     [FINE-TUNING ALIGNED LANGUAGE MODELS COMPROMISES SAFETY EVEN WHEN USERS DO NOT INTEND TO!](https://arxiv.org/abs/2310.03693)                                                                                                                     |                     **Fine-tuning****Safety Risks**&**Adversarial Training**                      |
| 23.10 |                                                                                                                     UC Riverside                                                                                                                     |                        arXiv                        |                                                                                                                        [Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks](https://arxiv.org/abs/2310.10844)                                                                                                                        |                  **Adversarial Attacks**&**Vulnerabilities**&**Model Security**                   |
| 23.10 |                                                                                                                   Rice University                                                                                                                    |                 NAACL2024(findings)                 | [Secure Your Model: An Effective Key Prompt Protection Mechanism for Large Language Models](https://www.researchgate.net/publication/374555007_Secure_Your_Model_An_Effective_Key_Prompt_Protection_Mechanism_for_Large_Language_Models/link/65f8a03b286738732d5ce0d3/download?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6InB1YmxpY2F0aW9uIiwicGFnZSI6InB1YmxpY2F0aW9uIn19) |      **Key Prompt Protection**&**Large Language Models**&**Unauthorized Access Prevention**       |
| 23.11 |                                                                                                                       KAIST AI                                                                                                                       |                        arxiv                        |                                                                                                                               [HARE: Explainable Hate Speech Detection with Step-by-Step Reasoning](https://arxiv.org/abs/2311.00321)                                                                                                                                |                                   **Hate Speech**&**Detection**                                   |
| 23.11 |                                                                                                                         CMU                                                                                                                          |          AACL2023(ART or Safety workshop)           |                                                                                                                                                  [Measuring Adversarial Datasets](https://arxiv.org/abs/2311.03566)                                                                                                                                                  |                 **Adversarial Robustness**&**AI Safety**&**Adversarial Datasets**                 |
| 23.11 |                                                                                                                         UIUC                                                                                                                         |                        arxiv                        |                                                                                                                                        [Removing RLHF Protections in GPT-4 via Fine-Tuning](https://arxiv.org/abs/2311.05553)                                                                                                                                        |                               **Remove Protection**&**Fine-Tuning**                               |
| 23.11 |                                                                                                 IT University of CopenhagenÔºåUniversity of Washington                                                                                                 |                        arxiv                        |                                                                                                                           [Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild](https://arxiv.org/abs/2311.06237)                                                                                                                           |                                          **Red Teaming**                                          |
| 23.11 |                                                                                                           Fudan University&Shanghai AI lab                                                                                                           |                        arxiv                        |                                                                                                                                          [Fake Alignment: Are LLMs Really Aligned Well?](https://arxiv.org/abs/2311.05915)                                                                                                                                           |                            **Alignment Failure**&**Safety Evaluation**                            |
| 23.11 |                                                                                                          University of Southern California                                                                                                           |                        arxiv                        |                                                                                                                             [SAFER-INSTRUCT: Aligning Language Models with Automated Preference Data](https://arxiv.org/abs/2311.08685)                                                                                                                              |                                        **RLHF**&**Safety**                                        |
| 23.11 |                                                                                                                   Google Research                                                                                                                    |                        arxiv                        |                                                                                                                   [AART: AI-Assisted Red-Teaming with Diverse Data Generation for New LLM-powered Applications](https://arxiv.org/abs/2311.08592)                                                                                                                    |            **Adversarial Testing**&**AI-Assisted Red Teaming**&**Application Safety**             |
| 23.11 |                                                                                                                    Tencent AI Lab                                                                                                                    |                        arxiv                        |                                                                                                                                               [ADVERSARIAL PREFERENCE OPTIMIZATION](https://arxiv.org/abs/2311.08045)                                                                                                                                                |  **Human Preference Alignment**&**Adversarial Preference Optimization**&**Annotation Reduction**  |
| 23.11 |                                                                                                                       Docta.ai                                                                                                                       |                        arxiv                        |                                                                                                              [Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models](https://arxiv.org/abs/2311.11202)                                                                                                               |                             **Data Credibility**&**Safety alignment**                             |
| 23.11 |                                                                                                                 CIIRC CTU in Prague                                                                                                                  |                        arxiv                        |                                                                                                                                        [A Security Risk Taxonomy for Large Language Models](https://arxiv.org/abs/2311.11415)                                                                                                                                        |                     **Security risks**&**Taxonomy**&**Prompt-based attacks**                      |
| 23.11 |                                                                                                     Meta&University of Illinois Urbana-Champaign                                                                                                     |                      NAACL2024                      |                                                                                                                                [MART: Improving LLM Safety with Multi-round Automatic Red-Teaming](https://arxiv.org/abs/2311.07689)                                                                                                                                 |              **Automatic Red-Teaming**&**LLM Safety**&**Adversarial Prompt Writing**              |
| 23.11 |                                                                                              The Ohio State University&University of California, Davis                                                                                               |                      NAACL2024                      |                                                                                                          [How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities](https://arxiv.org/abs/2311.09447)                                                                                                          |               **Open-Source LLMs**&**Malicious Demonstrations**&**Trustworthiness**               |
| 23.12 |                                                                                                                  Drexel University                                                                                                                   |                        arXiv                        |                                                                                                                    [A Survey on Large Language Model (LLM) Security and Privacy: The Good the Bad and the Ugly](https://arxiv.org/abs/2312.02003)                                                                                                                    |                               **Security**&**Privacy**&**Attacks**                                |
| 23.12 |                                                                                                                        Tenyx                                                                                                                         |                        arXiv                        |                                                                                                                      [Characterizing Large Language Model Geometry Solves Toxicity Detection and Generation](https://arxiv.org/abs/2312.01648)                                                                                                                       |            **Geometric Interpretation**&**Intrinsic Dimension**&**Toxicity Detection**            |
| 23.12 |                                                                                                         Independent (Now at Google DeepMind)                                                                                                         |                        arXiv                        |                                                                                                                                [Scaling Laws for Adversarial Attacks on Language Model Activations](https://arxiv.org/abs/2312.02780)                                                                                                                                |              **Adversarial Attacks**&**Language Model Activations**&**Scaling Laws**              |
| 23.12 |                                                                                                University of Liechtenstein, University of Duesseldorf                                                                                                |                        arxiv                        |                                                                                                                             [NEGOTIATING WITH LLMS: PROMPT HACKS, SKILL GAPS, AND REASONING DEFICITS](https://arxiv.org/abs/2312.03720)                                                                                                                              |                         **Negotiation**&**Reasoning**&**Prompt Hacking**                          |
| 23.12 |                                                                            University of Wisconsin Madison, University of Michigan Ann Arbor, ASU, Washington University                                                                             |                        arXiv                        |                                                                                                                                [Exploring the Limits of ChatGPT in Software Security Applications](https://arxiv.org/abs/2312.05275)                                                                                                                                 |                                         Software Security                                         |
| 23.12 |                                                                                                                    GenAI at Meta                                                                                                                     |                        arxiv                        |                                                                                                                             [Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations](https://arxiv.org/abs/2312.06674)                                                                                                                             |                            Human-AI Conversation&Safety Risk taxonomy                             |
| 23.12 |                                                                                                    University of California Riverside, Microsoft                                                                                                     |                        arxiv                        |                                                                                                                       [Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack](https://arxiv.org/abs/2312.06924)                                                                                                                        |                           Safety Alignment&Summarization&Vulnerability                            |
| 23.12 |                                                                                                                     MIT, Harvard                                                                                                                     |                 NIPS2023(Workshop)                  |                                                                                                                               [Forbidden Facts: An Investigation of Competing Objectives in Llama-2](https://arxiv.org/abs/2312.08793)                                                                                                                               |             **Competing Objectives**&**Forbidden Fact Task**&**Model Decomposition**              |
| 23.12 |                                                                                                    University of Science and Technology of China                                                                                                     |                        arxiv                        |                                                                                                                      [Silent Guardian: Protecting Text from Malicious Exploitation by Large Language Models](https://arxiv.org/abs/2312.09669)                                                                                                                       |                              **Text Protection**&**Silent Guardian**                              |
| 23.12 |                                                                                                                        OpenAI                                                                                                                        |                       Open AI                       |                                                                                                                      [Practices for Governing Agentic AI Systems](https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf)                                                                                                                      |                             **Agentic AI Systems**&**LM Based Agent**                             |
| 23.12 |                                                                      University of Massachusetts Amherst, Columbia University, Google, Stanford University, New York University                                                                      |                        arxiv                        |                                                                                                                                 [Learning and Forgetting Unsafe Examples in Large Language Models](https://arxiv.org/abs/2312.12736)                                                                                                                                 |                  **Safety Issues**&**ForgetFilter Algorithm**&**Unsafe Content**                  |
| 23.12 |                                                                                                 Tencent AI Lab, The Chinese University of Hong Kong                                                                                                  |                        arxiv                        |                                                                                                                                             [Aligning Language Models with Judgments](https://arxiv.org/abs/2312.14591)                                                                                                                                              |                   **Judgment Alignment**&**Contrastive Unlikelihood Training**                    |
| 24.01 |                                                                                                            Delft University of Technology                                                                                                            |                        arxiv                        |                                                                                                                   [Red Teaming for Large Language Models At Scale: Tackling Hallucinations on Mathematics Tasks](https://arxiv.org/abs/2401.00290)                                                                                                                   |                     **Red Teaming**&**Hallucinations**&**Mathematics Tasks**                      |
| 24.01 |                                                                                Apart Research, University of Edinburgh, Imperial College London, University of Oxford                                                                                |                        arxiv                        |                                                                                                                                          [Large Language Models Relearn Removed Concepts](https://arxiv.org/abs/2401.01814)                                                                                                                                          |                          **Neuroplasticity**&**Concept Redistribution**                           |
| 24.01 |                                Tsinghua University, Xiaomi AI Lab, Huawei, Shenzhen Heytap Technology, vivo AI Lab, Viomi Technology, Li Auto, Beijing University of Posts and Telecommunications, Soochow University                                |                        arxiv                        |                                                                                                                      [PERSONAL LLM AGENTS: INSIGHTS AND SURVEY ABOUT THE CAPABILITY EFFICIENCY AND SECURITY](https://arxiv.org/abs/2401.05459)                                                                                                                       |             **Intelligent Personal Assistant**&**LLM Agent**&**Security and Privacy**             |
| 24.01 |                                                              Zhongguancun Laboratory, Tsinghua University, Institute of Information Engineering Chinese Academy of Sciences, Ant Group                                                               |                        arxiv                        |                                                                                                                        [Risk Taxonomy Mitigation and Assessment Benchmarks of Large Language Model Systems](https://arxiv.org/abs/2401.05778)                                                                                                                        |                      **Safety**&**Risk Taxonomy**&**Mitigation Strategies**                       |
| 24.01 |                                                                                                                   Google Research                                                                                                                    |                        arxiv                        |                                                                                                                    [Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models](https://arxiv.org/abs/2401.06102)                                                                                                                    |                                       **Interpretability**                                        |
| 24.01 |                                                                                                      Ben-Gurion University of the Negev Israel                                                                                                       |                        arxiv                        |                                                                                                                                       [GPT IN SHEEP‚ÄôS CLOTHING: THE RISK OF CUSTOMIZED GPTS](https://arxiv.org/abs/2401.09075)                                                                                                                                       |                              **GPTs**&**Cybersecurity**&**ChatGPT**                               |
| 24.01 |                                                                                                            Shanghai Jiao Tong University                                                                                                             |                        arxiv                        |                                                                                                                                    [R-Judge: Benchmarking Safety Risk Awareness for LLM Agents](https://arxiv.org/abs/2401.10019)                                                                                                                                    |                      **LLM Agents**&**Safety Risk Awareness**&**Benchmark**                       |
| 24.01 |                                                                                                                      Ant Group                                                                                                                       |                        arxiv                        |                                                                                                                                 [A FAST PERFORMANT SECURE DISTRIBUTED TRAINING FRAMEWORK FOR LLM](https://arxiv.org/abs/2401.09796)                                                                                                                                  |                                 **Distributed LLM**&**Security**                                  |
| 24.01 |                                                             Shanghai Artificial Intelligence Laboratory, Dalian University of Technology, University of Science and Technology of China                                                              |                        arxiv                        |                                                                                                      [PsySafe: A Comprehensive Framework for Psychological-based Attack Defense and Evaluation of Multi-agent System Safety](https://arxiv.org/abs/2401.11880)                                                                                                       |                      **Multi-agent Systems**&**Agent Psychology**&**Safety**                      |
| 24.01 |                                                                                                          Rochester Institute of Technology                                                                                                           |                        arxiv                        |                                                                                                                                               [Mitigating Security Threats in LLMs](https://arxiv.org/abs/2401.12273)                                                                                                                                                |                    **Security Threats**&**Prompt Injection**&**Jailbreaking**                     |
| 24.01 |                                                                                     Johns Hopkins University, University of Pennsylvania, Ohio State University                                                                                      |                        arxiv                        |                                                                                                                       [The Language Barrier: Dissecting Safety Challenges of LLMs in Multilingual Contexts](https://arxiv.org/abs/2401.13136)                                                                                                                        |                       **Multilingualism**&**Safety**&**Resource Disparity**                       |
| 24.01 |                                                                                                                University of Florida                                                                                                                 |                        arxiv                        |                                                                                                                                        [Adaptive Text Watermark for Large Language Models](https://arxiv.org/abs/2401.13927)                                                                                                                                         |                         **Text Watermarking**&**Robustness**&**Security**                         |
| 24.01 |                                                                                                                The Hebrew University                                                                                                                 |                        arXiv                        |                                                                                                                                  [TRADEOFFS BETWEEN ALIGNMENT AND HELPFULNESS IN LANGUAGE MODELS](https://arxiv.org/abs/2401.16332)                                                                                                                                  |             **Language Model Alignment**&**AI Safety**&**Representation Engineering**             |
| 24.01 |                                                                                                              Google ResearchÔºå Anthropic                                                                                                              |                        arxiv                        |                                                                                                                                            [Gradient-Based Language Model Red Teaming](https://arxiv.org/abs/2401.16656)                                                                                                                                             |                          **Red Teaming**&**Safety**&**Prompt Learning**                           |
| 24.01 |                                                                                           National University of SingaporeÔºå Pennsylvania State University                                                                                            |                        arxiv                        |                                                                                                                      [Provably Robust Multi-bit Watermarking for AI-generated Text via Error Correction Code](https://arxiv.org/abs/2401.16820)                                                                                                                      |                     **Watermarking**&**Error Correction Code**&**AI Ethics**                      |
| 24.01 |                                                                                  Tsinghua University, University of California Los Angeles, WeChat AI Tencent Inc.                                                                                   |                        arxiv                        |                                                                                                                             [Prompt-Driven LLM Safeguarding via Directed Representation Optimization](https://arxiv.org/abs/2401.18018)                                                                                                                              |                        **Safety Prompts**&**Representation Optimization**                         |
| 24.02 |                                                                                   Rensselaer Polytechnic Institute, IBM T.J. Watson Research Center, IBM Research                                                                                    |                        arxiv                        |                                                                                                                                   [Adaptive Primal-Dual Method for Safe Reinforcement Learning](https://arxiv.org/abs/2402.00355)                                                                                                                                    |       **Safe Reinforcement Learning**&**Adaptive Primal-Dual**&**Adaptive Learning Rates**        |
| 24.02 |                                                      Jagiellonian University, University of Modena and Reggio Emilia, Alma Mater Studiorum University of Bologna, European University Institute                                                      |                        arxiv                        |                                                                                                                                  [No More Trade-Offs: GPT and Fully Informative Privacy Policies](https://arxiv.org/abs/2402.00013)                                                                                                                                  |                      **ChatGPT**&**Privacy Policies**&**Legal Requirements**                      |
| 24.02 |                                                                                                           Florida International University                                                                                                           |                        arxiv                        |                                                                                                                                [Security and Privacy Challenges of Large Language Models: A Survey](https://arxiv.org/abs/2402.00888)                                                                                                                                |                          **Security**&**Privacy Challenges**&**Suevey**                           |
| 24.02 |                                                                                    Rutgers University, University of California, Santa Barbara, NEC Labs America                                                                                     |                        arxiv                        |                                                                                                                       [TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution](https://arxiv.org/abs/2402.01586)                                                                                                                       |                        **LLM-based Agents**&**Safety**&**Trustworthiness**                        |
| 24.02 |                                                                        University of Maryland College Park, JPMorgan AI Research, University of Waterloo, Salesforce Research                                                                        |                        arxiv                        |                                                                                                                                     [Shadowcast: Stealthy Data Poisoning Attacks against VLMs](https://arxiv.org/abs/2402.06659)                                                                                                                                     |                    **Vision-Language Models**&**Data Poisoning**&**Security**                     |
| 24.02 |                                                    Shanghai Artificial Intelligence Laboratory, Harbin Institute of Technology, Beijing Institute of Technology, Chinese University of Hong Kong                                                     |                        arxiv                        |                                                                                                                     [SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models](https://arxiv.org/abs/2402.05044)                                                                                                                     |                **Safety Benchmark**&Safety Evaluation**&**Hierarchical Taxonomy**                 |
| 24.02 |                                                                                                                   Fudan University                                                                                                                   |                        arxiv                        |                                                                                                                 [ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages](https://arxiv.org/abs/2402.10753)                                                                                                                 |        **Tool Learning**&**Large Language Models (LLMs)**&**Safety Issues**&**ToolSword**         |
| 24.02 |                                                                                   Paul G. Allen School of Computer Science & Engineering, University of Washington                                                                                   |                        arxiv                        |                                                                                                                                 [SPML: A DSL for Defending Language Models Against Prompt Attacks](https://arxiv.org/abs/2402.11755)                                                                                                                                 | **Domain-Specific Language (DSL)**&**Chatbot Definitions**&**System Prompt Meta Language (SPML)** |
| 24.02 |                                                                                                                 Tsinghua University                                                                                                                  |                        arxiv                        |                                                                                                                        [ShieldLM: Empowering LLMs as Aligned Customizable and Explainable Safety Detectors](https://arxiv.org/abs/2402.16444)                                                                                                                        |                       **Safety Detectors**&**Customizable**&**Explainable**                       |
| 24.02 |                                                                                                                 Dalhousie University                                                                                                                 |                        arxiv                        |                                                                                                                                         [Immunization Against Harmful Fine-tuning Attacks](https://arxiv.org/abs/2402.16382)                                                                                                                                         |                             **Fine-tuning Attacks**&**Immunization**                              |
| 24.02 |                                                                                Chinese Academy of Sciences, University of Chinese Academy of Sciences, Alibaba Group                                                                                 |                        arxiv                        |                                                                                                                                 [SoFA: Shielded On-the-fly Alignment via Priority Rule Following](https://arxiv.org/abs/2402.17358)                                                                                                                                  |                             **Priority Rule Following**&**Alignment**                             |
| 24.02 |                                                                                                        Universidade Federal de Santa Catarina                                                                                                        |                        arxiv                        |                                                                                                                                        [A Survey of Large Language Models in Cybersecurity](https://arxiv.org/abs/2402.16968)                                                                                                                                        |                          **Cybersecurity**&**Vulnerability Assessment**                           |
| 24.02 |                                                                                                                 Zhejiang University                                                                                                                  |                        arxiv                        |                                                                                                                               [PRSA: Prompt Reverse Stealing Attacks against Large Language Models](https://arxiv.org/abs/2402.19200)                                                                                                                                |                         **Prompt Reverse Stealing Attacks**&**Security**                          |
| 24.02 |                                                                                                     Shanghai Artificial Intelligence Laboratory                                                                                                      |                      NAACL2024                      |                                                                                                                             [Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey](https://arxiv.org/abs/2402.09283)                                                                                                                              |                   **Large Language Models**&**Conversation Safety**&**Survey**                    |
| 24.03 |                                                                                                                  Tulane University                                                                                                                   |                        arxiv                        |                                                                                                                               [ENHANCING LLM SAFETY VIA CONSTRAINED DIRECT PREFERENCE OPTIMIZATION](https://arxiv.org/abs/2403.02475)                                                                                                                                |               **Reinforcement Learning**&**Human Feedback**&**Safety Constraints**                |
| 24.03 |                                                                                                       University of Illinois Urbana-Champaign                                                                                                        |                        arxiv                        |                                                                                                                [INJECAGENT: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents](https://arxiv.org/abs/2403.02691)                                                                                                                |                  **Tool Integration**&**Security**&**Indirect Prompt Injection**                  |
| 24.03 |                                                                                                                  Harvard University                                                                                                                  |                        arxiv                        |                                                                                                                                   [Towards Safe and Aligned Large Language Models for Medicine](https://arxiv.org/abs/2403.03744)                                                                                                                                    |                     ***Medical Safety**&**Alignment**&**Ethical Principles**                      |
| 24.03 |                                                                            Rensselaer Polytechnic Institute, University of Michigan, IBM Research, MIT-IBM Watson AI Lab                                                                             |                        arxiv                        |                                                                                                                                             [ALIGNERS: DECOUPLING LLMS AND ALIGNMENT](https://arxiv.org/abs/2403.04224)                                                                                                                                              |                                 **Alignment**&**Synthetic Data**                                  |
| 24.03 | MIT, Princeton University, Stanford University, Georgetown University, AI Risk and Vulnerability Alliance, Eleuther AI, Brown University, Carnegie Mellon University, Virginia Tech, Northeastern University, UCSB, University of Pennsylvania, UIUC |                        arxiv                        |                                                                                                                                         [A Safe Harbor for AI Evaluation and Red Teaming](https://arxiv.org/abs/2403.04893)                                                                                                                                          |                         **AI Evaluation**&**Red Teaming**&**Safe Harbor**                         |
| 24.03 |                                                                                                          University of Southern California                                                                                                           |                        arxiv                        |                                                                                                                                    [Logits of API-Protected LLMs Leak Proprietary Information](https://arxiv.org/abs/2403.09539)                                                                                                                                     |            **API-Protected LLMs**&**Softmax Bottleneck**&**Embedding Size Detection**             |
| 24.03 |                                                                                                                University of Bristol                                                                                                                 |                        arxiv                        |                                                                                                                [Helpful or Harmful? Exploring the Efficacy of Large Language Models for Online Grooming Prevention](https://arxiv.org/abs/2403.09795)                                                                                                                |                                 **Safety**&**Prompt Engineering**                                 |
| 24.03 |                                                                 XiaMen University, Yanshan University, IDEA Research, Inner Mongolia University, Microsoft, Microsoft Research Asia                                                                  |                        arxiv                        |                                                                                                                     [Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models](https://arxiv.org/abs/2403.11838)                                                                                                                     |                              **Safety**&**Guidelines**&**Alignment**                              |
| 24.03 |                                                               Tianjin University, Tianjin University, Zhengzhou University, China Academy of Information and Communications Technology                                                               |                        arxiv                        |                                                                                                                           [OpenEval: Benchmarking Chinese LLMs across Capability, Alignment, and Safety](https://arxiv.org/abs/2403.12316)                                                                                                                           |                           **Chinese LLMs**&**Benchmarking**&**Safety**                            |
| 24.03 |                                                                      Center for Cybersecurity Systems and Networks, AIShield Bosch Global Software Technologies Bengaluru India                                                                      |                        arxiv                        |                                                                                                                      [Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal](https://arxiv.org/abs/2403.13309)                                                                                                                       |                     **LLM Security**&**Threat modeling**&**Risk Assessment**                      |
| 24.03 |                                                                                                              Queen‚Äôs University Belfast                                                                                                              |                        arxiv                        |                                                                                                                                  [AI Safety: Necessary but insufficient and possibly problematic](https://arxiv.org/abs/2403.17419)                                                                                                                                  |                        **AI Safety**&**Transparency**&**Structural Harm**                         |
| 24.04 |                                                                      Provable Responsible AI and Data Analytics (PRADA) Lab, King Abdullah University of Science and Technology                                                                      |                        arxiv                        |                                                                                                                         [Dialectical Alignment: Resolving the Tension of 3H and Security Threats of LLMs](https://arxiv.org/abs/2404.00486)                                                                                                                          |                  **Dialectical Alignment**&**3H Principle**&**Security Threats**                  |
| 24.04 |                                                                 LibrAI, Tsinghua University, Harbin Institute of Technology, Monash University, The University of Melbourne, MBZUAI                                                                  |                        arxiv                        |                                                                                                                            [Against The Achilles‚Äô Heel: A Survey on Red Teaming for Generative Models](https://arxiv.org/abs/2404.00629)                                                                                                                             |                                    **Red Teaming**&**Safety**                                     |
| 24.04 |                                                                                                   University of California, Santa Barbara, Meta AI                                                                                                   |                        arxiv                        |                                                                                                                     [Towards Safety and Helpfulness Balanced Responses via Controllable Large Language Models](https://arxiv.org/abs/2404.01295)                                                                                                                     |                          **Safety**&**Helpfulness**&**Controllability**                           |
| 24.04 |                                                                       School of Information and Software Engineering, University of Electronic Science and Technology of China                                                                       |                        arxiv                        |                                                                                                                                        [Exploring Backdoor Vulnerabilities of Chat Models](https://arxiv.org/abs/2404.02406)                                                                                                                                         |                         **Backdoor Attacks**&**Chat Models**&**Security**                         |
| 24.04 |                                                                                                                      Enkrypt AI                                                                                                                      |                        arxiv                        |                                                                                                                                 [INCREASED LLM VULNERABILITIES FROM FINE-TUNING AND QUANTIZATION](https://arxiv.org/abs/2404.04392)                                                                                                                                  |                     **Fine-tuning**&**Quantization**&**LLM Vulnerabilities**                      |
| 24.04 |                                                          TongJi University, Tsinghua University&, eijing University of Technology, Nanyang Technological University, Peng Cheng Laboratory                                                           |                        arxiv                        |                                                                                                          [Unbridled Icarus: A Survey of the Potential Perils of Image Inputs in Multimodal Large Language Model Security](https://arxiv.org/abs/2404.05264)                                                                                                          |        **Multimodal Large Language Models**&**Security Vulnerabilities**&**Image Inputs**         |
| 24.04 |                                                                    University of Washington, Carnegie Mellon University, University of British Columbia, Vector Institute for AI                                                                     |                        arxiv                        |                                                                                                           [CulturalTeaming: AI-Assisted Interactive Red-Teaming for Challenging LLMs‚Äô (Lack of) Multicultural Knowledge](https://arxiv.org/abs/2404.06664)                                                                                                           |                      **AI-Assisted Red-Teaming**&**Multicultural Knowledge**                      |
| 24.04 |                                                                                                                  Nanjing University                                                                                                                  |                      DLSP 2024                      |                                                                                                                      [Subtoxic Questions: Dive Into Attitude Change of LLM‚Äôs Response in Jailbreak Attempts](https://arxiv.org/abs/2404.08309)                                                                                                                       |                        **Jailbreak**&**Subtoxic Questions**&**GAC Model**                         |
| 24.04 |                                                                                                                       Innodata                                                                                                                       |                        arxiv                        |                                                                                                          [Benchmarking Llama2, Mistral, Gemma, and GPT for Factuality, Toxicity, Bias, and Propensity for Hallucinations](https://arxiv.org/abs/2404.09785)                                                                                                          |                                     **Evaluation**&**Safety**                                     |
| 24.04 |                                                                                               University of Cambridge, New York University, ETH Zurich                                                                                               |                        arxiv                        |                                                                                                                        [Foundational Challenges in Assuring Alignment and Safety of Large Language Models](https://arxiv.org/abs/2404.09932)                                                                                                                         |                                     **Alignment**&**Safety**                                      |
| 24.04 |                                                                                                                 Zhejiang University                                                                                                                  |                        arxiv                        |                                                                                                                    [TransLinkGuard: Safeguarding Transformer Models Against Model Stealing in Edge Deployment](https://arxiv.org/abs/2404.11121)                                                                                                                     |             **Intellectual Property Protection**&**Edge-deployed Transformer Model**              |
| 24.04 |                                                                                                                  Harvard University                                                                                                                  |                        arxiv                        |                                                                                                               [More RLHF, More Trust? On The Impact of Human Preference Alignment On Language Model Trustworthiness](https://arxiv.org/abs/2404.18870)                                                                                                               |                **Reinforcement Learning from Human Feedback**&**Trustworthiness**                 |
| 24.05 |                                                                                                                University of Maryland                                                                                                                |                        arxiv                        |                                                                                                                                         [Constrained Decoding for Secure Code Generation](https://arxiv.org/abs/2405.00218)                                                                                                                                          |             **Code Generation**&**Code LLM**&**Secure Code Generation**&**AI Safety**             |
| 24.05 |                                                                                                    Huazhong University of Science and Technology                                                                                                     |                        arxiv                        |                                                                                                                             [Large Language Models for Cyber Security: A Systematic Literature Review](https://arxiv.org/abs/2405.04760)                                                                                                                             |                              **Cybersecurity**&**Systematic Review**                              |
| 24.04 |                                                                                                                    CSIRO‚Äôs Data61                                                                                                                    | ACM International Conference on AI-powered Software |                                                                                                               [An AI System Evaluation Framework for Advancing AI Safety: Terminology, Taxonomy, Lifecycle Mapping](https://arxiv.org/abs/2404.05388)                                                                                                                |                  **AI Safety**&**Evaluation Framework**&**AI Lifecycle Mapping**                  |
| 24.05 |                                                                                                                 CSAIL and CBMM, MIT                                                                                                                  |                        arxiv                        |                                                                                                        [SecureLLM: Using Compositionality to Build Provably Secure Language Models for Private, Sensitive, and Secret Data](https://arxiv.org/abs/2405.09805)                                                                                                        |                                **SecureLLM**&**Compositionality**                                 |
| 24.05 |                                                                                                              Carnegie Mellon University                                                                                                              |                        arxiv                        |                                                                                                                            [Human‚ÄìAI Safety: A Descendant of Generative AI and Control Systems Safety](https://arxiv.org/abs/2405.09794)                                                                                                                             |                               **Human‚ÄìAI Safety**&**Generative AI**                               |
| 24.05 |                                                                                                                  University of York                                                                                                                  |                        arxiv                        |                                                                                                                           [Safe Reinforcement Learning in Black-Box Environments via Adaptive Shielding](https://arxiv.org/abs/2405.18180)                                                                                                                           |         **Safe Reinforcement Learning**&**Black-Box Environments**&**Adaptive Shielding**         |
| 24.05 |                                                                                                                 Princeton University                                                                                                                 |                        arxiv                        |                                                                                                                                  [AI Risk Management Should Incorporate Both Safety and Security](https://arxiv.org/abs/2405.19524)                                                                                                                                  |                         **AI Safety**&**AI Security**&**Risk Management**                         |
| 24.05 |                                                                                                                  University of Oslo                                                                                                                  |                        arxiv                        |                                                                                                                                                [AI Safety: A Climb to Armageddon?](https://arxiv.org/abs/2405.19832)                                                                                                                                                 |                       **AI Safety**&**Existential Risk**&**AI Governance**                        |
| 24.06 |                                                                                                                    Zscaler, Inc.                                                                                                                     |                        arxiv                        |                                                                                                                           [Exploring Vulnerabilities and Protections in Large Language Models: A Survey](https://arxiv.org/abs/2406.00240)                                                                                                                           |                       **Prompt Hacking**&**Adversarial Attacks**&**Suvery**                       |
| 24.06 |                                                                                                         Texas A & M University - San Antonio                                                                                                         |                        arxiv                        |                                                                                                           [Transforming Computer Security and Public Trust Through the Exploration of Fine-Tuning Large Language Models](https://arxiv.org/abs/2406.00628)                                                                                                           |                                **Fine-Tuning**&**Cyber Security**                                 |
| 24.06 |                                                                                                                    Alibaba Group                                                                                                                     |                        arxiv                        |                                                                                                                     [How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States](https://arxiv.org/abs/2406.05644)                                                                                                                      |                            **LLM Safety**&**Alignment**&**Jailbreak**                             |
| 24.06 |                                                                                                                       UC Davis                                                                                                                       |                        arxiv                        |                                                                                                                                                      [Security of AI Agents](https://arxiv.org/abs/2406.08689)                                                                                                                                                       |                          **Security**&**AI Agents**&**Vulnerabilities**                           |
| 24.06 |                                                                                                              University of Connecticut                                                                                                               |                 USENIX Security ‚Äò24                 |                                                                                             [An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities against Strong Detection](https://arxiv.org/abs/2406.06822)                                                                                              |            **Backdoor Attack**&**Code Completion Models**&**Vulnerability Detection**             |
| 24.06 |                                                                                                           University of California, Irvine                                                                                                           |                        arxiv                        |                                                                                                                                         [TorchOpera: A Compound AI System for LLM Safety](https://arxiv.org/abs/2406.10847)                                                                                                                                          |                       **TorchOpera**&**LLM Safety**&**Compound AI System**                        |
| 24.06 |                                                                                                                  NVIDIA Corporation                                                                                                                  |                        arxiv                        |                                                                                                                                  [garak: A Framework for Security Probing Large Language Models](https://arxiv.org/abs/2406.11036)                                                                                                                                   |                                  **garak**&**Security Probing**                                   |
| 24.06 |                                                                                                              Carnegie Mellon University                                                                                                              |                        arxiv                        |                                                                                                                                           [Current State of LLM Risks and AI Guardrails](https://arxiv.org/abs/2406.12934)                                                                                                                                           |                                  **LLM Risks**&**AI Guardrails**                                  |
| 24.06 |                                                                                                               Johns Hopkins University                                                                                                               |                        arxiv                        |                                                                                                                                  [Every Language Counts: Learn and Unlearn in Multilingual LLMs](https://arxiv.org/abs/2406.13748)                                                                                                                                   |                     **Multilingual LLMs**&**Fake Information**&**Unlearning**                     |
| 24.06 |                                                                                                                 Tsinghua University                                                                                                                  |                        arxiv                        |                                                                                                                                         [Finding Safety Neurons in Large Language Models](https://arxiv.org/abs/2406.14144)                                                                                                                                          |                 **Safety Neurons**&**Mechanistic Interpretability**&**AI Safety**                 |
| 24.06 |                                                                                       Center for AI Safety and Governance, Institute for AI, Peking University                                                                                       |                        arxiv                        |                                                                                                                    [SAFESORA: Towards Safety Alignment of Text2Video Generation via a Human Preference Dataset](https://arxiv.org/abs/2406.14477)                                                                                                                    |                          **Safety Alignment**&**Text2Video Generation**                           |
| 24.06 |                                                                                                Samsung R&D Institute UK, KAUST, University of Oxford                                                                                                 |                        arxiv                        |                                                                                                                                [Model Merging and Safety Alignment: One Bad Model Spoils the Bunch](https://arxiv.org/abs/2406.14563)                                                                                                                                |                              **Model Merging**&**Safety Alignment**                               |
| 24.06 |                                                                                                                  Hofstra University                                                                                                                  |                        arxiv                        |                                                                                                                                       [Analyzing Multi-Head Attention on Trojan BERT Models](https://arxiv.org/abs/2406.16925)                                                                                                                                       |                    **Trojan Attack**&**BERT Models**&**Multi-Head Attention**                     |
| 24.06 |                                                                                                                   Fudan University                                                                                                                   |                        arxiv                        |                                                                                                                     [SafeAligner: Safety Alignment against Jailbreak Attacks via Response Disparity Guidance](https://arxiv.org/abs/2406.18118)                                                                                                                      |                 **Safety Alignment**&**Jailbreak Attacks**&**Response Disparity**                 |
| 24.06 |                                                                                                                Stony Brook University                                                                                                                |                 NAACL 2024 Workshop                 |                                                                                                                                      [Automated Adversarial Discovery for Safety Classifiers](https://arxiv.org/abs/2406.17104)                                                                                                                                      |                    **Safety Classifiers**&**Adversarial Attacks**&**Toxicity**                    |
| 24.07 |                                                                                                                  University of Utah                                                                                                                  |                        arxiv                        |                                                                                                                            [Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression](https://arxiv.org/abs/2407.04965)                                                                                                                             |                            **Model Compression**&**Safety Evaluation**                            |
| 24.07 |                                                                                                                University of Alberta                                                                                                                 |                        arxiv                        |                                                                                                                           [Multilingual Blending: LLM Safety Alignment Evaluation with Language Mixture](https://arxiv.org/abs/2407.07342)                                                                                                                           |              **Multilingual Blending**&**LLM Safety Alignment**&**Language Mixture**              |
| 24.07 |                                                                                                            Singapore National Eye Centre                                                                                                             |                        arxiv                        |                                                                                            [A Proposed S.C.O.R.E. Evaluation Framework for Large Language Models ‚Äì Safety, Consensus, Objectivity, Reproducibility and Explainability](https://arxiv.org/abs/2407.07666)                                                                                             |                                     **Evaluation Framework**                                      |
| 24.07 |                                                                                                                      Microsoft                                                                                                                       |                        arxiv                        |                                                                                                                                       [SLIP: Securing LLM‚Äôs IP Using Weights Decomposition](https://arxiv.org/abs/2407.10886)                                                                                                                                        |                 **Hybrid Inference**&**Model Security**&**Weights Decomposition**                 |
| 24.07 |                                                                                                                      Microsoft                                                                                                                       |                        arxiv                        |                                                                                                                          [Phi-3 Safety Post-Training: Aligning Language Models with a ‚ÄúBreak-Fix‚Äù Cycle](https://arxiv.org/abs/2407.13833)                                                                                                                           |                                **Phi-3**&**Safety Post-Training**                                 |
| 24.07 |                                                                                                                 Tsinghua University                                                                                                                  |                        arxiv                        |                                                                                                                                 [Course-Correction: Safety Alignment Using Synthetic Preferences](https://arxiv.org/abs/2407.16637)                                                                                                                                  |               **Course-Correction**&**Safety Alignment**&**Synthetic Preferences**                |
| 24.07 |                                                                                                               Northwestern University                                                                                                                |                        arxiv                        |                                                                                                                   [From Sands to Mansions: Enabling Automatic Full-Life-Cycle Cyberattack Construction with LLM](https://arxiv.org/abs/2407.16928)                                                                                                                   |                         **Cyberattack Construction**&**Full-Life-Cycle**                          |
| 24.07 |                                                                                                    Singapore University of Technology and Design                                                                                                     |                        arxiv                        |                                                                                                                                    [AI Safety in Generative AI Large Language Models: A Survey](https://arxiv.org/abs/2407.18369)                                                                                                                                    |                                  **Generative AI**&**AI Safety**                                  |
| 24.07 |                                                                                                                  Lehigh University                                                                                                                   |                        arxiv                        |                                                                                                                            [Blockchain for Large Language Model Security and Safety: A Holistic Survey](https://arxiv.org/abs/2407.20181)                                                                                                                            |                              **Blockchain**&**Security**&**Safety**                               |
| 24.08 |                                                                                                                        OpenAI                                                                                                                        |                       openai                        |                                                                                                                       [Rule-Based Rewards for Language Model Safety](https://cdn.openai.com/rule-based-rewards-for-language-model-safety.pdf)                                                                                                                        |                   **Reinforcement Learning**&**Safety**&**Rule-Based Rewards**                    |
| 24.08 |                                                                                                            University of Texas at Austin                                                                                                             |                        arxiv                        |                                                                                                                          [HIDE AND SEEK: Fingerprinting Large Language Models with Evolutionary Learning](https://arxiv.org/abs/2408.02871)                                                                                                                          |                         **Model Fingerprinting**&**In-context Learning**                          |
| 24.08 |                                                                                                            Technical University of Munich                                                                                                            |                        arxiv                        |                                                                                                                        [Large Language Models for Secure Code Assessment: A Multi-Language Empirical Study](https://arxiv.org/abs/2408.06428)                                                                                                                        |                      **Secure Code Assessment**&**Vulnerability Detection**                       |
| 24.08 |                                                                                                       Offenburg University of Applied Sciences                                                                                                       |                        arxiv                        |                                                                                                                                ["You still have to study" - On the Security of LLM generated code](https://arxiv.org/abs/2408.07106)                                                                                                                                 |                            **Code Security**&**Prompting Techniques**                             |
| 24.08 |                                                                                                              University of Connecticut                                                                                                               |                        arxiv                        |                                                                                                   [Clip2Safety: A Vision Language Model for Interpretable and Fine-Grained Detection of Safety Compliance in Diverse Workplaces](https://arxiv.org/abs/2408.07146)                                                                                                   |    **Vision Language Model**&**Safety Compliance**&**Personal Protective Equipment Detection**    |
| 24.08 |                                                                                                      Pabna University of Science and Technology                                                                                                      |                        arxiv                        |                                                                                                                [Risks, Causes, and Mitigations of Widespread Deployments of Large Language Models (LLMs): A Survey](https://arxiv.org/abs/2408.04643)                                                                                                                |                             **Privacy**&**Bias**&**Interpretability**                             |
| 24.08 |                                                                                                                Quinnipiac University                                                                                                                 |                        arxiv                        |                                                                                                     [Is Generative AI the Next Tactical Cyber Weapon For Threat Actors? Unforeseen Implications of AI Generated Cyber Attacks](https://arxiv.org/abs/2408.12806)                                                                                                     |                       **Generative AI**&**Cybersecurity**&**Cyber Attacks**                       |
| 24.08 |                                                                                                           Nanyang Technological University                                                                                                           |                        arxiv                        |                                                                                                   [Trustworthy, Responsible, and Safe AI: A Comprehensive Architectural Framework for AI Safety with Challenges and Mitigations](https://arxiv.org/abs/2408.12935)                                                                                                   |                           **AI Safety**&**Trustworthy**&**Responsible**                           |
| 24.08 |                                                                                                  King Abdullah University of Science and Technology                                                                                                  |                        arxiv                        |                                                                                                                      [Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in Language Models](https://arxiv.org/abs/2408.15313)                                                                                                                       |                           **Safety**&**Helpfulness**&**LLM Alignment**                            |
| 24.08 |                                                                                                                University of Calgary                                                                                                                 |                        arxiv                        |                                                                                                                       [Trustworthy and Responsible AI for Human-Centric Autonomous Decision-Making Systems](https://arxiv.org/abs/2408.15550)                                                                                                                        |                    **Trustworthy AI**&**Algorithmic Bias**&**Responsible AI**                     |
| 24.08 |                                                                                                                 University of Oxford                                                                                                                 |                        arxiv                        |                                                                                                                        [AI Security Audits: Challenges and Innovations in Assessing Large Language Models](https://arxiv.org/abs/2408.16163)                                                                                                                         |                 **AI Security Audits**&**Vulnerability Assessment**&**AI Ethics**                 |
| 24.08 |                                                                                                    University of Science and Technology of China                                                                                                     |                        arxiv                        |                                                                                                                             [Safety Layers of Aligned Large Language Models: The Key to LLM Security](https://arxiv.org/abs/2408.17003)                                                                                                                              |                    **Aligned LLM**&**Safety Layers**&**Security Degradation**                     |
| 24.09 |                                                                                                          University of Texas at San Antonio                                                                                                          |                        arxiv                        |                                                                                                              [Enhancing Source Code Security with LLMs: Demystifying The Challenges and Generating Reliable Repairs](https://arxiv.org/abs/2409.00571)                                                                                                               |                   **Source Code Security**&**LLMs**&**Reinforcement Learning**                    |
| 24.09 |                                                                                                         The Hong Kong Polytechnic University                                                                                                         |                        arxiv                        |                                                                                                                                [Alignment-Aware Model Extraction Attacks on Large Language Models](https://arxiv.org/abs/2409.02718)                                                                                                                                 |              **Model Extraction Attacks**&**LLM Alignment**&**Watermark Resistance**              |
| 24.09 |                                                                                                        University of Oxford, Redwood Research                                                                                                        |                        arxiv                        |                                                                                                                          [Games for AI Control: Models of Safety Evaluations of AI Deployment Protocols](https://arxiv.org/abs/2409.07985)                                                                                                                           |                        **AI Control**&**Safety Protocols**&**Game Theory**                        |
| 24.09 |                                                                                                                 University of Galway                                                                                                                 |                 ECAI AIEB Workshop                  |                                                                                                                                   [Ethical AI Governance: Methods for Evaluating Trustworthy AI](https://arxiv.org/abs/2409.07473)                                                                                                                                   |                          **Trustworthy AI**&**Ethics**&**AI Evaluation**                          |
| 24.09 |                                                                                                          University of Texas at San Antonio                                                                                                          |                        arxiv                        |                                                                                                         [AutoSafeCoder: A Multi-Agent Framework for Securing LLM Code Generation through Static Analysis and Fuzz Testing](https://arxiv.org/abs/2409.10737)                                                                                                         |          **Multi-Agent Systems**&**Code Security**&**Fuzz Testing**&**Static Analysis**           |
| 24.09 |                                                                                                                 Tsinghua University                                                                                                                  |                        arxiv                        |                                                                                                                                         [Language Models Learn to Mislead Humans via RLHF](https://arxiv.org/abs/2409.12822)                                                                                                                                         |      **Reinforcement Learning from Human Feedback (RLHF)**&**U-SOPHISTRY**&**Misleading AI**      |
| 24.09 |                                                                                                           Stevens Institute of Technology                                                                                                            |                        arxiv                        |                                                                                                                        [Measuring Copyright Risks of Large Language Model via Partial Information Probing](https://arxiv.org/abs/2409.13831)                                                                                                                         |                           **Copyright**&**Partial Information Probing**                           |
| 24.09 |                                                                                                                     IBM Research                                                                                                                     |                        arxiv                        |                                                                                                                    [Attack Atlas: A Practitioner‚Äôs Perspective on Challenges and Pitfalls in Red Teaming GenAI](https://arxiv.org/abs/2409.15398)                                                                                                                    |                     **Red Teaming**&**LLM Security**&**Adversarial Attacks**                      |
| 24.09 |                                                                                                                 Pengcheng Laboratory                                                                                                                 |                        arxiv                        |                                                                                                                                    [Multi-Designated Detector Watermarking for Language Models](https://arxiv.org/abs/2409.17518)                                                                                                                                    |             **Watermarking**&**Claimability**&**Multi-designated Verifier Signature**             |
| 24.09 |                                                                                                                      ETH Zurich                                                                                                                      |                        arxiv                        |                                                                                                                                  [An Adversarial Perspective on Machine Unlearning for AI Safety](https://arxiv.org/abs/2409.18025)                                                                                                                                  |             **Machine Unlearning**&**Adversarial Attacks**&**Unlearning Robustness**              |
| 24.10 |                                                                                                                   Google DeepMind                                                                                                                    |                        arxiv                        |                                                                                                                                            [A Watermark for Black-Box Language Models](https://arxiv.org/abs/2410.02099)                                                                                                                                             |                      **Watermarking**&**Black-Box Models**&**LLM Detection**                      |
| 24.10 |                                                                                               Mohamed Bin Zayed University of Artificial Intelligence                                                                                                |                        arxiv                        |                                                                                                                            [Optimizing Adaptive Attacks Against Content Watermarks for Language Models](https://arxiv.org/abs/2410.02440)                                                                                                                            |                      **Watermarking**&**Adaptive Attacks**&**LLM Security**                       |
| 24.10 |                                                                                                         Rice University, Rutgers University                                                                                                          |                        arxiv                        |                                                                                                                      [Taylor Unswift: Secured Weight Release for Large Language Models via Taylor Expansion](https://arxiv.org/abs/2410.05331)                                                                                                                       |                              **Taylor Expansion**&**Model Security**                              |
| 24.10 |                                                                                                                      PeopleTec                                                                                                                       |                        arxiv                        |                                                                                                                     [Hallucinating AI Hijacking Attack: Large Language Models and Malicious Code Recommenders](https://arxiv.org/abs/2410.06462)                                                                                                                     |                               **Cybersecurity**&**Hallucinations**                                |
| 24.10 |                                                                                                   Fondazione Bruno Kessler, Universit√© C√¥te d‚ÄôAzur                                                                                                   |                     EMNLP 2024                      |                                                                                                            [Is Safer Better? The Impact of Guardrails on the Argumentative Strength of LLMs in Hate Speech Countering](https://arxiv.org/abs/2410.03466)                                                                                                             |                              **Counterspeech**&**Safety Guardrails**                              |
| 24.10 |                                                                                                     University of California, Davis, AWS AI Labs                                                                                                     |                        arxiv                        |                                                                                                                         [Unraveling and Mitigating Safety Alignment Degradation of Vision-Language Models](https://arxiv.org/abs/2410.09047)                                                                                                                         |  **Safety alignment**&**Vision-Language models**&**Cross-modality representation manipulation**   |
| 24.10 |                                                                                                           North Carolina State University                                                                                                            |                        arxiv                        |                                                                                                               [Superficial Safety Alignment Hypothesis: The Need for Efficient and Robust Safety Mechanisms in LLMs](https://arxiv.org/abs/2410.10862)                                                                                                               |       **Superficial safety alignment**&**Safety mechanisms**&**Safety-critical components**       |
| 24.10 |                                                                            Shanghai Jiao Tong University, Chinese University of Hong Kong (Shenzhen), Tsinghua University                                                                            |                        arxiv                        |                                                                                                                            [ARCHILLES‚Äô HEEL IN SEMI-OPEN LLMS: HIDING BOTTOM AGAINST RECOVERY ATTACKS](https://arxiv.org/abs/2410.11182)                                                                                                                             |                   **Semi-open LLMs**&**Recovery attacks**&**Model resilience**                    |
| 24.10 |                                                                                                                 University of Tulsa                                                                                                                  |                        arxiv                        |                                                                                                              [Weak-to-Strong Generalization beyond Accuracy: A Pilot Study in Safety, Toxicity, and Legal Reasoning](https://arxiv.org/abs/2410.12621)                                                                                                               |                     **Weak-to-Strong Generalization**&**Safety**&**Toxicity**                     |
| 24.10 |                                                                                                                  Aalborg University                                                                                                                  |                        arxiv                        |                                                                                                         [Large Language Models are Easily Confused: A Quantitative Metric, Security Implications and Typological Analysis](https://arxiv.org/abs/2410.13237)                                                                                                         |             **Language confusion**&**Multilingual LLMs**&**Security vulnerabilities**             |
| 24.10 |                                                                                                              Carnegie Mellon University                                                                                                              |                        arxiv                        |                                                                                                                                   [Refusal-Trained LLMs Are Easily Jailbroken As Browser Agents](https://arxiv.org/abs/2410.13886)                                                                                                                                   |                         **LLM Safety**&**Browser Agents**&**Red Teaming**                         |
| 24.10 |                                                                                                                  Palisade Research                                                                                                                   |                        arxiv                        |                                                                                                                                   [LLM Agent Honeypot: Monitoring AI Hacking Agents in the Wild](https://arxiv.org/abs/2410.13919)                                                                                                                                   |                          **LLM Agents**&**Honeypots**&**Cybersecurity**                           |
| 24.10 |                                                                                                               University of Pittsburgh                                                                                                               |                        arxiv                        |                                                                                                                       [Coherence-Driven Multimodal Safety Dialogue with Active Learning for Embodied Agents](https://arxiv.org/abs/2410.14141)                                                                                                                       |                   **Embodied Agents**&**Multimodal Safety**&**Active Learning**                   |
| 24.10 |                                                                                                                    CSIRO‚Äôs Data61                                                                                                                    |                        arxiv                        |                                                                                                         [From Solitary Directives to Interactive Encouragement! LLM Secure Code Generation by Natural Language Prompting](https://arxiv.org/abs/2410.14321)                                                                                                          |                      **Secure Code Generation**&**Encouragement Prompting**                       |
| 24.10 |                                                                                                                       AppCubic                                                                                                                       |                        arxiv                        |                                                                                                                             [Jailbreaking and Mitigation of Vulnerabilities in Large Language Models](https://arxiv.org/abs/2410.15236)                                                                                                                              |                       **Prompt Injection**&**Jailbreaking**&**AI Security**                       |
| 24.10 |                                                                                                                     UC Berkeley                                                                                                                      |                        arxiv                        |                                                                                                                          [SAFETYANALYST: Interpretable, Transparent, and Steerable LLM Safety Moderation](https://arxiv.org/abs/2410.16665)                                                                                                                          |                    **LLM Safety**&**Interpretability**&**Content Moderation**                     |
| 24.10 |                                                                                                               ShanghaiTech University                                                                                                                |                        arxiv                        |                                                                                                                 [Enhancing Safety in Reinforcement Learning with Human Feedback via Rectified Policy Optimization](https://arxiv.org/abs/2410.19933)                                                                                                                 |              **Safety Alignment**&**Reinforcement Learning**&**Policy Optimization**              |
| 24.11 |                                                                                                                 Zhejiang University                                                                                                                  |                        arxiv                        |                                                                                                                      [Enhancing Multiple Dimensions of Trustworthiness in LLMs via Sparse Activation Control](https://arxiv.org/abs/2411.02461)                                                                                                                      |           **Trustworthiness**&**Sparse Activation Control**&**Representation Control**            |
| 24.11 |                                                                                                         University of California, Riverside                                                                                                          |                        arxiv                        |                                                                                                               [Unfair Alignment: Examining Safety Alignment Across Vision Encoder Layers in Vision-Language Models](https://arxiv.org/abs/2411.04291)                                                                                                                |           **Vision-Language Models**&**Safety Alignment**&**Cross-Layer Vulnerability**           |
| 24.11 |                                                                                                           National University of Singapore                                                                                                           |                     EMNLP 2024                      |                                                                                                                   [Multi-expert Prompting Improves Reliability, Safety and Usefulness of Large Language Models](https://arxiv.org/abs/2411.00492)                                                                                                                    |             **Multi-expert Prompting**&**LLM Safety**&**Reliability**&**Usefulness**              |
| 24.11 |                                                                                                                        OpenAI                                                                                                                        |                    NeurIPS 2024                     |                                                                                                                                           [Rule Based Rewards for Language Model Safety](https://arxiv.org/abs/2411.01111)                                                                                                                                           |                    **Rule Based Rewards**&**Safety Alignment**&**AI Feedback**                    |
| 24.11 |                                                                                        Center for Automation and Robotics, Spanish National Research Council                                                                                         |                        arXiv                        |                                                                                                                                 [Can Adversarial Attacks by Large Language Models Be Attributed?](https://arxiv.org/abs/2411.08003)                                                                                                                                  |              **Adversarial Attribution**&**LLM Security**&**Formal Language Theory**              |
| 24.11 |                                                                                                                  McGill University                                                                                                                   |                        arXiv                        |                                                                                                                               [Beyond the Safety Bundle: Auditing the Helpful and Harmless Dataset](https://arxiv.org/abs/2411.08243)                                                                                                                                |             **Helpful and Harmless Dataset**&**Safety Trade-offs**&**Bias Analysis**              |
| 24.11 |                                                                                                                   Fudan University                                                                                                                   |                        arxiv                        |                                                                                                                               [Safe Text-to-Image Generation: Simply Sanitize the Prompt Embedding](https://arxiv.org/abs/2411.10329)                                                                                                                                |             **Text-to-Image Generation**&**Safety**&**Prompt Embedding Sanitization**             |
| 24.11 |                                                                                                                         Meta                                                                                                                         |                        arxiv                        |                                                                                                                          [Llama Guard 3 Vision: Safeguarding Human-AI Image Understanding Conversations](https://arxiv.org/abs/2411.10414)                                                                                                                           |               **Multimodal LLM**&**Content Moderation**&**Adversarial Robustness**                |
| 24.11 |                                                                                                                 Columbia University                                                                                                                  |                        arxiv                        |                                                                                                                  [When Backdoors Speak: Understanding LLM Backdoor Attacks Through Model-Generated Explanations](https://arxiv.org/abs/2411.12701)                                                                                                                   |                              **Backdoor Attacks**&**Explainability**                              |
| 24.11 |                                                                                                          Ben-Gurion University of the Negev                                                                                                          |                        arxiv                        |                                                                                                                                   [The Information Security Awareness of Large Language Models](https://arxiv.org/abs/2411.13207)                                                                                                                                    |                        **Information Security Awareness**&**Benchmarking**                        |
| 24.11 |                                                                                                                  Fordham University                                                                                                                  |                        arxiv                        |                                                                                                                                 [Next-Generation Phishing: How LLM Agents Empower Cyber Attackers](https://arxiv.org/abs/2411.13874)                                                                                                                                 |                             **Phishing Detection**&**Cybersecurity**                              |
| 24.12 |                                                                                                                     UC Berkeley                                                                                                                      |                        arxiv                        |                                                                                                                                        [Trust & Safety of LLMs and LLMs in Trust & Safety](https://arxiv.org/abs/2412.02113)                                                                                                                                         |                             **Trust and Safety**&**Prompt Injection**                             |
| 24.12 |                                                                                                     Harvard Kennedy SchoolÔºå Avant Research Group                                                                                                     |                        arxiv                        |                                                                                                   [Evaluating Large Language Models' Capability to Launch Fully Automated Spear Phishing Campaigns: Validated on Human Subjects](https://arxiv.org/abs/2412.00586)                                                                                                   |                            **Phishing Attacks**&**Human-in-the-loop**                             |
| 24.12 |                                                                                                             University of Massachusetts                                                                                                              |                        arxiv                        |                                                                                                                           [Safe to Serve: Aligning Instruction-Tuned Models for Safety and Helpfulness](https://arxiv.org/abs/2412.00074)                                                                                                                            |                         **Instruction Tuning**&**Safety**&**Helpfulness**                         |
| 24.11 |                                                                                             University of Pennsylvania, IBM T.J. Watson Research Center                                                                                              |                        arxiv                        |                                                                                                                       [Cyber-Attack Technique Classification Using Two-Stage Trained Large Language Models](https://arxiv.org/abs/2411.18755)                                                                                                                        |                      **Cyber-Attack Classification**&**Two-Stage Training**                       |
| 24.12 |                                                                                                            University of New South Wales                                                                                                             |                        arxiv                        |                                                                                                                    [How Can LLMs and Knowledge Graphs Contribute to Robot Safety? A Few-Shot Learning Approach](https://arxiv.org/abs/2412.11387)                                                                                                                    |               **Robot Safety**&**Few-Shot Learning**&**Knowledge Graph Prompting**                |
| 24.12 |                                                                                                                  √ñrebro University                                                                                                                   |                        arxiv                        |                                                                                                                             [Large Language Models and Code Security: A Systematic Literature Review](https://arxiv.org/abs/2412.15004)                                                                                                                              |           **LLM-Generated Code**&**Vulnerability Detection**&**Data Poisoning Attacks**           |
| 24.12 |                                                                                                              Algiers Research Institute                                                                                                              |                        arxiv                        |                                                                                                                [On the Validity of Traditional Vulnerability Scoring Systems for Adversarial Attacks against LLMs](https://arxiv.org/abs/2412.20087)                                                                                                                 |               **Adversarial Attacks**&**Vulnerability Metrics**&**Risk Assessment**               |
| 25.01 |                                                                                                                         Meta                                                                                                                         |                        arxiv                        |                                                                                                                                     [MLLM-as-a-Judge for Image Safety without Human Labeling](https://arxiv.org/abs/2501.00192)                                                                                                                                      |           **Image Safety**&**Zero-Shot Judgment**&**Multimodal Large Language Models**            |
| 25.01 |                                                                                                                FAU Erlangen-N√ºrnberg                                                                                                                 |                        arxiv                        |                                                                                                                                [Refusal Behavior in Large Language Models: A Nonlinear Perspective](https://arxiv.org/abs/2501.08145)                                                                                                                                |              **Refusal Behavior**&**Mechanistic Interpretability**&**AI Alignment**               |
| 25.01 |                                                                                                                University of Waterloo                                                                                                                |                        arxiv                        |                                                                                                                                     [Advanced Real-Time Fraud Detection Using RAG-Based LLMs](https://arxiv.org/abs/2501.15290)                                                                                                                                      |         **Fraud Detection**&**Retrieval-Augmented Generation**&**Real-Time AI Security**          |
| 25.01 |                                                                                                     Mondragon University, University of Seville                                                                                                      |                        arxiv                        |                                                                                                                   [Early External Safety Testing of OpenAI‚Äôs O3-Mini: Insights from Pre-Deployment Evaluation](https://arxiv.org/abs/2501.17749v1)                                                                                                                   |                             **LLM Safety Testing**&**OpenAI O3-Mini**                             |
| 25.02 |                                                                                                           Nanyang Technological University                                                                                                           |                        arxiv                        |                                                                                                                  [Picky LLMs and Unreliable RMs: An Empirical Study on Safety Alignment after Instruction Tuning](https://arxiv.org/abs/2502.01116)                                                                                                                  |                    **LLM Alignment**&**Instruction Tuning**&**Reward Models**                     |
| 25.02 |                                                                                                                University of Bristol                                                                                                                 |                        arxiv                        |                                                                                                              [The Dark Deep Side of DeepSeek: Fine-Tuning Attacks Against the Safety Alignment of CoT-Enabled Models](https://arxiv.org/abs/2502.01225)                                                                                                              |                    **Chain of Thought**&**Fine-Tuning Attack**&**LLM Safety**                     |
| 25.02 |                                                                                                                  Marburg University                                                                                                                  |                        arxiv                        |                                                                                                                                     [Editing Large Language Models Poses Serious Safety Risks](https://arxiv.org/abs/2502.02958)                                                                                                                                     |             **Knowledge Editing**&**LLM Security Risks**&**Adversarial Manipulation**             |
| 25.02 |                                                                                                            Technical University of Munich                                                                                                            |                      AAAI 2025                      |                                                                                                                            [Medical Multimodal Model Stealing Attacks via Adversarial Domain Alignment](https://arxiv.org/abs/2502.02438)                                                                                                                            |         **Medical Multimodal Models**&**Model Stealing**&**Adversarial Domain Alignment**         |
| 25.02 |                                                                                                           Georgia Institute of Technology                                                                                                            |                        arxiv                        |                                                                                                                                [Enhancing Phishing Email Identification with Large Language Models](https://arxiv.org/abs/2502.04759)                                                                                                                                |                             **Phishing Detection**&**Cybersecurity**                              |
| 25.02 |                                                                                                                   Fudan University                                                                                                                   |                        arxiv                        |                                                                                                                                  [Safety at Scale: A Comprehensive Survey of Large Model Safety](https://arxiv.org/abs/2502.05206)                                                                                                                                   |                  **Large Model Safety**&**AI Security**&**Adversarial Attacks**                   |
| 25.02 |                                                                                                                University of Maryland                                                                                                                |                        arxiv                        |                                                                                                                           [Model Tampering Attacks Enable More Rigorous Evaluations of LLM Capabilities](https://arxiv.org/abs/2502.05209)                                                                                                                           |              **Model Tampering Attacks**&**LLM Security**&**Adversarial Robustness**              |
| 25.02 |                                                                                                                Penn State University                                                                                                                 |                        arxiv                        |                                                                                                                               [Can LLMs Rank the Harmfulness of Smaller LLMs? We are Not There Yet](https://arxiv.org/abs/2502.05291)                                                                                                                                |                     **Harmfulness Ranking**&**LLM Evaluation**&**AI Safety**                      |
| 25.02 |                                                                                                                  Peking University                                                                                                                   |                        arxiv                        |                                                                                                                    [Are Smarter LLMs Safer? Exploring Safety-Reasoning Trade-offs in Prompting and Fine-Tuning](https://arxiv.org/abs/2502.09673)                                                                                                                    |                      **LLM Safety**&**Reasoning Trade-off**&**Fine-Tuning**                       |
| 25.02 |                                                                                                             City University of Hong Kong                                                                                                             |                        arxiv                        |                                                                                                                           [The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Safety Analysis](https://arxiv.org/abs/2502.09674)                                                                                                                            |                  **LLM Alignment**&**Safety Fine-Tuning**&**Jailbreak Attacks**                   |
| 25.02 |                                                                                                                 Tsinghua University                                                                                                                  |                        arxiv                        |                                                                                                                  [‚ÄúNuclear Deployed!‚Äù: Analyzing Catastrophic Risks in Decision-making of Autonomous LLM Agents](https://arxiv.org/abs/2502.11355)                                                                                                                   |               **Autonomous LLM Agents**&**Catastrophic Risks**&**Decision-making**                |
| 25.02 |                                                                                                               University of Washington                                                                                                               |                        arxiv                        |                                                                                                                      [SAFECHAIN: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities](https://arxiv.org/abs/2502.12025)                                                                                                                      |                 **LLM Safety**&**Chain-of-Thought Reasoning**&**Model Alignment**                 |
| 25.02 |                                                                                                         University of California, Santa Cruz                                                                                                         |                        arxiv                        |                                                                                                                              [The Hidden Risks of Large Reasoning Models: A Safety Assessment of R1](https://arxiv.org/abs/2502.12659)                                                                                                                               |             **Large Reasoning Models**&**Safety Assessment**&**Adversarial Attacks**              |
| 25.02 |                                                                                                               University of Cambridge                                                                                                                |                        arxiv                        |                                                                                                                            [Improved Fine-Tuning of Large Multimodal Models for Hateful Meme Detection](https://arxiv.org/abs/2502.13061)                                                                                                                            |             **Hateful Meme Detection**&**Multimodal Models**&**Contrastive Learning**             |
| 25.02 |                                                                                                              Cooperative AI Foundation                                                                                                               |                        arXiv                        |                                                                                                                                                [Multi-Agent Risks from Advanced AI](https://arxiv.org/abs/2502.14143)                                                                                                                                                |                       **Multi-Agent Systems**&**AI Risk**&**AI Governance**                       |
| 25.02 |                                                                                            Apart Research, University of Science and Technology of Hanoi                                                                                             |        AAAI 2025 Workshop on Theory of Mind         |                                                                                                               [A Survey of Theory of Mind in Large Language Models: Evaluations, Representations, and Safety Risks](https://arxiv.org/abs/2502.06470)                                                                                                                |                                 **Theory of Mind**&**AI Safety**                                  |
| 25.02 |                                                                                                        Truthful AI, University College London                                                                                                        |                        arxiv                        |                                                                                                                           [Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs](https://arxiv.org/abs/2502.17424)                                                                                                                           |                 **LLM Alignment**&**Fine-tuning Risks**&**Emergent Misalignment**                 |
| 25.02 |                                                                                                                   Wuhan University                                                                                                                   |                        arxiv                        |                                                                                                                      [A Survey of Safety on Large Vision-Language Models: Attacks, Defenses and Evaluations](https://arxiv.org/abs/2502.14881)                                                                                                                       |                  **LVLM Safety**&**Adversarial Attacks**&**Defense Mechanisms**                   |
| 25.02 |                                                                                                               Clark Atlanta University                                                                                                               |                        arxiv                        |                                                                                                      [SoK: Exploring Hallucinations and Security Risks in AI-Assisted Software Development with Insights for LLM Deployment](https://arxiv.org/abs/2502.18468)                                                                                                       |            **Hallucinations**&**Security Risks**&**AI-Assisted Software Development**             |
| 25.02 |                                                                                                   Stony Brook Universit, Michigan State University                                                                                                   |                        arXiv                        |                                                                                                                         [Cyber Defense Reinvented: Large Language Models as Threat Intelligence Copilots](https://arxiv.org/abs/2502.20791)                                                                                                                          |           **Cyber Threat Intelligence**&**Large Language Models**&**Threat Detection**            |
| 25.03 |                                                                                                                      HydroX AI                                                                                                                       |                        arXiv                        |                                                                                                                                 [Output Length Effect on DeepSeek-R1‚Äôs Safety in Forced Thinking](https://arxiv.org/abs/2503.01923)                                                                                                                                  |                       **Output Length**&**LLM Safety**&**Forced Thinking**                        |
| 25.03 |                                                                                                                  Tampere University                                                                                                                  |                        arxiv                        |                                                                                                              [Mapping Trustworthiness in Large Language Models: A Bibliometric Analysis Bridging Theory to Practice](https://arxiv.org/abs/2503.04785)                                                                                                               |                                 **Trustworthiness**&**AI Ethics**                                 |
| 25.03 |                                                                                                       University of California, Santa Barbara                                                                                                        |                        arxiv                        |                                                                                                                         [Graphormer-Guided Task Planning: Beyond Static Rules with LLM Safety Perception](https://arxiv.org/abs/2503.06866)                                                                                                                          |                      **LLM Planning**&**Graphormer**&**Risk-Aware Robotics**                      |
| 25.03 |                                                                                                              University of Pennsylvania                                                                                                              |                        arxiv                        |                                                                                                                                             [Safety Guardrails for LLM-Enabled Robots](https://arxiv.org/abs/2503.07885)                                                                                                                                             |          **LLM-enabled Robotics**&**Jailbreaking Defense**&**Formal Safety Guarantees**           |
| 25.03 |                                                                                                                  Peking University                                                                                                                   |                        arxiv                        |                                                                                                                                         [LIFE-CYCLE ROUTING VULNERABILITIES OF LLM ROUTER](https://arxiv.org/abs/2503.08704)                                                                                                                                         |                     **LLM Router**&**Adversarial Attack**&**Backdoor Attack**                     |
| 25.03 |                                                                                                                 Squirrel AI Learning                                                                                                                 |                        arxiv                        |                                                                                                                                 [A Survey on Trustworthy LLM Agents: Threats and Countermeasures](https://arxiv.org/abs/2503.09648)                                                                                                                                  |                 **Trustworthy Agent**&**LLM-based Agents**&**Multi-Agent System**                 |
| 25.03 |                                                                                                                     Cornell Tech                                                                                                                     |                        arxiv                        |                                                                                                                                       [Multi-Agent Systems Execute Arbitrary Malicious Code](https://arxiv.org/abs/2503.12188)                                                                                                                                       |          **Multi-Agent Systems**&**Control-Flow Hijacking**&**Arbitrary Code Execution**          |
| 25.03 |                                                                                                                  University of Utah                                                                                                                  |                        arxiv                        |                                                                                                                                       [A Comprehensive Study of LLM Secure Code Generation](https://arxiv.org/abs/2503.15554)                                                                                                                                        |        **Secure Code Generation**&**Vulnerability Scanning**&**Functionality Evaluation**         |
| 25.03 |                                                                                                               University of Minnesota                                                                                                                |                        arxiv                        |                                                                                                                                 [Safety Aware Task Planning via Large Language Models in Robotics](https://arxiv.org/abs/2503.15707)                                                                                                                                 |        **LLM Robotics Planning**&**Safety-Aware Framework**&**Control Barrier Functions**         |
| 25.03 |                                                                                               Peking University, Zhongguancun Lab, Tsinghua University                                                                                               |                        arxiv                        |                                                                                                                [Large Language Models powered Network Attack Detection: Architecture, Opportunities and Case Study](https://arxiv.org/abs/2503.18487)                                                                                                                |                  **Network Security**&**LLM for Security**&**Anomaly Detection**                  |
| 25.03 |                                                                                            Aim Intelligence, Yonsei University, Seoul National University                                                                                            |                        arxiv                        |                                                                                                                                                   [sudo rm -rf agentic_security](https://arxiv.org/abs/2503.20279)                                                                                                                                                   |              **Agent Security**&**Multimodal Jailbreak**&**LLM Agent Exploitation**               |
| 25.03 |                                                                                                   Georgia Institute of Technology, IMT Mines Albi                                                                                                    |                        arxiv                        |                                                                                                              [Leveraging Large Language Models for Risk Assessment in Hyperconnected Logistic Hub Network Deployment](https://arxiv.org/abs/2503.21115)                                                                                                              |              **Risk Assessment**&**LLMs for Logistics**&**Supply Chain Resilience**               |
| 25.04 |                                                                                                                 University of Twente                                                                                                                 |                        arxiv                        |                                                                                                                     [Safety and Security Risk Mitigation in Satellite Missions via Attack-Fault-Defense Trees](https://arxiv.org/abs/2504.00988)                                                                                                                     |      **Cyber-Physical Systems**&**Attack-Fault-Defense Trees**&**Satellite Ground Segment**       |
| 25.04 |                                                                                                                   Google DeepMind                                                                                                                    |                        arxiv                        |                                                                                                                                         [An Approach to Technical AGI Safety and Security](https://arxiv.org/abs/2504.01849)                                                                                                                                         |                 **AGI Safety**&**Misalignment Mitigation**&**Capability Control**                 |
| 25.04 |                                                                                                                   Earlham College                                                                                                                    |                     ISDFS 2025                      |                                                                                                                                   [Debate-Driven Multi-Agent LLMs for Phishing Email Detection](https://arxiv.org/abs/2503.22038)                                                                                                                                    |                 **Phishing Detection**&**Multi-Agent LLMs**&**Debate Framework**                  |
| 25.04 |                                                                                                        Indian Institute of Technology Kanpur                                                                                                         |                      MSR 2025                       |                                                                                                            [MaLAware: Automating the Comprehension of Malicious Software Behaviours using Large Language Models (LLMs)](https://arxiv.org/abs/2504.01145)                                                                                                            |             **Malware Analysis**&**Behavior Explanation**&**LLMs for Cybersecurity**              |
| 25.04 | Leidos | arxiv | [MCP Safety Audit: LLMs with the Model Context Protocol Allow Major Security Exploits](https://arxiv.org/abs/2504.03767) | **Model Context Protocol**&**Security Audit**&**Agentic LLM Exploits** |
| 25.04 | Norwegian University of Science and Technology | arxiv | [An LLM Framework For Cryptography Over Chat Channels](https://arxiv.org/abs/2504.08871) | **LLMs**&**Cryptography**&**Steganography** |
| 25.04 | Peking University | arxiv | [SaRO: Enhancing LLM Safety through Reasoning-based Alignment](https://arxiv.org/abs/2504.09420) | **Safety Alignment**&**Reasoning-based Alignment**&**LLMs** |
| 25.04 | Johns Hopkins University | arxiv | [An Investigation of Large Language Models and Their Vulnerabilities in Spam Detection](https://arxiv.org/abs/2504.09776) | **Spam Detection**&**Adversarial Attack**&**Data Poisoning** |
| 25.04 | TU Wien | arxiv | [Benchmarking Practices in LLM-driven Offensive Security: Testbeds, Metrics, and Experiment Design](https://arxiv.org/abs/2504.10112) | **Offensive Security**&**Benchmarking**&**LLM Penetration Testing** |
| 25.04 | Fraunhofer Institute for Cognitive Systems IKS | arxiv | [Towards Automated Safety Requirements Derivation Using Agent-based RAG](https://arxiv.org/abs/2504.11243) | **Agent-based RAG**&**Safety Requirements Derivation**&**Autonomous Driving** |
| 25.04 | Nanjing University | arxiv | [Everything You Wanted to Know About LLM-based Vulnerability Detection But Were Afraid to Ask](https://arxiv.org/abs/2504.13474) | **LLM-based Vulnerability Detection**&**Contextual Reasoning**&**Benchmark Evaluation** |
| 25.04 | Nanyang Technological University | arxiv | [A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment](https://arxiv.org/abs/2504.15585) | **LLM Safety**&**LLM Lifecycle**&**Agent Alignment** |
| 25.04 | Arab American University | arxiv | [Multimodal Large Language Models for Enhanced Traffic Safety: A Comprehensive Review and Future Trends](https://arxiv.org/abs/2504.16134) | **Traffic Safety**&**Multimodal Large Language Models**&**ADAS** |
| 25.04 | National University of Singapore | arxiv | [Safety in Large Reasoning Models: A Survey](https://arxiv.org/abs/2504.17704) | **Large Reasoning Models**&**Safety Taxonomy**&**Adversarial Attacks** |


## üíªPresentations & Talks


## üìñTutorials & Workshops

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |

## üì∞News & Articles

| Date  |  Type  |                                Title                                 |                           URL                            |
|:-----:|:------:|:--------------------------------------------------------------------:|:--------------------------------------------------------:|
| 23.01 | video  | ChatGPT and InstructGPT: Aligning Language Models to Human Intention | [link](https://www.youtube.com/watch?v=RkFS6-GwCxE&t=6s) |
| 23.06 | Report |         ‚ÄúDual-use dilemma‚Äù for GenAI Workshop Summarization          |         [link](https://arxiv.org/abs/2308.14840)         |
| 23.10 |  News  |              Joint Statement on AI Safety and Openness               |         [link](https://open.mozilla.org/letter/)         |

## üßë‚Äçüè´Scholars
