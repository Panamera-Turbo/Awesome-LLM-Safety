# Privacy

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating with the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"


## üìëPapers

| Date  |                                                                                                    Institute                                                                                                    |                Publication                |                                                                                          Paper                                                                                           |                                               Keywords                                                |
|:-----:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-----------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-----------------------------------------------------------------------------------------------------:|
| 18.02 |                                                                                                  Google Brain                                                                                                   |           USENIX Security 2021            |        [The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks](https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting)        |                                       **Memorization**&**LSTM**                                       |
| 19.12 |                                                                                                    Microsoft                                                                                                    |                  CCS2020                  |                                [Analyzing Information Leakage of Updates to Natural Language Models](https://dl.acm.org/doi/abs/10.1145/3372297.3417880)                                 |                          **Privacy Leakage**&**Model Update**&**Duplicated**                          |
| 21.07 |                                                                                                 Google Research                                                                                                 |                  ACL2022                  |                                         [Deduplicating Training Data Makes Language Models Better](https://aclanthology.org/2022.acl-long.577/)                                          |                       **Privacy Protected**&**Deduplication**&**Memorization**                        |
| 21.10 |                                                                                                    Stanford                                                                                                     |                 ICLR2022                  |                                    [Large language models can be strong differentially private learners](https://openreview.net/forum?id=bVuP3ltATMz)                                    |                            **Differential Privacy**&**Gradient Clipping**                             |
| 22.02 |                                                                                                 Google Research                                                                                                 |                 ICLR2023                  |                                           [Quantifying Memorization Across Neural Language Models](https://openreview.net/forum?id=TatRHT_1cK)                                           |                                **Memorization**&**Verbatim Sequence**                                 |
| 22.02 |                                                                                                 UNC Chapel Hill                                                                                                 |                 ICML2022                  |                               [Deduplicating Training Data Mitigates Privacy Risks in Language Models](https://proceedings.mlr.press/v162/kandpal22a.html)                               |                            **Memorization**&**Deduplicate Training Data**                             |
| 22.05 |                                                                                                      UCSD                                                                                                       |                 EMNLP2022                 |                           [An Empirical Analysis of Memorization in Fine-tuned Autoregressive Language Models](https://aclanthology.org/2022.emnlp-main.119/)                            |                                  **Privacy Risks**&**Memorization**                                   |
| 22.05 |                                                                                                    Princeton                                                                                                    |                 NIPS2022                  | [Recovering Private Text in Federated Learning of Language Models](https://proceedings.neurips.cc/paper_files/paper/2022/hash/35b5c175e139bff5f22a5361270fce87-Abstract-Conference.html) |                               **Federated Learning**&**Gradient Based**                               |
| 22.05 |                                                                                   University of Illinois at Urbana-Champaign                                                                                    |            EMNLP2022(findings)            |                              [Are Large Pre-Trained Language Models Leaking Your Personal Information?](https://aclanthology.org/2022.findings-emnlp.148/)                               |                      **Personal Information**&**Memorization**&**Privacy Risk**                       |
| 22.10 |                                                                                                 Google Research                                                                                                 |                 INLG2023                  |                      [Preventing Generation of Verbatim Memorization in Language Models Gives a False Sense of Privacy](https://aclanthology.org/2023.inlg-main.3/)                      |                    **Verbatim Memorization**&**Filter**&**Style Transfer Prompts**                    |
| 23.02 |                                                                                             University of Waterloo                                                                                              |         Security and Privacy2023          |             [Analyzing Leakage of Personally Identifiable Information in Language Models](https://www.computer.org/csdl/proceedings-article/sp/2023/933600a346/1NrbXJj80H6)              |                    **PII Leakage**&**PII Reconstruction**&**Differential Privacy**                    |
| 23.04 |                                                                                 Hong Kong University of Science and Technology                                                                                  |            EMNLP2023(findings)            |                                                  [Multi-step Jailbreaking Privacy Attacks on ChatGPT](https://arxiv.org/abs/2304.05197)                                                  |                                      **Privacy**&**Jailbreaks**                                       |
| 23.05 |                                                                                   University of Illinois at Urbana-Champaign                                                                                    |                   arxiv                   |                        [Quantifying Association Capabilities of Large Language Models and Its Implications on Privacy Leakage](https://arxiv.org/abs/2305.12707)                         |                                       **Co-occurrence**&**PII**                                       |
| 23.05 |                                                                                        The University of Texas at Dallas                                                                                        |                  ACL2023                  |                               [Controlling the Extraction of Memorized Datafrom Large Language Models via Prompt-Tuning](https://arxiv.org/abs/2305.11759)                               |                                  **Prompt-Tuning**&**Memorization**                                   |
| 23.05 |                                                                                                 Google Research                                                                                                 |            NAACL2024(findings)            |                                    [Can Public Large Language Models Help Private Cross-device Federated Learning?](https://arxiv.org/abs/2305.12132)                                    |               **Federated Learning**&**Large Language Models**&**Differential Privacy**               |
| 23.06 |                                                                                   University of Illinois at Urbana-Champaign                                                                                    |                   arxiv                   |                                      [DECODINGTRUST: A Comprehensive Assessment of Trustworthiness in GPT Models](https://arxiv.org/abs/2306.11698)                                      |                          **Robustness**&**Ethics**&**Privacy**&**Toxicity**                           |
| 23.08 |                                                                                       Bern University of Applied Sciences                                                                                       |            NAACL2024(findings)            |                       [Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models in Court Decisions](https://arxiv.org/abs/2308.11103)                        |                   **Anonymization**&**Re-Identification**&**Large Language Models**                   |
| 23.09 |                                                                                                 UNC Chapel Hill                                                                                                 |                   arxiv                   |                         [Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks](https://arxiv.org/abs/2309.17410)                          |         **Hidden States Attack**&**Hidden States Defense**&**Deleting Sensitive Information**         |
| 23.09 |                                                                                         Princeton University&Microsoft                                                                                          |                   arxiv                   |                                [Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation](https://arxiv.org/abs/2309.11765)                                |                           **In-Context Learning**&**Differential Privacy**                            |
| 23.10 |                                                                                                       ETH                                                                                                       |                   arxiv                   |                                   [Beyond Memorization: Violating Privacy Via Inference with Large Language Models](https://arxiv.org/abs/2310.07298)                                    |                      **Context Inference**&**Privacy-Invasive**&**Extract PII**                       |
| 23.10 |                                                                                         Indiana University Bloomington                                                                                          |                 CCS 2024                  |                              [The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks](https://arxiv.org/abs/2310.15469)                               |                                  **Privacy risks**&**PII Recovery**                                   |
| 23.10 |                                                                     University of Washington & Allen Institute for Artificial Intelligence                                                                      |                   arxiv                   |                       [Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory](https://arxiv.org/abs/2310.17884)                        |                       **Benchmark**&**Contextual Privacy**&**Chain-of-thought**                       |
| 23.10 |                                                                                         Georgia Institute of Technology                                                                                         |                   arxiv                   |                                            [Unlearn What You Want to Forget: Efficient Unlearning for LLMs](https://arxiv.org/abs/2310.20150)                                            |                   **Unlearning**&**Teacher-student Framework**&**Data Protection**                    |
| 23.10 |                                                                                               Tianjin University                                                                                                |                 EMNLP2023                 |                                      [DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models](https://arxiv.org/abs/2310.20138)                                       |                 **Privacy Neuron Detection**&**Model Editing**&**Data Memorization**                  |
| 23.11 |                                                                                               Zhejiang University                                                                                               |                   arxiv                   |                                     [Input Reconstruction Attack against Vertical Federated Large Language Models](https://arxiv.org/abs/2311.07585)                                     |             **Vertical Federated Learning**&**Input Reconstruction**&**Privacy Concerns**             |
| 23.11 |                                                                           Georgia Institute of Technology, Carnegie Mellon University                                                                           |                   arxiv                   |                                        [Reducing Privacy Risks in Online Self-Disclosures with Language Models](https://arxiv.org/abs/2311.09538)                                        |             **Online Self-Disclosure**&**Privacy Risks**&**Self-Disclosure Abstraction**              |
| 23.11 |                                                                                               Cornell University                                                                                                |                   arxiv                   |                                                               [Language Model Inversion](https://arxiv.org/abs/2311.13647)                                                               |                       **Model Inversion**&**Prompt Reconstruction**&**Privacy**                       |
| 23.11 |                                                                                                    Ant Group                                                                                                    |                   arxiv                   |                                                   [PrivateLoRA for Efficient Privacy Preserving LLM](https://arxiv.org/abs/2311.14030)                                                   |                                    **Privacy Preserving**&**LoRA**                                    |
| 23.12 |                                                                                                Drexel University                                                                                                |                   arXiv                   |                              [A Survey on Large Language Model (LLM) Security and Privacy: The Good the Bad and the Ugly](https://arxiv.org/abs/2312.02003)                              |                                 **Security**&**Privacy**&**Attacks**                                  |
| 23.12 |                                                                 University of Texas at Austin, Princeton University, MIT, University of Chicago                                                                 |                   arxiv                   |                                      [DP-OPT: MAKE LARGE LANGUAGE MODEL YOUR PRIVACY-PRESERVING PROMPT ENGINEER](https://arxiv.org/abs/2312.03724)                                       |                              **Prompt Tuning**&**Differential Privacy**                               |
| 23.12 |                                                                                         Delft University of Technology                                                                                          |                 ICSE 2024                 |                                               [Traces of Memorisation in Large Language Models for Code](https://arxiv.org/abs/2312.11658)                                               |                           **Code Memorisation**&**Data Extraction Attacks**                           |
| 23.12 |                                                                                          University of Texas at Austin                                                                                          |                   arXiv                   |                     [SentinelLMs: Encrypted Input Adaptation and Fine-tuning of Language Models for Private and Secure Inference](https://arxiv.org/abs/2312.17342)                      |                        **Privacy**&**Security**&**Encrypted Input Adaptation**                        |
| 23.12 |                                                                              Rensselaer Polytechnic Institute, Columbia University                                                                              |                   arXiv                   |                             [Differentially Private Low-Rank Adaptation of Large Language Model Using Federated Learning](https://arxiv.org/abs/2312.17493)                              |               **Federated Learning**&**Differential Privacy**&**Efficient Fine-Tuning**               |
| 24.01 |                                                                     Harbin Institute of Technology Shenzhen&Peng Cheng Laboratory Shenzhen                                                                      |                   arxiv                   |                             [SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models](https://arxiv.org/abs/2401.00793)                              | **Privacy-Preserving Inference (PPI)**&**Secure Multi-Party Computing (SMPC)**&**Transformer Models** |
| 24.01 |                                                           NUS (Chongqing) Research Institute, Huawei Noah‚Äôs Ark Lab, National University of Singapore                                                           |                   arxiv                   |                                                    [Teach Large Language Models to Forget Privacy](https://arxiv.org/abs/2401.00870)                                                     |                    **Data Privacy**&**Prompt Learning**&**Problem Decomposition**                     |
| 24.01 |                                                                                 Princeton University, Google DeepMind, Meta AI                                                                                  |                   arxiv                   |                                     [Private Fine-tuning of Large Language Models with Zeroth-order Optimization](https://arxiv.org/abs/2401.04343)                                      |                        **Differential Privacy**&**Zeroth-order Optimization**                         |
| 24.01 |                                                                                 Harvard&USC&UCLA&UW Seattle&UW-Madison&UC Davis                                                                                 |                 NAACL2024                 |                                                [Instructional Fingerprinting of Large Language Models](https://arxiv.org/abs/2401.12255)                                                 |                **Model Fingerprinting**&**Instructional Backdoor**&**Model Ownership**                |
| 24.02 |                                                                                        Florida International University                                                                                         |                   arxiv                   |                                          [Security and Privacy Challenges of Large Language Models: A Survey](https://arxiv.org/abs/2402.00888)                                          |                            **Security**&**Privacy Challenges**&**Suevey**                             |
| 24.02 |                                                              Northeastern University, Carnegie Mellon University, Rensselaer Polytechnic Institute                                                              |                   arxiv                   |                                         [Human-Centered Privacy Research in the Age of Large Language Models](https://arxiv.org/abs/2402.01994)                                          |                     **Generative AI**&**Privacy**&**Human-Computer Interaction**                      |
| 24.02 |                                                                                 CISPA Helmholtz Center for Information Security                                                                                 |                   arxiv                   |                                                [Conversation Reconstruction Attack Against GPT Models](https://arxiv.org/abs/2402.02987)                                                 |                 **Conversation Reconstruction Attack**&**Privacy risks**&**Security**                 |
| 24.02 |                                                                             Columbia University, M365 Research, Microsoft Research                                                                              |                   arxiv                   |                                             [Differentially Private Training of Mixture of Experts Models](https://arxiv.org/abs/2402.07334)                                             |                            **Differential Privacy**&**Mixture of Experts**                            |
| 24.02 |                                                                                Stanford University, Truera ,Princeton University                                                                                |                   arxiv                   |                                      [De-amplifying Bias from Differential Privacy in Language Model Fine-tuning](https://arxiv.org/abs/2402.04489)                                      |                      **Fairness**&**Differential Privacy**&**Data Augmentation**                      |
| 24.02 |                                                                                     Sun Yat-sen University, Google Research                                                                                     |                   arxiv                   |                                          [Privacy-Preserving Instructions for Aligning Large Language Models](https://arxiv.org/abs/2402.13659)                                          |                             **Privacy Risks**&**Synthetic Instructions**                              |
| 24.02 |                                                                                    National University of Defense Technology                                                                                    |                   arxiv                   |            [LLM-based Privacy Data Augmentation Guided by Knowledge Distillation with a Distribution Tutor for Medical Text Classification](https://arxiv.org/abs/2402.16515)            |       **Privacy Data Augmentation**&**Knowledge Distillation**&**Medical Text Classification**        |
| 24.02 |                                                                                      Michigan State University, Baidu Inc.                                                                                      |                   arxiv                   |                                [The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2402.16893)                                |                         **Privacy**&**Retrieval-Augmented Generation (RAG)**                          |
| 24.02 |                                                                      University of Washington&Allen Institute for Artificial Intelligence                                                                       |                 NAACL2024                 |                          [JAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding over Small Language Models](https://arxiv.org/abs/2402.08761)                           |             **Authorship Obfuscation**&**Constrained Decoding**&**Small Language Models**             |
| 24.03 |                                                                                                  Virginia Tech                                                                                                  |                   arxiv                   |                                                [Privacy-Aware Semantic Cache for Large Language Models](https://arxiv.org/abs/2403.02694)                                                |                           **Federated Learning**&**Cache Hit**&**Privacy**                            |
| 24.03 |                                                                                               Tsinghua University                                                                                               |                   arxiv                   |                 [CoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following](https://arxiv.org/abs/2403.03129)                  |             **Small Language Models**&**Privacy**&**Context-Aware Instruction Following**             |
| 24.03 |                                                                            Shandong University, Leiden University, Drexel University                                                                            |                   arxiv                   |                                       [On Protecting the Data Privacy of Large Language Models (LLMs): A Survey](https://arxiv.org/abs/2403.05156)                                       |                          **Data Privacy**&**Privacy Protection**&**Survey**                           |
| 24.03 |                 Arizona State University, University of Minnesota, University of Science and Technology of China, North Carolina State University, University of North Carolina at Chapel Hill                  |                   arxiv                   |                                       [Privacy-preserving Fine-tuning of Large Language Models through Flatness](https://arxiv.org/abs/2403.04124)                                       |                           **Differential Privacy**&**Model Generalization**                           |
| 24.03 |                                                                                        University of Southern California                                                                                        |                   arxiv                   |                                        [Differentially Private Next-Token Prediction of Large Language Models](https://arxiv.org/abs/2403.15638)                                         |                                       **Differential Privacy**                                        |
| 24.04 |                                    University of Maryland, Oregon State University, ELLIS Institute T√ºbingen & MPI Intelligent Systems, T√ºbingen AI Center, Google DeepMind                                     |                   arxiv                   |                                [Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models](https://arxiv.org/abs/2404.01231)                                |                  **Privacy Backdoors**&**Membership Inference**&**Model Poisoning**                   |
| 24.04 |                                                                City University of Hong Kong, The Hong Kong University of Science and Technology                                                                 |                   arxiv                   |                                           [LMEraser: Large Model Unlearning through Adaptive Prompt Tuning](https://arxiv.org/abs/2404.11056)                                            |               **Machine Unlearning**&**Adaptive Prompt Tuning**&**Privacy Protection**                |
| 24.04 |                                                           University of Electronic Science and Technology of China, Chengdu University of Technology                                                            |                   arxiv                   |                                      [Understanding Privacy Risks of Embeddings Induced by Large Language Models](https://arxiv.org/abs/2404.16587)                                      |                                   **Privacy Risks**&**Embeddings**                                    |
| 24.04 |                                                                                             Salesforce AI Research                                                                                              |                   arxiv                   |                            [Investigating the prompt leakage effect and black-box defenses for multi-turn LLM interactions](https://arxiv.org/abs/2404.16251)                            |                               **Prompt Leakage**&**Black-box Defenses**                               |
| 24.04 |                                                   University of Texas at El Paso, Texas A&M University Central Texas, University of Maryland Baltimore County                                                   |                   arxiv                   |                     [PrivComp-KG: Leveraging Knowledge Graph and Large Language Models for Privacy Policy Compliance Verification](https://arxiv.org/abs/2404.19744)                     |                     **Privacy Policy**&**Policy Compliance**&**Knowledge Graph**                      |
| 24.05 |                                                                                           Renmin University of China                                                                                            |                COLING 2024                |                                                  [Locally Differentially Private In-Context Learning](https://arxiv.org/abs/2405.04032)                                                  |                        **In-context Learning**&**Local Differential Privacy**                         |
| 24.05 |                                                                                             University of Maryland                                                                                              |                 NAACL2024                 |                                              [Keep It Private: Unsupervised Privatization of Online Text](https://arxiv.org/abs/2405.10260)                                              |               **Unsupervised Privatization**&**Online Text**&**Large Language Models**                |
| 24.05 |                                                                                               Zhejiang University                                                                                               |                   arxiv                   |                                    [PermLLM: Private Inference of Large Language Models within 3 Seconds under WAN](https://arxiv.org/abs/2405.18744)                                    |                             **Private Inference**&**Secure Computation**                              |
| 24.05 |                                                                                            University of Connecticut                                                                                            |                   arxiv                   |                    [LMO-DP: Optimizing the Randomization Mechanism for Differentially Private Fine-Tuning of Large Language Models](https://arxiv.org/abs/2405.18776)                    |                               **Differential Privacy**&**Fine-Tuning**                                |
| 24.05 |                                                                                        University of Technology, Sydney                                                                                         |                   arxiv                   |                                        [Large Language Model Watermark Stealing with Mixed Integer Programming](https://arxiv.org/abs/2405.19677)                                        |                 **Watermark Stealing**&**Mixed Integer Programming**&**LLM Security**                 |
| 24.05 |                                                                                  Huazhong University of Science and Technology                                                                                  |         Procedia Computer Science         |                                              [No Free Lunch Theorem for Privacy-Preserving LLM Inference](https://arxiv.org/abs/2405.20681)                                              |                        **Privacy**&**LLM Inference**&**No Free Lunch Theory**                         |
| 24.05 |                                                                                                   ETH Zurich                                                                                                    |                   arxiv                   |                                                   [Black-Box Detection of Language Model Watermarks](https://arxiv.org/abs/2405.20777)                                                   |                             **Watermark Detection**&**Black-Box Testing**                             |
| 24.06 |                                                                                      South China University of Technology                                                                                       |                   arxiv                   |                      [PrivacyRestore: Privacy-Preserving Inference in Large Language Models via Privacy Removal and Restoration](https://arxiv.org/abs/2406.01394)                       |                                 **Privacy-Preserving**&**Inference**                                  |
| 24.06 |                                                                                           Carnegie Mellon University                                                                                            |                 ICML 2024                 |                                   [PrE-Text: Training Language Models on Private Federated Data in the Age of LLMs](https://arxiv.org/abs/2406.02958)                                    |                  **Federated Learning**&**Differential Privacy**&**Synthetic Data**                   |
| 24.06 |                                                                                      University of California, Santa Cruz                                                                                       |                   arxiv                   |                                           [Large Language Model Unlearning via Embedding-Corrupted Prompts](https://arxiv.org/abs/2406.07933)                                            |                            **Unlearning**&**Embedding-Corrupted Prompts**                             |
| 24.06 |                                                                                         University of Technology Sydney                                                                                         |                   arxiv                   |                                 [Unique Security and Privacy Threats of Large Language Model: A Comprehensive Survey](https://arxiv.org/abs/2406.07973)                                  |                               **Security Threats**&**Privacy Threats**                                |
| 24.06 |                                                                                                UC Santa Barbara                                                                                                 |                   arxiv                   |                         [Reversing the Forget-Retain Objectives: An Efficient LLM Unlearning Framework from Logit Difference](https://arxiv.org/abs/2406.08607)                          |                          **LLM Unlearning**&**Logit Difference**&**Privacy**                          |
| 24.06 |                                                                                    Technion ‚Äì Israel Institute of Technology                                                                                    |                   arxiv                   |                          [REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space](https://arxiv.org/abs/2406.09325)                          |                               **Unlearning**&**Sensitive Information**                                |
| 24.06 |                                                         University of Maryland, ELLIS Institute T√ºbingen, Max Planck Institute for Intelligent Systems                                                          |                   arxiv                   |                                    [Be like a Goldfish, Don‚Äôt Memorize! Mitigating Memorization in Generative LLMs](https://arxiv.org/abs/2406.10209)                                    |                                  **Memorization**&**Goldfish Loss**                                   |
| 24.06 |                                                                            McCombs School of Business, University of Texas at Austin                                                                            |                   arxiv                   |                                          [PRISM: A Design Framework for Open-Source Foundation Model Safety](https://arxiv.org/abs/2406.10415)                                           |                         **PRISM**&**Open-Source**&**Foundation Model Safety**                         |
| 24.06 |                                                                                         Zhejiang University, MIT, UCLA                                                                                          |                   arxiv                   |                                          [MemDPT: Differential Privacy for Memory Efficient Language Models](https://arxiv.org/abs/2406.11087)                                           |               **MemDPT**&**Differential Privacy**&**Memory Efficient Language Models**                |
| 24.06 |                                                                                         Zhejiang University, MIT, UCLA                                                                                          |                   arxiv                   |                              [GOLDCOIN: Grounding Large Language Models in Privacy Laws via Contextual Integrity Theory](https://arxiv.org/abs/2406.11149)                               |                     **GOLDCOIN**&**Contextual Integrity Theory**&**Privacy Laws**                     |
| 24.06 |                                                                                                  IBM Research                                                                                                   |                   arxiv                   |                               [Split, Unlearn, Merge: Leveraging Data Attributes for More Effective Unlearning in LLMs](https://arxiv.org/abs/2406.11780)                                |                                  **SPUNGE**&**Unlearning**&**LLMs**                                   |
| 24.06 |                                                                                     Ping An Technology (Shenzhen) Co., Ltd.                                                                                     |                   arxiv                   |                                             [PFID: Privacy First Inference Delegation Framework for LLMs](https://arxiv.org/abs/2406.12238)                                              |                             **PFID**&**Privacy**&**Inference Delegation**                             |
| 24.06 |                                                                                 Hong Kong University of Science and Technology                                                                                  |                   arxiv                   |                             [PDSS: A Privacy-Preserving Framework for Step-by-Step Distillation of Large Language Models](https://arxiv.org/abs/2406.12403)                              |                                    **PDSS**&**Privacy-Preserving**                                    |
| 24.06 |                                                                                         KAIST AI, Hyundai Motor Company                                                                                         |                   arxiv                   |                        [Protecting Privacy Through Approximating Optimal Parameters for Sequence Unlearning in Language Models](https://arxiv.org/abs/2406.14091)                        |                 **Privacy Protection**&**Optimal Parameters**&**Sequence Unlearning**                 |
| 24.06 |                                                                                               Nanjing University                                                                                                |                   arxiv                   |                                    [The Fire Thief Is Also the Keeper: Balancing Usability and Privacy in Prompts](https://arxiv.org/abs/2406.14318)                                     |                      **Prompt Privacy**&**Anonymization**&**Privacy Protection**                      |
| 24.06 |                                                                                   University of Massachusetts Amherst, Google                                                                                   |                   arxiv                   |                                           [POSTMARK: A Robust Blackbox Watermark for Large Language Models](https://arxiv.org/abs/2406.14517)                                            |                     **Blackbox Watermark**&**Paraphrasing Attacks**&**Detection**                     |
| 24.06 |                                                                                            Michigan State University                                                                                            |                   arxiv                   |                            [Mitigating the Privacy Issues in Retrieval-Augmented Generation (RAG) via Pure Synthetic Data](https://arxiv.org/abs/2406.14773)                             |                   **Retrieval-Augmented Generation**&**Privacy**&**Synthetic Data**                   |
| 24.06 |                                                                                               Beihang University                                                                                                |                   arxiv                   |                              [Safely Learning with Private Data: A Federated Learning Framework for Large Language Model](https://arxiv.org/abs/2406.14898)                              |                                  **Federated Learning**&**Privacy**                                   |
| 24.06 |                                                                                         University of Rome Tor Vergata                                                                                          |                   arxiv                   |                                 [Enhancing Data Privacy in Large Language Models through Private Association Editing](https://arxiv.org/abs/2406.18221)                                  |                           **Data Privacy**&**Private Association Editing**                            |
| 24.07 |                                                                                          Huawei Munich Research Center                                                                                          |                   arxiv                   |                     [IncogniText: Privacy-enhancing Conditional Text Anonymization via LLM-based Private Attribute Randomization](https://arxiv.org/abs/2407.02956)                      |                                  **Text Anonymization**&**Privacy**                                   |
| 24.07 |                                                                                          Huawei Munich Research Center                                                                                          |                   arxiv                   |                          [ObfuscaTune: Obfuscated Offsite Fine-tuning and Inference of Proprietary LLMs on Private Datasets](https://arxiv.org/abs/2407.02960)                           |                          **Inference**&**Proprietary LLMs**&**Private Data**                          |
| 24.07 |                                                                                              Texas A&M University                                                                                               |                   arxiv                   |                               [Exposing Privacy Gaps: Membership Inference Attack on Preference Data for LLM Alignment](https://arxiv.org/abs/2407.06443)                                |                 **Membership Inference Attack**&**Preference Data**&**LLM Alignment**                 |
| 24.07 |                                                                                                 Google Research                                                                                                 |                   arxiv                   |                                        [Fine-Tuning Large Language Models with User-Level Differential Privacy](https://arxiv.org/abs/2407.07737)                                        |                          **User-Level Differential Privacy**&**Fine-Tuning**                          |
| 24.07 |                                                                                              Newcastle University                                                                                               |                   arxiv                   |                              [Privacy-Preserving Data Deduplication for Enhancing Federated Learning of Language Models](https://arxiv.org/abs/2407.08152)                               |       **Privacy-Preserving Deduplication**&**Federated Learning**&**Private Set Intersection**        |
| 24.07 |                                                                                  Huazhong University of Science and Technology                                                                                  |                   arxiv                   |                                                        [On the (In)Security of LLM App Stores](https://arxiv.org/abs/2407.08422)                                                         |                              **LLM App Stores**&**Security**&**Privacy**                              |
| 24.07 |                                                                                               Soochow University                                                                                                |                   arxiv                   |                                             [Learning to Refuse: Towards Mitigating Privacy Risks in LLMs](https://arxiv.org/abs/2407.10058)                                             |                               **Privacy Risks**&**Machine Unlearning**                                |
| 24.07 |                                                                            The University of Texas Health Science Center at Houston                                                                             |                   arxiv                   |                        [Robust Privacy Amidst Innovation with Large Language Models Through a Critical Assessment of the Risks](https://arxiv.org/abs/2407.16166)                        |                   **Text Generation**&**Privacy**&**Protected Health Information**                    |
| 24.07 |                                                                                          Huawei Munich Research Center                                                                                          |             ACL 2024 Workshop             |                            [PII-Compass: Guiding LLM training data extraction prompts towards the target PII via grounding](https://arxiv.org/abs/2407.02943)                            |                         **PII Extraction**&**Data Privacy**&**LLM Security**                          |
| 24.07 |                                                                                         University of Technology Sydney                                                                                         |                   arxiv                   |                                      [The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies](https://arxiv.org/abs/2407.19354)                                       |                          **LLM Agent**&**Privacy Preservation**&**Defense**                           |
| 24.07 |                                                                                            University of Notre Dame                                                                                             |                   arxiv                   |                                                    [Machine Unlearning in Generative AI: A Survey](https://arxiv.org/abs/2407.20516)                                                     |                       **Generative Models**&**Trustworthy ML**&**Data Privacy**                       |
| 24.07 |                                                                                                                                                                                                                 |                   arxiv                   |                                 [Adaptive Pre-training Data Detection for Large Language Models via Surprising Tokens](https://arxiv.org/abs/2407.21248)                                 |                **Pre-training Data Detection**&**Surprising Tokens**&**Data Privacy**                 |
| 24.08 |                                                                                               Sichuan University                                                                                                |                   arxiv                   |                                     [HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy Protection](https://arxiv.org/abs/2408.02927)                                      |                           **Tabular Data Synthesis**&**Privacy Protection**                           |
| 24.08 |                                                                                                   UC Berkeley                                                                                                   |                   arxiv                   |                                                          [MPC-Minimized Secure LLM Inference](https://arxiv.org/abs/2408.03561)                                                          |                  **Secure Multi-party Computation**&**Privacy-Preserving Inference**                  |
| 24.08 |                                                                                           Sapienza University of Rome                                                                                           |                   arxiv                   |                                [Preserving Privacy in Large Language Models: A Survey on Current Threats and Solutions](https://arxiv.org/abs/2408.05212)                                |                             **Privacy Attacks**&**Differential Privacy**                              |
| 24.08 |                                                                                       New Jersey Institute of Technology                                                                                        |                   arxiv                   |                              [Casper: Prompt Sanitization for Protecting User Privacy in Web-Based Large Language Models](https://arxiv.org/abs/2408.07004)                              |                               **Prompt Sanitization**&**User Privacy**                                |
| 24.08 |                                                                                                Xidian University                                                                                                |                   arxiv                   |                    [DePrompt: Desensitization and Evaluation of Personal Identifiable Information in Large Language Model Prompts](https://arxiv.org/abs/2408.08930)                     |        **Personal Identifiable Information**&**Prompt Desensitization**&**Privacy Protection**        |
| 24.08 |                                                                                       Huawei Technologies Canada Co. Ltd                                                                                        |                   arxiv                   |                             [Tracing Privacy Leakage of Language Models to Training Data via Adjusted Influence Functions](https://arxiv.org/abs/2408.10468)                             |                              **Privacy Leakage**&**Influence Functions**                              |
| 24.08 |                                                                                           Chinese Academy of Sciences                                                                                           |                   arxiv                   |       [Towards Robust Knowledge Unlearning: An Adversarial Framework for Assessing and Improving Unlearning Robustness in Large Language Models](https://arxiv.org/abs/2408.10682)       |              **Knowledge Unlearning**&**Adversarial Attacks**&**Unlearning Robustness**               |
| 24.08 |                                                                                          Universit√§tsklinikum Erlangen                                                                                          |                   arxiv                   |         [Fine-Tuning a Local LLaMA-3 Large Language Model for Automated Privacy-Preserving Physician Letter Generation in Radiation Oncology](https://arxiv.org/abs/2408.10715)          |                        **Radiation Oncology**&**Data Privacy**&**Fine-Tuning**                        |
| 24.08 |                                                                                       University of California, Berkeley                                                                                        |                   arxiv                   |                                               [LLM-PBE: Assessing Data Privacy in Large Language Models](https://arxiv.org/abs/2408.12787)                                               |                                     **Data Privacy**&**Toolkit**                                      |
| 24.08 |                                                                                    Mitsubishi Electric Research Laboratories                                                                                    |                   arxiv                   |                         [Forget to Flourish: Leveraging Machine-Unlearning on Pretrained Language Models for Privacy Leakage](https://arxiv.org/abs/2408.17354)                          |                **Privacy Leakage**&**Model-Unlearning**&**Pretrained Language Models**                |
| 24.09 |                                                                                               Stanford University                                                                                               |                   arxiv                   |                                     [PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in Action](https://arxiv.org/abs/2409.00138)                                      |                        **Privacy Norm Awareness**&**Privacy Risk Evaluation**                         |
| 24.09 |                                                                                                    ByteDance                                                                                                    |                   arxiv                   |                         [How Privacy-Savvy Are Large Language Models? A Case Study on Compliance and Privacy Technical Review](https://arxiv.org/abs/2409.02375)                         |        **Privacy Compliance**&**Privacy Information Extraction**&**Technical Privacy Review**         |
| 24.09 |                                                                                                       MIT                                                                                                       |                 COLM 2024                 |                                                   [Unforgettable Generalization in Language Models](https://arxiv.org/abs/2409.02228)                                                    |                          **Unlearning**&**Generalization**&**Random Labels**                          |
| 24.09 |                                                                             Anhui University of Technology, University of Cambridge                                                                             |                   arxiv                   |                              [On the Weaknesses of Backdoor-based Model Watermarks: An Information-theoretic Perspective](https://arxiv.org/abs/2409.06130)                              |                  **Model Watermarking**&**Backdoor Attacks**&**Information Theory**                   |
| 24.09 |                                                                                               Bilkent University                                                                                                |                   arxiv                   |                       [Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large Language Models on Generated Data](https://arxiv.org/abs/2409.11423)                        |                       **Privacy Risks**&**PII**&**Membership Inference Attack**                       |
| 24.09 |                                                                                        National University of Singapore                                                                                         |                   arxiv                   |                                 [Context-Aware Membership Inference Attacks against Pre-trained Large Language Models](https://arxiv.org/abs/2409.13745)                                 |                         **Membership Inference Attack**&**Context-Awareness**                         |
| 24.09 |                                                                                             George Mason University                                                                                             |                   arxiv                   |                                     [Unlocking Memorization in Large Language Models with Dynamic Soft Prompting](https://arxiv.org/abs/2409.13853)                                      |                              **Memorization**&**Dynamic Soft Prompting**                              |
| 24.09 |                                                                               CAS Key Lab of Network Data Science and Technology                                                                                |                EMNLP 2024                 |                             [Pretraining Data Detection for Large Language Models: A Divergence-based Calibration Method](https://arxiv.org/abs/2409.14781)                              |          **Pretraining Data Detection**&**Divergence Calibration**&**Membership Inference**           |
| 24.09 |                                                                                               √âcole Polytechnique                                                                                               |                   arxiv                   |                                    [Predicting and Analyzing Memorization Within Fine-Tuned Large Language Models](https://arxiv.org/abs/2409.18858)                                     |                             **Memorization**&**Fine-tuning**&**Privacy**                              |
| 24.10 |                                                                                        University of Southern California                                                                                        |                   arxiv                   |                                          [ADAPTIVELY PRIVATE NEXT-TOKEN PREDICTION OF LARGE LANGUAGE MODELS](https://arxiv.org/abs/2410.02016)                                           |                  **Differential Privacy**&**Next-Token Prediction**&**Adaptive DP**                   |
| 24.10 |                                                                                              University of Chicago                                                                                              |                   arxiv                   |                                                      [MITIGATING MEMORIZATION IN LANGUAGE MODELS](https://arxiv.org/abs/2410.02159)                                                      |                              **Memorization Mitigation**&**Unlearning**                               |
| 24.10 |                                                                                             University of Groningen                                                                                             |                   arxiv                   |                                             [Undesirable Memorization in Large Language Models: A Survey](https://arxiv.org/abs/2410.02650)                                              |                                     **Memorization**&**Privacy**                                      |
| 24.10 |                                                                               University of California, Santa Barbara, AWS AI Lab                                                                               |                   arxiv                   |                                    [Detecting Training Data of Large Language Models via Expectation Maximization](https://arxiv.org/abs/2410.07582)                                     |                     **Membership Inference Attack**&**Expectation Maximization**                      |
| 24.10 |                                                                                   Queen‚Äôs University, J.P. Morgan AI Research                                                                                   |            EMNLP 2024 Findings            |                               [Fine-Tuning Language Models with Differential Privacy through Adaptive Noise Allocation](https://arxiv.org/abs/2410.02912)                                |                        **Differential Privacy**&**Adaptive Noise Allocation**                         |
| 24.10 |                                                                   King Abdullah University of Science and Technology, Ruhr University Bochum                                                                    |                EMNLP 2024                 |                                              [Private Language Models via Truncated Laplacian Mechanism](https://arxiv.org/abs/2410.08027)                                               |             **Differential Privacy**&**Word Embedding**&**Truncated Laplacian Mechanism**             |
| 24.10 |                                                                               Purdue University, Georgia Institute of Technology                                                                                |                   arxiv                   |                                [Privately Learning from Graphs with Applications in Fine-tuning Large Language Models](https://arxiv.org/abs/2410.08299)                                 |                **Privacy-preserving learning**&**Graph learning**&**Fine-tuning LLMs**                |
| 24.10 |                                                                                             Northeastern University                                                                                             |                   arxiv                   |            [Rescriber: Smaller-LLM-Powered User-Led Data Minimization for Navigating Privacy Trade-offs in LLM-Based Conversational Agents](https://arxiv.org/abs/2410.11876)            |                                          **Privacy**&**PII**                                          |
| 24.10 |                                                         The Pennsylvania State University, University of California Los Angeles, University of Virginia                                                         |                   arxiv                   |                                    [Data-adaptive Differentially Private Prompt Synthesis for In-Context Learning](https://arxiv.org/abs/2410.12085)                                     |            **Differential privacy**&**In-context learning**&**Synthetic data generation**             |
| 24.10 |             Nanjing University of Science and Technology, Western Sydney University, Institute of Information Engineering (Chinese Academy of Sciences), CSIRO‚Äôs Data61, The University of Chicago              |                   arxiv                   |                                 [Reconstruction of Differentially Private Text Sanitization via Large Language Models](https://arxiv.org/abs/2410.12443)                                 |                 **Differential Privacy**&**Reconstruction attacks**&**Privacy risks**                 |
| 24.10 |                                                                                       University of California San Diego                                                                                        |                   arxiv                   |                                                [Imprompter: Tricking LLM Agents into Improper Tool Use](https://arxiv.org/abs/2410.14923)                                                |                      **Prompt Injection**&**LLM Agents**&**Adversarial Prompts**                      |
| 24.10 |                                                                                       University of California, San Diego                                                                                       |                   arxiv                   |                                                 [Evaluating Deep Unlearning in Large Language Models](https://arxiv.org/abs/2410.15153)                                                  |                               **Deep Unlearning**&**Knowledge Removal**                               |
| 24.10 |                                                                                        The Pennsylvania State University                                                                                        |                   arxiv                   |                            [Does Your LLM Truly Unlearn? An Embarrassingly Simple Approach to Recover Unlearned Knowledge](https://arxiv.org/abs/2410.16454)                             |                    **Machine Unlearning**&**Knowledge Recovery**&**Quantization**                     |
| 24.10 |                                                                                                 Google DeepMind                                                                                                 |                   arxiv                   |                                             [Remote Timing Attacks on Efficient Language Model Inference](https://arxiv.org/abs/2410.17175)                                              |                        **Timing Attacks**&**Efficient Inference**&**Privacy**                         |
| 24.10 |                                                                                         Huawei Technologies D√ºsseldorf                                                                                          |                   arxiv                   |                                       [PSY: Posterior Sampling Based Privacy Enhancer in Large Language Models](https://arxiv.org/abs/2410.18824)                                        |                **Privacy Enhancing Technology**&**Posterior Sampling**&**LLM Privacy**                |
| 24.10 |                                                                                             Northwestern University                                                                                             |                   arxiv                   |                         [LanFL: Differentially Private Federated Learning with Large Language Models using Synthetic Samples](https://arxiv.org/abs/2410.19114)                          |                            **Federated Learning**&**Differential Privacy**                            |
| 24.11 |                                                                                        Technical University of Darmstadt                                                                                        |                   arxiv                   |                                [Scaling Up Membership Inference: When and How Attacks Succeed on Large Language Models](https://arxiv.org/abs/2411.00154)                                |                                 **Membership Inference**&**Privacy**                                  |
| 24.11 |                                                                                              University of Toronto                                                                                              |                   arxiv                   |                                            [Privacy Risks of Speculative Decoding in Large Language Models](https://arxiv.org/abs/2411.01076)                                            |                     **Privacy**&**Speculative Decoding**&**Side-channel Attacks**                     |
| 24.11 |                                                                                             Northeastern University                                                                                             |                   arxiv                   |         [Can Humans Oversee Agents to Prevent Privacy Leakage? A Study on Privacy Awareness, Preferences, and Trust in Language Model Agents](https://arxiv.org/abs/2411.01344)          |                **Privacy Awareness**&**Trust in LLMs**&**Privacy Leakage Prevention**                 |
| 24.11 |                                                                                               Guangxi University                                                                                                |                   arxiv                   |                              [A Practical and Privacy-Preserving Framework for Real-World Large Language Model Services](https://arxiv.org/abs/2411.01471)                               |                **Privacy-Preserving Framework**&**Blind Signatures**&**LLM Services**                 |
| 24.11 |                                                                                       University of Massachusetts Amherst                                                                                       |                   arxiv                   |                                       [Data Extraction Attacks in Retrieval-Augmented Generation via Backdoors](https://arxiv.org/abs/2411.01705)                                        |              **Data Extraction**&**Backdoor Attacks**&**Retrieval-Augmented Generation**              |
| 24.11 |                                                                                                      EPFL                                                                                                       |               NeurIPS 2024                |                                          [Membership Inference Attacks against Large Vision-Language Models](https://arxiv.org/abs/2411.02902)                                           |                    **Membership Inference**&**Vision-Language Models**&**Privacy**                    |
| 24.11 |                                                                                             University of Helsinki                                                                                              |  NeurIPS 2024 Foundation Model Workshop   |                                          [Differentially Private Continual Learning using Pre-Trained Models](https://arxiv.org/abs/2411.04680)                                          |                **Differential Privacy**&**Continual Learning**&**Pre-trained Models**                 |
| 24.11 |                                                                                               Zhejiang University                                                                                               |                   arXiv                   |                                         [Mitigating Privacy Risks in LLM Embeddings from Embedding Inversion](https://arxiv.org/abs/2411.05034)                                          |                            **Embedding Inversion**&**Privacy Protection**                             |
| 24.11 |                                                                                              University of Toronto                                                                                              |                   arxiv                   |                                                      [On the Privacy Risk of In-context Learning](https://arxiv.org/abs/2411.10512)                                                      |               **In-context Learning**&**Privacy Risk**&**Membership Inference Attack**                |
| 24.11 |                                                                                                Fudan University                                                                                                 |                   arxiv                   |               [RAG-Thief: Scalable Extraction of Private Data from Retrieval-Augmented Generation Applications with Agent-based Attacks](https://arxiv.org/abs/2411.14110)               |       **Privacy Attacks**&**Retrieval-Augmented Generation**&**Automated Adversarial Attacks**        |
| 24.11 |                                                                                              Texas A&M University                                                                                               |                 NDSS 2025                 |                                                   [LLMPirate: LLMs for Black-box Hardware IP Piracy](https://arxiv.org/abs/2411.16111)                                                   |                   **LLM-based Attack**&**Hardware IP Piracy**&**Piracy Detection**                    |
| 24.11 |                                                                       Imperial College London, Flashbots, Technical University of Munich                                                                        |                   arxiv                   |                 [Efficient and Private: Memorisation under Differentially Private Parameter-Efficient Fine-Tuning in Language Models](https://arxiv.org/abs/2411.15831)                  |           **Differential Privacy**&**Parameter-Efficient Fine-Tuning**&**Privacy Leakage**            |
| 24.12 |                                                                                   University of North Carolina at Chapel Hill                                                                                   |                   arxiv                   |                                     [A Novel Compact LLM Framework for Local, High-Privacy EHR Data Applications](https://arxiv.org/abs/2412.02868)                                      |                        **Privacy-Preserving LLMs**&**Healthcare**&**EHR Data**                        |
| 24.12 |                                                                                 CISPA Helmholtz Center for Information Security                                                                                 |                   arxiv                   |                                [DP-2Stage: Adapting Language Models as Differentially Private Tabular Data Generators](https://arxiv.org/abs/2412.02467)                                 |                         **Differential Privacy**&**Tabular Data Generation**                          |
| 24.12 |                                                                                               New York University                                                                                               |                   arxiv                   |                                              [TruncFormer: Private LLM Inference Using Only Truncations](https://arxiv.org/abs/2412.01042)                                               |                            **Private Inference**&**LLMs**&**Truncations**                             |
| 24.12 |                                                                             Dalhousie University, Canada; Vector Institute, Canada                                                                              |                   arxiv                   |                                       [Can Large Language Models Be Privacy-Preserving and Fair Medical Coders?](https://arxiv.org/abs/2412.05533)                                       |                     **Differential Privacy**&**ICD Classification**&**Fairness**                      |
| 24.12 |                                                                                       UC Santa Barbara, UC Berkeley, Meta                                                                                       |                   arxiv                   |                                             [PrivAgent: Agentic-based Red-teaming for LLM Privacy Leakage](https://arxiv.org/abs/2412.05734)                                             |                      **Privacy Leakage**&**Red-teaming**&**Adversarial Prompts**                      |
| 24.12 |                                                                                       University of California San Diego                                                                                        |                   arxiv                   |                              [Privacy-Preserving Large Language Models: Mechanisms, Applications, and Future Directions](https://arxiv.org/abs/2412.06113)                               |           **Privacy-Preserving Mechanisms**&**Differential Privacy**&**Federated Learning**           |
| 24.12 |                                                                                             Sun Yat-Sen University                                                                                              |                   arxiv                   |                                 [MemHunter: Automated and Verifiable Memorization Detection at Dataset-scale in LLMs](https://arxiv.org/abs/2412.07261)                                  |                             **Memorization Detection**&**Privacy Risks**                              |
| 24.12 |                               Nanyang Technological University, Singapore University of Technology and Design, Zhejiang University, Chengdu University of Information Technology                                |                   arxiv                   |                          [MRP-LLM: Multitask Reflective Large Language Models for Privacy-Preserving Next POI Recommendation](https://arxiv.org/abs/2412.07796)                          |               **Privacy-Preserving**&**Next POI Recommendation**&**Multitask Learning**               |
| 24.12 |                                                                                        Nanyang Technological University                                                                                         |                   arxiv                   |                                                      [A Survey on Private Transformer Inference](https://arxiv.org/abs/2412.08145)                                                       |                    **Transformer Models**&**Private Inference**&**Data Security**                     |
| 24.12 |                                                                                             University of Sheffield                                                                                             |                   arxiv                   |                                            [How Private are Language Models in Abstractive Summarization?](https://arxiv.org/abs/2412.12040)                                             |                **Privacy Preservation**&**Abstractive Summarization**&**PII Leakage**                 |
| 24.12 |                                                                                  University of Science and Technology of China                                                                                  |                   arxiv                   |                                                [RemoteRAG: A Privacy-Preserving LLM Cloud RAG Service](https://arxiv.org/abs/2412.12775)                                                 |            **Privacy-Preserving RAG**&**Differential Privacy**&**Embedding Perturbation**             |
| 24.12 |                                                                                               Lanzhou University                                                                                                |                ICASSP 2025                |                          [EM-MIAs: Enhancing Membership Inference Attacks in Large Language Models through Ensemble Modeling](https://arxiv.org/abs/2412.17249)                          |              **Membership Inference Attacks**&**Privacy Leakage**&**Ensemble Modeling**               |
| 24.12 |                                        Hunan University, National University of Defense Technology, University of Science and Technology of China, Lancaster University                                         |                   arxiv                   |                           [Accelerating Private Large Transformers Inference through Fine-grained Collaborative Computation](https://arxiv.org/abs/2412.16537)                           |                      **Privacy-Preserving Inference**&**Homomorphic Encryption**                      |
| 24.12 |                                   Hokkaido University, China University of Mining and Technology, Institute of Science Tokyo, Hong Kong University of Science and Technology                                    |                   arxiv                   |                                [Privacy in Fine-tuning Large Language Models: Attacks, Defenses, and Future Directions](https://arxiv.org/abs/2412.16504)                                |                             **Privacy**&**Fine-tuning LLMs**&**Attacks**                              |
| 24.12 |                                                                            Zhejiang Laboratory, The Chinese University of Hong Kong                                                                             |                   arxiv                   |                      [DR-ENCODER: Encode Low-Rank Gradients with Random Prior for Large Language Models Differentially Privately](https://arxiv.org/abs/2412.17053)                      |               **Differential Privacy**&**Gradient Compression**&**Federated Learning**                |
| 24.12 | Yandex, Moscow Institute of Physics and Technology (MIPT), HSE University, Skoltech, Together AI, Ivannikov Institute for System Programming of the Russian Academy of Sciences (ISP RAS), Innopolis University |                   arxiv                   |                                  [Label Privacy in Split Learning for Large Models with Parameter-Efficient Training](https://arxiv.org/abs/2412.16669)                                  |             **Label Privacy**&**Split Learning**&**Parameter-Efficient Training (PEFT)**              |
| 24.12 |                                                                         Institute of Computing Technology, Chinese Academy of Sciences                                                                          |                   arxiv                   |                           [Multi-P2A: A Multi-perspective Benchmark on Privacy Assessment for Large Vision-Language Models](https://arxiv.org/abs/2412.19496)                            |                 **Privacy Assessment**&**Vision-Language Models**&**Privacy Leakage**                 |
| 24.12 |                                                                                              University of Alberta                                                                                              |                   arxiv                   |              [SafeSynthDP: Leveraging Large Language Models for Privacy-Preserving Synthetic Data Generation Using Differential Privacy](https://arxiv.org/abs/2412.20641)               |           **Differential Privacy**&**Synthetic Data Generation**&**Large Language Models**            |
| 25.01 |                                                                                               Tsinghua University                                                                                               |                   arxiv                   |                            [Shifting-Merging: Secure, High-Capacity and Efficient Steganography via Large Language Models](https://arxiv.org/abs/2501.00786)                             |            **Secure Steganography**&**High-Capacity Embedding**&**Large Language Models**             |
| 25.01 |                                                                                               Tsinghua University                                                                                               |                   arxiv                   |                                 [A Survey on Responsible LLMs: Inherent Risk, Malicious Use, and Mitigation Strategy](https://arxiv.org/abs/2501.09431)                                  |                          **Responsible LLMs**&**Privacy**&**Hallucination**                           |
| 25.01 |                                                                                       Ben-Gurion University of the Negev                                                                                        |                   arxiv                   |                     [Tag&Tab: Pretraining Data Detection in Large Language Models Using Keyword-Based Membership Inference Attack](https://arxiv.org/abs/2501.08454)                     |          **Membership Inference Attacks**&**Pretraining Data Detection**&**Keyword Entropy**          |
| 25.01 |                                                                                               New York University                                                                                               |            AAAI PPAI Workshop             |                                                      [Entropy-Guided Attention for Private LLMs](https://arxiv.org/abs/2501.03489)                                                       |       **Privacy-Preserving Inference**&**Entropy-Guided Attention**&**Nonlinear Optimization**        |
| 25.01 |                                                                        Institute of Information Engineering, Chinese Academy of Sciences                                                                        |                   arxiv                   |                   [RAG-WM: An Efficient Black-Box Watermarking Approach for Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2501.05249)                   |            **Black-Box Watermarking**&**Retrieval-Augmented Generation**&**IP Protection**            |
| 25.01 |                                                                                         Case Western Reserve University                                                                                         |                   arxiv                   |                                  [Navigating the Designs of Privacy-Preserving Fine-tuning for Large Language Models](https://arxiv.org/abs/2501.04323)                                  |      **Privacy-Preserving Fine-tuning**&**Data Reconstruction Attack Defense**&**GuardedTuning**      |
| 25.01 |                                                                                        Communication University of China                                                                                        |                   arxiv                   |                    [Practical Secure Inference Algorithm for Fine-tuned Large Language Model Based on Fully Homomorphic Encryption](https://arxiv.org/abs/2501.01672)                    |     **Fully Homomorphic Encryption**&**Privacy-Preserving Techniques**&**Large Language Models**      |
| 25.01 |                                                                                              University at Albany                                                                                               |                   arxiv                   |                   [LegalGuardian: A Privacy-Preserving Framework for Secure Integration of Large Language Models in Legal Practice](https://arxiv.org/abs/2501.10915)                    |               **Legal Practice**&**Privacy Preservation**&**Named Entity Recognition**                |
| 25.01 |                                                                                                  IBM Research                                                                                                   |     AAAI 2025 Deployable AI Workshop      |                                                 Deploying Privacy Guardrails for LLMs: A Comparative Analysis of Real-World Applications                                                 |                        **Privacy Guardrails**&**PII Detection**&**Compliance**                        |
| 25.01 |                                                                                         Delft University of Technology                                                                                          |                   arxiv                   |                [How Much Do Code Language Models Remember? An Investigation on Data Extraction Attacks before and after Fine-tuning](https://arxiv.org/abs/2501.17501v1)                 |                 **Code Language Models**&**Data Extraction Attacks**&**Memorization**                 |
| 25.01 |                                                    Technical University of Darmstadt, Max Planck Institute for Intelligent Systems, University of Copenhagen                                                    |                 ICLR 2025                 |                                       [PSA: Differentially Private Steering for Large Language Model Alignment](https://arxiv.org/abs/2501.18532)                                        |            **Differential Privacy**&**Activation Editing**&**Membership Inference Attack**            |
| 25.01 |                                                                                                 Google Research                                                                                                 |                   arxiv                   |                                               [Scaling Laws for Differentially Private Language Models](https://arxiv.org/abs/2501.18914)                                                |                      **Differential Privacy**&**Scaling Laws**&**LLM Training**                       |
| 25.02 |                                                                                               Zhejiang University                                                                                               |                   arxiv                   |                                     [SecPE: Secure Prompt Ensembling for Private and Robust Large Language Models](https://arxiv.org/abs/2502.00847)                                     |             **Privacy-Preserving LLMs**&**Prompt Ensembling**&**Homomorphic Encryption**              |
| 25.02 |                                                                                               Columbia University                                                                                               |                   arxiv                   |                                    [Skewed Memorization in Large Language Models: Quantification and Decomposition](https://arxiv.org/abs/2502.01187)                                    |                    **LLM Memorization**&**Privacy Risks**&**Fine-tuning Analysis**                    |
| 25.02 |                                                                                       University of California, San Diego                                                                                       |                   arxiv                   |                                 [Robust and Secure Code Watermarking for Large Language Models via ML/Crypto Codesign](https://arxiv.org/abs/2502.02068)                                 |                   **Code Watermarking**&**LLM Security**&**Zero-Knowledge Proofs**                    |
| 25.02 |                                                                                         University of British Columbia                                                                                          |                   arxiv                   |                          [SimMark: A Robust Sentence-Level Similarity-Based Watermarking Algorithm for Large Language Models](https://arxiv.org/abs/2502.02787)                          |             **LLM Watermarking**&**Sentence-Level Similarity**&**Adversarial Robustness**             |
| 25.02 |                                                                                     Tune Insight SA, EPFL, Yale University                                                                                      |                   arxiv                   |                                     [Mitigating Unintended Memorization with LoRA in Federated Learning for LLMs](https://arxiv.org/abs/2502.05087)                                      |                       **Federated Learning**&**Privacy-Preserving Techniques**                        |
| 25.02 |                                                                               Beijing University of Posts and Telecommunications                                                                                |                   arxiv                   |                                   [Mitigating Sensitive Information Leakage in LLMs4Code through Machine Unlearning](https://arxiv.org/abs/2502.05739)                                   |                      **Machine Unlearning**&**Privacy Protection**&**LLMs4Code**                      |
| 25.02 |                                                                                           University of South Florida                                                                                           |                   arxiv                   |                [An Interactive Framework for Implementing Privacy-Preserving Federated Learning: Experiments on Large Language Models](https://arxiv.org/abs/2502.08008)                 |                  **Federated Learning**&**Differential Privacy**&**LLM Fine-Tuning**                  |
| 25.02 |                                                                                                     Amazon                                                                                                      |                   arxiv                   |                                   [HAS MY SYSTEM PROMPT BEEN USED? LARGE LANGUAGE MODEL PROMPT MEMBERSHIP INFERENCE](https://arxiv.org/abs/2502.09974)                                   |             **Prompt Membership Inference**&**LLM Security**&**Statistical Verification**             |
| 25.02 |                                                                                              University of Florida                                                                                              |                   arxiv                   |                                       [Dataset Protection via Watermarked Canaries in Retrieval-Augmented LLMs](https://arxiv.org/abs/2502.10673)                                        |                 **Dataset Protection**&**Watermarking**&**Retrieval-Augmented LLMs**                  |
| 25.02 |                                                                         The Hong Kong University of Science and Technology (Guangzhou)                                                                          |                   arxiv                   |                       [MMUNLEARNER: Reformulating Multimodal Machine Unlearning in the Era of Multimodal Large Language Models](https://arxiv.org/abs/2502.11051)                        |                   **Multimodal Machine Unlearning**&**MLLMs**&**Gradient Descent**                    |
| 25.02 |                                                                                Institute of Software Chinese Academy of Sciences                                                                                |                   arxiv                   |                     [Mimicking the Familiar: Dynamic Command Generation for Information Theft Attacks in LLM Tool-Learning System](https://arxiv.org/abs/2502.11358)                     |          **Information Theft Attacks**&**LLM Tool-Learning**&**Dynamic Command Generation**           |
| 25.02 |                                                                                               Zhejiang University                                                                                               |                   arxiv                   |                                        [R.R.: Unveiling LLM Training Privacy through Recollection and Ranking](https://arxiv.org/abs/2502.12658)                                         |                      **Privacy Leakage**&**PII Reconstruction**&**LLM Security**                      |
| 25.02 |                                                                                            Michigan State University                                                                                            |                   arXiv                   |                                                     [Unveiling Privacy Risks in LLM Agent Memory](https://arxiv.org/abs/2502.13172)                                                      |                         **LLM Agent**&**Privacy Risks**&**Memory Extraction**                         |
| 25.02 |                                                                                                MPI‚ÄìSWS, Germany                                                                                                 |                   arXiv                   |                            [Revisiting Privacy, Utility, and Efficiency Trade-offs when Fine-Tuning Large Language Models](https://arxiv.org/abs/2502.13313)                             |                          **Privacy**&**Utility**&**Fine-Tuning Efficiency**                           |
| 25.02 |                                                                                                 Apart Research                                                                                                  |        AAAI 2025 Workshop DATASAFE        |                                   [Evaluating Precise Geolocation Inference Capabilities of Vision Language Models](https://arxiv.org/abs/2502.14412)                                    |                **Vision-Language Models**&**Geolocation Inference**&**Privacy Risks**                 |
| 25.02 |                                                                                  Huazhong University of Science and Technology                                                                                  |                   arxiv                   |                                   [Merger-as-a-Stealer: Stealing Targeted PII from Aligned LLMs with Model Merging](https://arxiv.org/abs/2502.16094)                                    |                   **Model Merging**&**PII Extraction**&**Security Vulnerabilities**                   |
| 25.02 |                                                                                             Northeastern University                                                                                             |                   arxiv                   |                            [Privacy Ripple Effects from Adding or Removing Personal Information in Language Model Training](https://arxiv.org/abs/2502.15680)                            |                      **PII Memorization**&**LLM Privacy**&**Training Dynamics**                       |
| 25.02 |                                                                                                    Microsoft                                                                                                    |                   arxiv                   |                          [Obliviate: Efficient Unmemorization for Protecting Intellectual Property in Large Language Models](https://arxiv.org/abs/2502.15010)                           |           **Unmemorization**&**Intellectual Property Protection**&**Large Language Models**           |
| 25.02 |                                                                                                Yonsei University                                                                                                |                   arxiv                   |                                 [Marking Code Without Breaking It: Code Watermarking for Detecting LLM-Generated Code](https://arxiv.org/abs/2502.18851)                                 |                     **Code Watermarking**&**LLM Detection**&**Software Security**                     |
| 25.02 |                                                                                      South China University of Technology                                                                                       |                   arxiv                   |                         [RewardDS: Privacy-Preserving Fine-Tuning for Large Language Models via Reward Driven Data Synthesis](https://arxiv.org/abs/2502.18517)                          |           **Privacy-Preserving Fine-Tuning**&**Synthetic Data Generation**&**Reward Model**           |
| 25.02 |                                                                                                  IBM Research                                                                                                   |                   arxiv                   |                     [Protecting Users From Themselves: Safeguarding Contextual Privacy in Interactions with Conversational Agents](https://arxiv.org/abs/2502.18509)                     |             **Contextual Privacy**&**Conversational Agents**&**User Privacy Protection**              |
| 25.02 |                                                                                     Indian Institute of Technology Roorkee                                                                                      |                   arxiv                   |                                         [Pruning as a Defense: Reducing Memorization in Large Language Models](https://arxiv.org/abs/2502.15796)                                         |                        **Pruning**&**Memorization Reduction**&**LLM Privacy**                         |
| 25.02 |                                                                                                Emory University                                                                                                 |                   arxiv                   |        [Tokens for Learning, Tokens for Unlearning: Mitigating Membership Inference Attacks in Large Language Models via Dual-Purpose Training](https://arxiv.org/abs/2502.19726)        |            **Membership Inference Attacks**&**Privacy Defense**&**Dual-Purpose Training**             |
| 25.02 |                                                                                            Seoul National University                                                                                            |                   arxiv                   |                     [FAITHUN: Toward Faithful Forgetting in Language Models by Investigating the Interconnectedness of Knowledge](https://arxiv.org/abs/2502.19207)                      |             **Faithful Unlearning**&**Knowledge Interconnectedness**&**Privacy in LLMs**              |
| 25.02 |                                                                                                 Duke University                                                                                                 |                 ICLR 2025                 |                    [Proactive Privacy Amnesia for Large Language Models: Safeguarding PII with Negligible Impact on Model Utility](https://arxiv.org/abs/2502.17591)                     |       **Privacy Protection**&**Machine Unlearning**&**Personal Identifiable Information (PII)**       |
| 25.03 |                                                           Xidian University, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences                                                            |                   arXiv                   |                        [PriFFT: Privacy-preserving Federated Fine-tuning of Large Language Models via Function Secret Sharing](https://arxiv.org/abs/2503.03146)                         |    **Privacy-preserving Federated Learning**&**Function Secret Sharing**&**Large Language Models**    |
| 25.03 |                                                                       Ben-Gurion University of the Negev, Nuclear Research Center ‚Äì Negev                                                                       |                   arXiv                   |                                                     [Token-Level Privacy in Large Language Models](https://arxiv.org/abs/2503.03652)                                                     |              **Token-Level Privacy**&**Large Language Models**&**Differential Privacy**               |
| 25.03 |                                                               The Hong Kong University of Science and Technology (Guangzhou), Tsinghua University                                                               | Workshop on GenAI Watermarking, ICLR 2025 |                              [MARK YOUR LLM: DETECTING THE MISUSE OF OPEN-SOURCE LARGE LANGUAGE MODELS VIA WATERMARKING](https://arxiv.org/abs/2503.04636)                               |                      **Open-Source LLMs**&**Watermarking**&**Misuse Detection**                       |
| 25.03 |                                                                                      University of Maryland, College Park                                                                                       |                   arxiv                   |                                              [Mitigating Memorization in LLMs using Activation Steering](https://arxiv.org/abs/2503.06040)                                               |              **Activation Steering**&**Memorization Mitigation**&**Sparse Autoencoders**              |
| 25.03 |                                                                                                Wuhan University                                                                                                 |                   arxiv                   |                                           [Privacy-Enhancing Paradigms within Federated Multi-Agent Systems](https://arxiv.org/abs/2503.08175)                                           |                        **Federated MAS**&**Privacy Protection**&**EPEAgents**                         |
| 25.03 |                                                                                                  FAIR at Meta                                                                                                   |                   arxiv                   |                                            [AgentDAM: Privacy Leakage Evaluation for Autonomous Web Agents](https://arxiv.org/abs/2503.09780)                                            |                         **LLM Agents**&**Privacy Leakage**&**Web Navigation**                         |
| 25.03 |                                                                                                      UCLA                                                                                                       |           Blogpost @ ICLR 2025            |                                    [Peeking Behind Closed Doors: Risks of LLM Evaluation by Private Data Curators](https://arxiv.org/abs/2503.04756)                                     |                     **Private Evaluation**&**LLM-as-a-Judge**&**Evaluation Bias**                     |
| 25.03 |                                                                                              Princeton University                                                                                               |                 ICLR 2025                 |                                                      [Privacy Auditing of Large Language Models](https://arxiv.org/abs/2503.06808)                                                       |             **Privacy Auditing**&**Membership Inference Attack**&**Differential Privacy**             |
| 25.03 |                                                                                          Huawei Munich Research Center                                                                                          |                   arxiv                   |                        [PrivacyScalpel: Enhancing LLM Privacy via Interpretable Feature Intervention with Sparse Autoencoders](https://arxiv.org/abs/2503.11232)                         |              **Privacy in LLMs**&**Sparse Autoencoders**&**Feature-Level Intervention**               |
| 25.03 |                                                                                               Zhejiang University                                                                                               |                   arxiv                   |                                [Safeguarding LLM Embeddings in End-Cloud Collaboration via Entropy-Driven Perturbation](https://arxiv.org/abs/2503.12896)                                |           **Embedding Privacy**&**End-Cloud Collaboration**&**Entropy-based Perturbation**            |
| 25.03 |                                                                                                 RMIT University                                                                                                 |                   arxiv                   |                                                   [Deep Contrastive Unlearning for Language Models](https://arxiv.org/abs/2503.14900)                                                    |                   **Machine Unlearning**&**Contrastive Learning**&**Data Privacy**                    |
| 25.03 |                                                                                             University of Maryland                                                                                              |                   arxiv                   |                                  [Prada: Black-Box LLM Adaptation with Private Data on Resource-Constrained Devices](https://arxiv.org/abs/2503.14932)                                   |              **Black-Box Adaptation**&**Data Privacy**&**Resource-Constrained Devices**               |
| 25.03 |                                                                                            Seoul National University                                                                                            |                   arxiv                   |                                         [Prompt Flow Integrity to Prevent Privilege Escalation in LLM Agents](https://arxiv.org/abs/2503.15547)                                          |                   **LLM Agents**&**Privilege Escalation**&**Prompt Flow Integrity**                   |
| 25.03 |                                                                               Beijing University of Posts and Telecommunications                                                                                |                   arxiv                   |                                              [Privacy-Aware RAG: Secure and Isolated Knowledge Retrieval](https://arxiv.org/abs/2503.15548)                                              |              **RAG Security**&**Encrypted Embeddings**&**Privacy-Preserving Retrieval**               |
| 25.03 |                                                                                Shanghai Jiao Tong University, University of Kent                                                                                |                   arxiv                   |                         [Using LLMs for Automated Privacy Policy Analysis: Prompt Engineering, Fine-Tuning and Explainability](https://arxiv.org/abs/2503.16516)                         |                     **Privacy Policy**&**Prompt Engineering**&**Explainability**                      |
| 25.03 |                                                                                  Henry Ford Health, Michigan State University                                                                                   |                   arxiv                   |                      [Autonomous Radiotherapy Treatment Planning Using DOLA: A Privacy-Preserving, LLM-Based Optimization Agent](https://arxiv.org/abs/2503.17553)                       |        **Radiotherapy Planning**&**LLM Agent**&**RAG**&**Reinforcement Learning**&**Privacy**         |
| 25.03 |                                                                        The Hong Kong Polytechnic University, Institute of Science Tokyo                                                                         |                   arxiv                   |                                             [Membership Inference Attacks on Large-Scale Models: A Survey](https://arxiv.org/abs/2503.19338)                                             |                **Membership Inference**&**LLMs Privacy**&**Multimodal Model Security**                |
| 25.03 |                                                                    University College London, King‚Äôs College Hospital, King‚Äôs College London                                                                    |                   arxiv                   |                                               [Clean & Clear: Feasibility of Safe LLM Clinical Guidance](https://arxiv.org/abs/2503.20953)                                               |                 **Clinical Guidance**&**Hallucination Mitigation**&**Healthcare LLM**                 |
| 25.03 |                                                                                             UL Research Institutes                                                                                              |                   arxiv                   |                              [Malicious and Unintentional Disclosure Risks in Large Language Models for Code Generation](https://arxiv.org/abs/2503.22760)                               |                 **Code Generation**&**Unintended Memorization**&**Disclosure Risks**                  |
| 25.04 |                                                                                               University of Oslo                                                                                                |                   arxiv                   |                             [Leaking LoRa: An Evaluation of Password Leaks and Knowledge Storage in Large Language Models](https://arxiv.org/abs/2504.00031)                             |                       **Cybersecurity**&**LoRA Fine-tuning**&**Model Editing**                        |
| 25.04 |                                                                               Beijing University of Posts and Telecommunications                                                                                |           USENIX Security 2025            |             [Whispering Under the Eaves: Protecting User Privacy Against Commercial and LLM-powered Automatic Speech Recognition Systems](https://arxiv.org/abs/2504.00858)              |                 **Audio Adversarial Examples**&**Speech Privacy**&**LLM-powered ASR**                 |
| 25.04 | Amazon AGI |                   arxiv                   | [SemEval-2025 Task 4: Unlearning Sensitive Content from Large Language Models](https://arxiv.org/abs/2504.02883) | **LLM Unlearning**&**Privacy**&**Benchmarking** |
| 25.04 | University of Michigan |                   arxiv                   | [Prœµœµmpt: Sanitizing Sensitive Prompts for LLMs](https://arxiv.org/abs/2504.05147) | **Prompt Privacy**&**Format-Preserving Encryption**&**Metric Differential Privacy** |
| 25.04 | Centre for Frontier AI Research |                   arxiv                   | [Position Paper: Rethinking Privacy in RL for Sequential Decision-making in the Age of LLMs](https://arxiv.org/abs/2504.11511) | **Privacy**&**Reinforcement Learning**&**Sequential Decision-making** |
| 25.04 | Korea University |                IJCNN 2025                 | [GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs](https://arxiv.org/abs/2504.12681) | **Machine Unlearning**&**LLM Privacy**&**Multi-domain Forgetting** |
| 25.04 | University of Chicago |                   arxiv                   | [How Private is Your Attention? Bridging Privacy with In-Context Learning](https://arxiv.org/abs/2504.16000) | **In-Context Learning**&**Differential Privacy**&**Attention Mechanism** |
| 25.04 | Universit√© de Toulouse |          CL4Health @ NAACL 2025           | [PatientDx: Merging Large Language Models for Protecting Data-Privacy in Healthcare](https://arxiv.org/abs/2504.17360) | **Model Merging**&**Healthcare Privacy**&**LLMs for EHR** |


## üíªPresentations & Talks


## üìñTutorials & Workshops

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |

## üì∞News & Articles
| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.11 | News  |Wild: GPT-3.5 leaked a random dude's photo in the output. | [link](https://twitter.com/thealexker/status/1719896871009694057) |

## üßë‚Äçüè´Scholars
