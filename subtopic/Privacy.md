# Privacy

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating with the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"


## üìëPapers

| Date  |                                                                                                    Institute                                                                                                    |                Publication                |                                                                                          Paper                                                                                           |                                               Keywords                                                |
|:-----:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-----------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-----------------------------------------------------------------------------------------------------:|
| 18.02 |                                                                                                  Google Brain                                                                                                   |           USENIX Security 2021            |        [The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks](https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting)        |                                       **Memorization**&**LSTM**                                       |
| 19.12 |                                                                                                    Microsoft                                                                                                    |                  CCS2020                  |                                [Analyzing Information Leakage of Updates to Natural Language Models](https://dl.acm.org/doi/abs/10.1145/3372297.3417880)                                 |                          **Privacy Leakage**&**Model Update**&**Duplicated**                          |
| 21.07 |                                                                                                 Google Research                                                                                                 |                  ACL2022                  |                                         [Deduplicating Training Data Makes Language Models Better](https://aclanthology.org/2022.acl-long.577/)                                          |                       **Privacy Protected**&**Deduplication**&**Memorization**                        |
| 21.10 |                                                                                                    Stanford                                                                                                     |                 ICLR2022                  |                                    [Large language models can be strong differentially private learners](https://openreview.net/forum?id=bVuP3ltATMz)                                    |                            **Differential Privacy**&**Gradient Clipping**                             |
| 22.02 |                                                                                                 Google Research                                                                                                 |                 ICLR2023                  |                                           [Quantifying Memorization Across Neural Language Models](https://openreview.net/forum?id=TatRHT_1cK)                                           |                                **Memorization**&**Verbatim Sequence**                                 |
| 22.02 |                                                                                                 UNC Chapel Hill                                                                                                 |                 ICML2022                  |                               [Deduplicating Training Data Mitigates Privacy Risks in Language Models](https://proceedings.mlr.press/v162/kandpal22a.html)                               |                            **Memorization**&**Deduplicate Training Data**                             |
| 22.05 |                                                                                                      UCSD                                                                                                       |                 EMNLP2022                 |                           [An Empirical Analysis of Memorization in Fine-tuned Autoregressive Language Models](https://aclanthology.org/2022.emnlp-main.119/)                            |                                  **Privacy Risks**&**Memorization**                                   |
| 22.05 |                                                                                                    Princeton                                                                                                    |                 NIPS2022                  | [Recovering Private Text in Federated Learning of Language Models](https://proceedings.neurips.cc/paper_files/paper/2022/hash/35b5c175e139bff5f22a5361270fce87-Abstract-Conference.html) |                               **Federated Learning**&**Gradient Based**                               |
| 22.05 |                                                                                   University of Illinois at Urbana-Champaign                                                                                    |            EMNLP2022(findings)            |                              [Are Large Pre-Trained Language Models Leaking Your Personal Information?](https://aclanthology.org/2022.findings-emnlp.148/)                               |                      **Personal Information**&**Memorization**&**Privacy Risk**                       |
| 22.10 |                                                                                                 Google Research                                                                                                 |                 INLG2023                  |                      [Preventing Generation of Verbatim Memorization in Language Models Gives a False Sense of Privacy](https://aclanthology.org/2023.inlg-main.3/)                      |                    **Verbatim Memorization**&**Filter**&**Style Transfer Prompts**                    |
| 23.02 |                                                                                             University of Waterloo                                                                                              |         Security and Privacy2023          |             [Analyzing Leakage of Personally Identifiable Information in Language Models](https://www.computer.org/csdl/proceedings-article/sp/2023/933600a346/1NrbXJj80H6)              |                    **PII Leakage**&**PII Reconstruction**&**Differential Privacy**                    |
| 23.04 |                                                                                 Hong Kong University of Science and Technology                                                                                  |            EMNLP2023(findings)            |                                                  [Multi-step Jailbreaking Privacy Attacks on ChatGPT](https://arxiv.org/abs/2304.05197)                                                  |                                      **Privacy**&**Jailbreaks**                                       |
| 23.05 |                                                                                   University of Illinois at Urbana-Champaign                                                                                    |                   arxiv                   |                        [Quantifying Association Capabilities of Large Language Models and Its Implications on Privacy Leakage](https://arxiv.org/abs/2305.12707)                         |                                       **Co-occurrence**&**PII**                                       |
| 23.05 |                                                                                        The University of Texas at Dallas                                                                                        |                  ACL2023                  |                               [Controlling the Extraction of Memorized Datafrom Large Language Models via Prompt-Tuning](https://arxiv.org/abs/2305.11759)                               |                                  **Prompt-Tuning**&**Memorization**                                   |
| 23.05 |                                                                                                 Google Research                                                                                                 |            NAACL2024(findings)            |                                    [Can Public Large Language Models Help Private Cross-device Federated Learning?](https://arxiv.org/abs/2305.12132)                                    |               **Federated Learning**&**Large Language Models**&**Differential Privacy**               |
| 23.06 |                                                                                   University of Illinois at Urbana-Champaign                                                                                    |                   arxiv                   |                                      [DECODINGTRUST: A Comprehensive Assessment of Trustworthiness in GPT Models](https://arxiv.org/abs/2306.11698)                                      |                          **Robustness**&**Ethics**&**Privacy**&**Toxicity**                           |
| 23.08 |                                                                                       Bern University of Applied Sciences                                                                                       |            NAACL2024(findings)            |                       [Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models in Court Decisions](https://arxiv.org/abs/2308.11103)                        |                   **Anonymization**&**Re-Identification**&**Large Language Models**                   |
| 23.09 |                                                                                                 UNC Chapel Hill                                                                                                 |                   arxiv                   |                         [Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks](https://arxiv.org/abs/2309.17410)                          |         **Hidden States Attack**&**Hidden States Defense**&**Deleting Sensitive Information**         |
| 23.09 |                                                                                         Princeton University&Microsoft                                                                                          |                   arxiv                   |                                [Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation](https://arxiv.org/abs/2309.11765)                                |                           **In-Context Learning**&**Differential Privacy**                            |
| 23.10 |                                                                                                       ETH                                                                                                       |                   arxiv                   |                                   [Beyond Memorization: Violating Privacy Via Inference with Large Language Models](https://arxiv.org/abs/2310.07298)                                    |                      **Context Inference**&**Privacy-Invasive**&**Extract PII**                       |
| 23.10 |                                                                                         Indiana University Bloomington                                                                                          |                 CCS 2024                  |                              [The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks](https://arxiv.org/abs/2310.15469)                               |                                  **Privacy risks**&**PII Recovery**                                   |
| 23.10 |                                                                     University of Washington & Allen Institute for Artificial Intelligence                                                                      |                   arxiv                   |                       [Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory](https://arxiv.org/abs/2310.17884)                        |                       **Benchmark**&**Contextual Privacy**&**Chain-of-thought**                       |
| 23.10 |                                                                                         Georgia Institute of Technology                                                                                         |                   arxiv                   |                                            [Unlearn What You Want to Forget: Efficient Unlearning for LLMs](https://arxiv.org/abs/2310.20150)                                            |                   **Unlearning**&**Teacher-student Framework**&**Data Protection**                    |
| 23.10 |                                                                                               Tianjin University                                                                                                |                 EMNLP2023                 |                                      [DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models](https://arxiv.org/abs/2310.20138)                                       |                 **Privacy Neuron Detection**&**Model Editing**&**Data Memorization**                  |
| 23.11 |                                                                                               Zhejiang University                                                                                               |                   arxiv                   |                                     [Input Reconstruction Attack against Vertical Federated Large Language Models](https://arxiv.org/abs/2311.07585)                                     |             **Vertical Federated Learning**&**Input Reconstruction**&**Privacy Concerns**             |
| 23.11 |                                                                           Georgia Institute of Technology, Carnegie Mellon University                                                                           |                   arxiv                   |                                        [Reducing Privacy Risks in Online Self-Disclosures with Language Models](https://arxiv.org/abs/2311.09538)                                        |             **Online Self-Disclosure**&**Privacy Risks**&**Self-Disclosure Abstraction**              |
| 23.11 |                                                                                               Cornell University                                                                                                |                   arxiv                   |                                                               [Language Model Inversion](https://arxiv.org/abs/2311.13647)                                                               |                       **Model Inversion**&**Prompt Reconstruction**&**Privacy**                       |
| 23.11 |                                                                                                    Ant Group                                                                                                    |                   arxiv                   |                                                   [PrivateLoRA for Efficient Privacy Preserving LLM](https://arxiv.org/abs/2311.14030)                                                   |                                    **Privacy Preserving**&**LoRA**                                    |
| 23.12 |                                                                                                Drexel University                                                                                                |                   arXiv                   |                              [A Survey on Large Language Model (LLM) Security and Privacy: The Good the Bad and the Ugly](https://arxiv.org/abs/2312.02003)                              |                                 **Security**&**Privacy**&**Attacks**                                  |
| 23.12 |                                                                 University of Texas at Austin, Princeton University, MIT, University of Chicago                                                                 |                   arxiv                   |                                      [DP-OPT: MAKE LARGE LANGUAGE MODEL YOUR PRIVACY-PRESERVING PROMPT ENGINEER](https://arxiv.org/abs/2312.03724)                                       |                              **Prompt Tuning**&**Differential Privacy**                               |
| 23.12 |                                                                                         Delft University of Technology                                                                                          |                 ICSE 2024                 |                                               [Traces of Memorisation in Large Language Models for Code](https://arxiv.org/abs/2312.11658)                                               |                           **Code Memorisation**&**Data Extraction Attacks**                           |
| 23.12 |                                                                                          University of Texas at Austin                                                                                          |                   arXiv                   |                     [SentinelLMs: Encrypted Input Adaptation and Fine-tuning of Language Models for Private and Secure Inference](https://arxiv.org/abs/2312.17342)                      |                        **Privacy**&**Security**&**Encrypted Input Adaptation**                        |
| 23.12 |                                                                              Rensselaer Polytechnic Institute, Columbia University                                                                              |                   arXiv                   |                             [Differentially Private Low-Rank Adaptation of Large Language Model Using Federated Learning](https://arxiv.org/abs/2312.17493)                              |               **Federated Learning**&**Differential Privacy**&**Efficient Fine-Tuning**               |
| 24.01 |                                                                     Harbin Institute of Technology Shenzhen&Peng Cheng Laboratory Shenzhen                                                                      |                   arxiv                   |                             [SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models](https://arxiv.org/abs/2401.00793)                              | **Privacy-Preserving Inference (PPI)**&**Secure Multi-Party Computing (SMPC)**&**Transformer Models** |
| 24.01 |                                                           NUS (Chongqing) Research Institute, Huawei Noah‚Äôs Ark Lab, National University of Singapore                                                           |                   arxiv                   |                                                    [Teach Large Language Models to Forget Privacy](https://arxiv.org/abs/2401.00870)                                                     |                    **Data Privacy**&**Prompt Learning**&**Problem Decomposition**                     |
| 24.01 |                                                                                 Princeton University, Google DeepMind, Meta AI                                                                                  |                   arxiv                   |                                     [Private Fine-tuning of Large Language Models with Zeroth-order Optimization](https://arxiv.org/abs/2401.04343)                                      |                        **Differential Privacy**&**Zeroth-order Optimization**                         |
| 24.01 |                                                                                 Harvard&USC&UCLA&UW Seattle&UW-Madison&UC Davis                                                                                 |                 NAACL2024                 |                                                [Instructional Fingerprinting of Large Language Models](https://arxiv.org/abs/2401.12255)                                                 |                **Model Fingerprinting**&**Instructional Backdoor**&**Model Ownership**                |
| 24.02 |                                                                                        Florida International University                                                                                         |                   arxiv                   |                                          [Security and Privacy Challenges of Large Language Models: A Survey](https://arxiv.org/abs/2402.00888)                                          |                            **Security**&**Privacy Challenges**&**Suevey**                             |
| 24.02 |                                                              Northeastern University, Carnegie Mellon University, Rensselaer Polytechnic Institute                                                              |                   arxiv                   |                                         [Human-Centered Privacy Research in the Age of Large Language Models](https://arxiv.org/abs/2402.01994)                                          |                     **Generative AI**&**Privacy**&**Human-Computer Interaction**                      |
| 24.02 |                                                                                 CISPA Helmholtz Center for Information Security                                                                                 |                   arxiv                   |                                                [Conversation Reconstruction Attack Against GPT Models](https://arxiv.org/abs/2402.02987)                                                 |                 **Conversation Reconstruction Attack**&**Privacy risks**&**Security**                 |
| 24.02 |                                                                             Columbia University, M365 Research, Microsoft Research                                                                              |                   arxiv                   |                                             [Differentially Private Training of Mixture of Experts Models](https://arxiv.org/abs/2402.07334)                                             |                            **Differential Privacy**&**Mixture of Experts**                            |
| 24.02 |                                                                                Stanford University, Truera ,Princeton University                                                                                |                   arxiv                   |                                      [De-amplifying Bias from Differential Privacy in Language Model Fine-tuning](https://arxiv.org/abs/2402.04489)                                      |                      **Fairness**&**Differential Privacy**&**Data Augmentation**                      |
| 24.02 |                                                                                     Sun Yat-sen University, Google Research                                                                                     |                   arxiv                   |                                          [Privacy-Preserving Instructions for Aligning Large Language Models](https://arxiv.org/abs/2402.13659)                                          |                             **Privacy Risks**&**Synthetic Instructions**                              |
| 24.02 |                                                                                    National University of Defense Technology                                                                                    |                   arxiv                   |            [LLM-based Privacy Data Augmentation Guided by Knowledge Distillation with a Distribution Tutor for Medical Text Classification](https://arxiv.org/abs/2402.16515)            |       **Privacy Data Augmentation**&**Knowledge Distillation**&**Medical Text Classification**        |
| 24.02 |                                                                                      Michigan State University, Baidu Inc.                                                                                      |                   arxiv                   |                                [The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2402.16893)                                |                         **Privacy**&**Retrieval-Augmented Generation (RAG)**                          |
| 24.02 |                                                                      University of Washington&Allen Institute for Artificial Intelligence                                                                       |                 NAACL2024                 |                          [JAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding over Small Language Models](https://arxiv.org/abs/2402.08761)                           |             **Authorship Obfuscation**&**Constrained Decoding**&**Small Language Models**             |
| 24.03 |                                                                                                  Virginia Tech                                                                                                  |                   arxiv                   |                                                [Privacy-Aware Semantic Cache for Large Language Models](https://arxiv.org/abs/2403.02694)                                                |                           **Federated Learning**&**Cache Hit**&**Privacy**                            |
| 24.03 |                                                                                               Tsinghua University                                                                                               |                   arxiv                   |                 [CoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following](https://arxiv.org/abs/2403.03129)                  |             **Small Language Models**&**Privacy**&**Context-Aware Instruction Following**             |
| 24.03 |                                                                            Shandong University, Leiden University, Drexel University                                                                            |                   arxiv                   |                                       [On Protecting the Data Privacy of Large Language Models (LLMs): A Survey](https://arxiv.org/abs/2403.05156)                                       |                          **Data Privacy**&**Privacy Protection**&**Survey**                           |
| 24.03 |                 Arizona State University, University of Minnesota, University of Science and Technology of China, North Carolina State University, University of North Carolina at Chapel Hill                  |                   arxiv                   |                                       [Privacy-preserving Fine-tuning of Large Language Models through Flatness](https://arxiv.org/abs/2403.04124)                                       |                           **Differential Privacy**&**Model Generalization**                           |
| 24.03 |                                                                                        University of Southern California                                                                                        |                   arxiv                   |                                        [Differentially Private Next-Token Prediction of Large Language Models](https://arxiv.org/abs/2403.15638)                                         |                                       **Differential Privacy**                                        |
| 24.04 |                                    University of Maryland, Oregon State University, ELLIS Institute T√ºbingen & MPI Intelligent Systems, T√ºbingen AI Center, Google DeepMind                                     |                   arxiv                   |                                [Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models](https://arxiv.org/abs/2404.01231)                                |                  **Privacy Backdoors**&**Membership Inference**&**Model Poisoning**                   |
| 24.04 |                                                                City University of Hong Kong, The Hong Kong University of Science and Technology                                                                 |                   arxiv                   |                                           [LMEraser: Large Model Unlearning through Adaptive Prompt Tuning](https://arxiv.org/abs/2404.11056)                                            |               **Machine Unlearning**&**Adaptive Prompt Tuning**&**Privacy Protection**                |
| 24.04 |                                                           University of Electronic Science and Technology of China, Chengdu University of Technology                                                            |                   arxiv                   |                                      [Understanding Privacy Risks of Embeddings Induced by Large Language Models](https://arxiv.org/abs/2404.16587)                                      |                                   **Privacy Risks**&**Embeddings**                                    |
| 24.04 |                                                                                             Salesforce AI Research                                                                                              |                   arxiv                   |                            [Investigating the prompt leakage effect and black-box defenses for multi-turn LLM interactions](https://arxiv.org/abs/2404.16251)                            |                               **Prompt Leakage**&**Black-box Defenses**                               |
| 24.04 |                                                   University of Texas at El Paso, Texas A&M University Central Texas, University of Maryland Baltimore County                                                   |                   arxiv                   |                     [PrivComp-KG: Leveraging Knowledge Graph and Large Language Models for Privacy Policy Compliance Verification](https://arxiv.org/abs/2404.19744)                     |                     **Privacy Policy**&**Policy Compliance**&**Knowledge Graph**                      |
| 24.05 |                                                                                           Renmin University of China                                                                                            |                COLING 2024                |                                                  [Locally Differentially Private In-Context Learning](https://arxiv.org/abs/2405.04032)                                                  |                        **In-context Learning**&**Local Differential Privacy**                         |
| 24.05 |                                                                                             University of Maryland                                                                                              |                 NAACL2024                 |                                              [Keep It Private: Unsupervised Privatization of Online Text](https://arxiv.org/abs/2405.10260)                                              |               **Unsupervised Privatization**&**Online Text**&**Large Language Models**                |
| 24.05 |                                                                                               Zhejiang University                                                                                               |                   arxiv                   |                                    [PermLLM: Private Inference of Large Language Models within 3 Seconds under WAN](https://arxiv.org/abs/2405.18744)                                    |                             **Private Inference**&**Secure Computation**                              |
| 24.05 |                                                                                            University of Connecticut                                                                                            |                   arxiv                   |                    [LMO-DP: Optimizing the Randomization Mechanism for Differentially Private Fine-Tuning of Large Language Models](https://arxiv.org/abs/2405.18776)                    |                               **Differential Privacy**&**Fine-Tuning**                                |
| 24.05 |                                                                                        University of Technology, Sydney                                                                                         |                   arxiv                   |                                        [Large Language Model Watermark Stealing with Mixed Integer Programming](https://arxiv.org/abs/2405.19677)                                        |                 **Watermark Stealing**&**Mixed Integer Programming**&**LLM Security**                 |
| 24.05 |                                                                                  Huazhong University of Science and Technology                                                                                  |         Procedia Computer Science         |                                              [No Free Lunch Theorem for Privacy-Preserving LLM Inference](https://arxiv.org/abs/2405.20681)                                              |                        **Privacy**&**LLM Inference**&**No Free Lunch Theory**                         |
| 24.05 |                                                                                                   ETH Zurich                                                                                                    |                   arxiv                   |                                                   [Black-Box Detection of Language Model Watermarks](https://arxiv.org/abs/2405.20777)                                                   |                             **Watermark Detection**&**Black-Box Testing**                             |
| 24.06 |                                                                                      South China University of Technology                                                                                       |                   arxiv                   |                      [PrivacyRestore: Privacy-Preserving Inference in Large Language Models via Privacy Removal and Restoration](https://arxiv.org/abs/2406.01394)                       |                                 **Privacy-Preserving**&**Inference**                                  |
| 24.06 |                                                                                           Carnegie Mellon University                                                                                            |                 ICML 2024                 |                                   [PrE-Text: Training Language Models on Private Federated Data in the Age of LLMs](https://arxiv.org/abs/2406.02958)                                    |                  **Federated Learning**&**Differential Privacy**&**Synthetic Data**                   |
| 24.06 |                                                                                      University of California, Santa Cruz                                                                                       |                   arxiv                   |                                           [Large Language Model Unlearning via Embedding-Corrupted Prompts](https://arxiv.org/abs/2406.07933)                                            |                            **Unlearning**&**Embedding-Corrupted Prompts**                             |
| 24.06 |                                                                                         University of Technology Sydney                                                                                         |                   arxiv                   |                                 [Unique Security and Privacy Threats of Large Language Model: A Comprehensive Survey](https://arxiv.org/abs/2406.07973)                                  |                               **Security Threats**&**Privacy Threats**                                |
| 24.06 |                                                                                                UC Santa Barbara                                                                                                 |                   arxiv                   |                         [Reversing the Forget-Retain Objectives: An Efficient LLM Unlearning Framework from Logit Difference](https://arxiv.org/abs/2406.08607)                          |                          **LLM Unlearning**&**Logit Difference**&**Privacy**                          |
| 24.06 |                                                                                    Technion ‚Äì Israel Institute of Technology                                                                                    |                   arxiv                   |                          [REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space](https://arxiv.org/abs/2406.09325)                          |                               **Unlearning**&**Sensitive Information**                                |
| 24.06 |                                                         University of Maryland, ELLIS Institute T√ºbingen, Max Planck Institute for Intelligent Systems                                                          |                   arxiv                   |                                    [Be like a Goldfish, Don‚Äôt Memorize! Mitigating Memorization in Generative LLMs](https://arxiv.org/abs/2406.10209)                                    |                                  **Memorization**&**Goldfish Loss**                                   |
| 24.06 |                                                                            McCombs School of Business, University of Texas at Austin                                                                            |                   arxiv                   |                                          [PRISM: A Design Framework for Open-Source Foundation Model Safety](https://arxiv.org/abs/2406.10415)                                           |                         **PRISM**&**Open-Source**&**Foundation Model Safety**                         |
| 24.06 |                                                                                         Zhejiang University, MIT, UCLA                                                                                          |                   arxiv                   |                                          [MemDPT: Differential Privacy for Memory Efficient Language Models](https://arxiv.org/abs/2406.11087)                                           |               **MemDPT**&**Differential Privacy**&**Memory Efficient Language Models**                |
| 24.06 |                                                                                         Zhejiang University, MIT, UCLA                                                                                          |                   arxiv                   |                              [GOLDCOIN: Grounding Large Language Models in Privacy Laws via Contextual Integrity Theory](https://arxiv.org/abs/2406.11149)                               |                     **GOLDCOIN**&**Contextual Integrity Theory**&**Privacy Laws**                     |
| 24.06 |                                                                                                  IBM Research                                                                                                   |                   arxiv                   |                               [Split, Unlearn, Merge: Leveraging Data Attributes for More Effective Unlearning in LLMs](https://arxiv.org/abs/2406.11780)                                |                                  **SPUNGE**&**Unlearning**&**LLMs**                                   |
| 24.06 |                                                                                     Ping An Technology (Shenzhen) Co., Ltd.                                                                                     |                   arxiv                   |                                             [PFID: Privacy First Inference Delegation Framework for LLMs](https://arxiv.org/abs/2406.12238)                                              |                             **PFID**&**Privacy**&**Inference Delegation**                             |
| 24.06 |                                                                                 Hong Kong University of Science and Technology                                                                                  |                   arxiv                   |                             [PDSS: A Privacy-Preserving Framework for Step-by-Step Distillation of Large Language Models](https://arxiv.org/abs/2406.12403)                              |                                    **PDSS**&**Privacy-Preserving**                                    |
| 24.06 |                                                                                         KAIST AI, Hyundai Motor Company                                                                                         |                   arxiv                   |                        [Protecting Privacy Through Approximating Optimal Parameters for Sequence Unlearning in Language Models](https://arxiv.org/abs/2406.14091)                        |                 **Privacy Protection**&**Optimal Parameters**&**Sequence Unlearning**                 |
| 24.06 |                                                                                               Nanjing University                                                                                                |                   arxiv                   |                                    [The Fire Thief Is Also the Keeper: Balancing Usability and Privacy in Prompts](https://arxiv.org/abs/2406.14318)                                     |                      **Prompt Privacy**&**Anonymization**&**Privacy Protection**                      |
| 24.06 |                                                                                   University of Massachusetts Amherst, Google                                                                                   |                   arxiv                   |                                           [POSTMARK: A Robust Blackbox Watermark for Large Language Models](https://arxiv.org/abs/2406.14517)                                            |                     **Blackbox Watermark**&**Paraphrasing Attacks**&**Detection**                     |
| 24.06 |                                                                                            Michigan State University                                                                                            |                   arxiv                   |                            [Mitigating the Privacy Issues in Retrieval-Augmented Generation (RAG) via Pure Synthetic Data](https://arxiv.org/abs/2406.14773)                             |                   **Retrieval-Augmented Generation**&**Privacy**&**Synthetic Data**                   |
| 24.06 |                                                                                               Beihang University                                                                                                |                   arxiv                   |                              [Safely Learning with Private Data: A Federated Learning Framework for Large Language Model](https://arxiv.org/abs/2406.14898)                              |                                  **Federated Learning**&**Privacy**                                   |
| 24.06 |                                                                                         University of Rome Tor Vergata                                                                                          |                   arxiv                   |                                 [Enhancing Data Privacy in Large Language Models through Private Association Editing](https://arxiv.org/abs/2406.18221)                                  |                           **Data Privacy**&**Private Association Editing**                            |
| 24.07 |                                                                                          Huawei Munich Research Center                                                                                          |                   arxiv                   |                     [IncogniText: Privacy-enhancing Conditional Text Anonymization via LLM-based Private Attribute Randomization](https://arxiv.org/abs/2407.02956)                      |                                  **Text Anonymization**&**Privacy**                                   |
| 24.07 |                                                                                          Huawei Munich Research Center                                                                                          |                   arxiv                   |                          [ObfuscaTune: Obfuscated Offsite Fine-tuning and Inference of Proprietary LLMs on Private Datasets](https://arxiv.org/abs/2407.02960)                           |                          **Inference**&**Proprietary LLMs**&**Private Data**                          |
| 24.07 |                                                                                              Texas A&M University                                                                                               |                   arxiv                   |                               [Exposing Privacy Gaps: Membership Inference Attack on Preference Data for LLM Alignment](https://arxiv.org/abs/2407.06443)                                |                 **Membership Inference Attack**&**Preference Data**&**LLM Alignment**                 |
| 24.07 |                                                                                                 Google Research                                                                                                 |                   arxiv                   |                                        [Fine-Tuning Large Language Models with User-Level Differential Privacy](https://arxiv.org/abs/2407.07737)                                        |                          **User-Level Differential Privacy**&**Fine-Tuning**                          |
| 24.07 |                                                                                              Newcastle University                                                                                               |                   arxiv                   |                              [Privacy-Preserving Data Deduplication for Enhancing Federated Learning of Language Models](https://arxiv.org/abs/2407.08152)                               |       **Privacy-Preserving Deduplication**&**Federated Learning**&**Private Set Intersection**        |
| 24.07 |                                                                                  Huazhong University of Science and Technology                                                                                  |                   arxiv                   |                                                        [On the (In)Security of LLM App Stores](https://arxiv.org/abs/2407.08422)                                                         |                              **LLM App Stores**&**Security**&**Privacy**                              |
| 24.07 |                                                                                               Soochow University                                                                                                |                   arxiv                   |                                             [Learning to Refuse: Towards Mitigating Privacy Risks in LLMs](https://arxiv.org/abs/2407.10058)                                             |                               **Privacy Risks**&**Machine Unlearning**                                |
| 24.07 |                                                                            The University of Texas Health Science Center at Houston                                                                             |                   arxiv                   |                        [Robust Privacy Amidst Innovation with Large Language Models Through a Critical Assessment of the Risks](https://arxiv.org/abs/2407.16166)                        |                   **Text Generation**&**Privacy**&**Protected Health Information**                    |
| 24.07 |                                                                                          Huawei Munich Research Center                                                                                          |             ACL 2024 Workshop             |                            [PII-Compass: Guiding LLM training data extraction prompts towards the target PII via grounding](https://arxiv.org/abs/2407.02943)                            |                         **PII Extraction**&**Data Privacy**&**LLM Security**                          |
| 24.07 |                                                                                         University of Technology Sydney                                                                                         |                   arxiv                   |                                      [The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies](https://arxiv.org/abs/2407.19354)                                       |                          **LLM Agent**&**Privacy Preservation**&**Defense**                           |
| 24.07 |                                                                                            University of Notre Dame                                                                                             |                   arxiv                   |                                                    [Machine Unlearning in Generative AI: A Survey](https://arxiv.org/abs/2407.20516)                                                     |                       **Generative Models**&**Trustworthy ML**&**Data Privacy**                       |
| 24.07 |                                                                                                                                                                                                                 |                   arxiv                   |                                 [Adaptive Pre-training Data Detection for Large Language Models via Surprising Tokens](https://arxiv.org/abs/2407.21248)                                 |                **Pre-training Data Detection**&**Surprising Tokens**&**Data Privacy**                 |
| 24.08 |                                                                                               Sichuan University                                                                                                |                   arxiv                   |                                     [HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy Protection](https://arxiv.org/abs/2408.02927)                                      |                           **Tabular Data Synthesis**&**Privacy Protection**                           |
| 24.08 |                                                                                                   UC Berkeley                                                                                                   |                   arxiv                   |                                                          [MPC-Minimized Secure LLM Inference](https://arxiv.org/abs/2408.03561)                                                          |                  **Secure Multi-party Computation**&**Privacy-Preserving Inference**                  |
| 24.08 |                                                                                           Sapienza University of Rome                                                                                           |                   arxiv                   |                                [Preserving Privacy in Large Language Models: A Survey on Current Threats and Solutions](https://arxiv.org/abs/2408.05212)                                |                             **Privacy Attacks**&**Differential Privacy**                              |
| 24.08 |                                                                                       New Jersey Institute of Technology                                                                                        |                   arxiv                   |                              [Casper: Prompt Sanitization for Protecting User Privacy in Web-Based Large Language Models](https://arxiv.org/abs/2408.07004)                              |                               **Prompt Sanitization**&**User Privacy**                                |
| 24.08 |                                                                                                Xidian University                                                                                                |                   arxiv                   |                    [DePrompt: Desensitization and Evaluation of Personal Identifiable Information in Large Language Model Prompts](https://arxiv.org/abs/2408.08930)                     |        **Personal Identifiable Information**&**Prompt Desensitization**&**Privacy Protection**        |
| 24.08 |                                                                                       Huawei Technologies Canada Co. Ltd                                                                                        |                   arxiv                   |                             [Tracing Privacy Leakage of Language Models to Training Data via Adjusted Influence Functions](https://arxiv.org/abs/2408.10468)                             |                              **Privacy Leakage**&**Influence Functions**                              |
| 24.08 |                                                                                           Chinese Academy of Sciences                                                                                           |                   arxiv                   |       [Towards Robust Knowledge Unlearning: An Adversarial Framework for Assessing and Improving Unlearning Robustness in Large Language Models](https://arxiv.org/abs/2408.10682)       |              **Knowledge Unlearning**&**Adversarial Attacks**&**Unlearning Robustness**               |
| 24.08 |                                                                                          Universit√§tsklinikum Erlangen                                                                                          |                   arxiv                   |         [Fine-Tuning a Local LLaMA-3 Large Language Model for Automated Privacy-Preserving Physician Letter Generation in Radiation Oncology](https://arxiv.org/abs/2408.10715)          |                        **Radiation Oncology**&**Data Privacy**&**Fine-Tuning**                        |
| 24.08 |                                                                                       University of California, Berkeley                                                                                        |                   arxiv                   |                                               [LLM-PBE: Assessing Data Privacy in Large Language Models](https://arxiv.org/abs/2408.12787)                                               |                                     **Data Privacy**&**Toolkit**                                      |
| 24.08 |                                                                                    Mitsubishi Electric Research Laboratories                                                                                    |                   arxiv                   |                         [Forget to Flourish: Leveraging Machine-Unlearning on Pretrained Language Models for Privacy Leakage](https://arxiv.org/abs/2408.17354)                          |                **Privacy Leakage**&**Model-Unlearning**&**Pretrained Language Models**                |
| 24.09 |                                                                                               Stanford University                                                                                               |                   arxiv                   |                                     [PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in Action](https://arxiv.org/abs/2409.00138)                                      |                        **Privacy Norm Awareness**&**Privacy Risk Evaluation**                         |
| 24.09 |                                                                                                    ByteDance                                                                                                    |                   arxiv                   |                         [How Privacy-Savvy Are Large Language Models? A Case Study on Compliance and Privacy Technical Review](https://arxiv.org/abs/2409.02375)                         |        **Privacy Compliance**&**Privacy Information Extraction**&**Technical Privacy Review**         |
| 24.09 |                                                                                                       MIT                                                                                                       |                 COLM 2024                 |                                                   [Unforgettable Generalization in Language Models](https://arxiv.org/abs/2409.02228)                                                    |                          **Unlearning**&**Generalization**&**Random Labels**                          |
| 24.09 |                                                                             Anhui University of Technology, University of Cambridge                                                                             |                   arxiv                   |                              [On the Weaknesses of Backdoor-based Model Watermarks: An Information-theoretic Perspective](https://arxiv.org/abs/2409.06130)                              |                  **Model Watermarking**&**Backdoor Attacks**&**Information Theory**                   |
| 24.09 |                                                                                               Bilkent University                                                                                                |                   arxiv                   |                       [Generated Data with Fake Privacy: Hidden Dangers of Fine-tuning Large Language Models on Generated Data](https://arxiv.org/abs/2409.11423)                        |                       **Privacy Risks**&**PII**&**Membership Inference Attack**                       |
| 24.09 |                                                                                        National University of Singapore                                                                                         |                   arxiv                   |                                 [Context-Aware Membership Inference Attacks against Pre-trained Large Language Models](https://arxiv.org/abs/2409.13745)                                 |                         **Membership Inference Attack**&**Context-Awareness**                         |
| 24.09 |                                                                                             George Mason University                                                                                             |                   arxiv                   |                                     [Unlocking Memorization in Large Language Models with Dynamic Soft Prompting](https://arxiv.org/abs/2409.13853)                                      |                              **Memorization**&**Dynamic Soft Prompting**                              |
| 24.09 |                                                                               CAS Key Lab of Network Data Science and Technology                                                                                |                EMNLP 2024                 |                             [Pretraining Data Detection for Large Language Models: A Divergence-based Calibration Method](https://arxiv.org/abs/2409.14781)                              |          **Pretraining Data Detection**&**Divergence Calibration**&**Membership Inference**           |
| 24.09 |                                                                                               √âcole Polytechnique                                                                                               |                   arxiv                   |                                    [Predicting and Analyzing Memorization Within Fine-Tuned Large Language Models](https://arxiv.org/abs/2409.18858)                                     |                             **Memorization**&**Fine-tuning**&**Privacy**                              |
| 24.10 |                                                                                        University of Southern California                                                                                        |                   arxiv                   |                                          [ADAPTIVELY PRIVATE NEXT-TOKEN PREDICTION OF LARGE LANGUAGE MODELS](https://arxiv.org/abs/2410.02016)                                           |                  **Differential Privacy**&**Next-Token Prediction**&**Adaptive DP**                   |
| 24.10 |                                                                                              University of Chicago                                                                                              |                   arxiv                   |                                                      [MITIGATING MEMORIZATION IN LANGUAGE MODELS](https://arxiv.org/abs/2410.02159)                                                      |                              **Memorization Mitigation**&**Unlearning**                               |
| 24.10 |                                                                                             University of Groningen                                                                                             |                   arxiv                   |                                             [Undesirable Memorization in Large Language Models: A Survey](https://arxiv.org/abs/2410.02650)                                              |                                     **Memorization**&**Privacy**                                      |
| 24.10 |                                                                               University of California, Santa Barbara, AWS AI Lab                                                                               |                   arxiv                   |                                    [Detecting Training Data of Large Language Models via Expectation Maximization](https://arxiv.org/abs/2410.07582)                                     |                     **Membership Inference Attack**&**Expectation Maximization**                      |
| 24.10 |                                                                                   Queen‚Äôs University, J.P. Morgan AI Research                                                                                   |            EMNLP 2024 Findings            |                               [Fine-Tuning Language Models with Differential Privacy through Adaptive Noise Allocation](https://arxiv.org/abs/2410.02912)                                |                        **Differential Privacy**&**Adaptive Noise Allocation**                         |
| 24.10 |                                                                   King Abdullah University of Science and Technology, Ruhr University Bochum                                                                    |                EMNLP 2024                 |                                              [Private Language Models via Truncated Laplacian Mechanism](https://arxiv.org/abs/2410.08027)                                               |             **Differential Privacy**&**Word Embedding**&**Truncated Laplacian Mechanism**             |
| 24.10 |                                                                               Purdue University, Georgia Institute of Technology                                                                                |                   arxiv                   |                                [Privately Learning from Graphs with Applications in Fine-tuning Large Language Models](https://arxiv.org/abs/2410.08299)                                 |                **Privacy-preserving learning**&**Graph learning**&**Fine-tuning LLMs**                |
| 24.10 |                                                                                             Northeastern University                                                                                             |                   arxiv                   |            [Rescriber: Smaller-LLM-Powered User-Led Data Minimization for Navigating Privacy Trade-offs in LLM-Based Conversational Agents](https://arxiv.org/abs/2410.11876)            |                                          **Privacy**&**PII**                                          |
| 24.10 |                                                         The Pennsylvania State University, University of California Los Angeles, University of Virginia                                                         |                   arxiv                   |                                    [Data-adaptive Differentially Private Prompt Synthesis for In-Context Learning](https://arxiv.org/abs/2410.12085)                                     |            **Differential privacy**&**In-context learning**&**Synthetic data generation**             |
| 24.10 |             Nanjing University of Science and Technology, Western Sydney University, Institute of Information Engineering (Chinese Academy of Sciences), CSIRO‚Äôs Data61, The University of Chicago              |                   arxiv                   |                                 [Reconstruction of Differentially Private Text Sanitization via Large Language Models](https://arxiv.org/abs/2410.12443)                                 |                 **Differential Privacy**&**Reconstruction attacks**&**Privacy risks**                 |
| 24.10 |                                                                                       University of California San Diego                                                                                        |                   arxiv                   |                                                [Imprompter: Tricking LLM Agents into Improper Tool Use](https://arxiv.org/abs/2410.14923)                                                |                      **Prompt Injection**&**LLM Agents**&**Adversarial Prompts**                      |
| 24.10 |                                                                                       University of California, San Diego                                                                                       |                   arxiv                   |                                                 [Evaluating Deep Unlearning in Large Language Models](https://arxiv.org/abs/2410.15153)                                                  |                               **Deep Unlearning**&**Knowledge Removal**                               |
| 24.10 |                                                                                        The Pennsylvania State University                                                                                        |                   arxiv                   |                            [Does Your LLM Truly Unlearn? An Embarrassingly Simple Approach to Recover Unlearned Knowledge](https://arxiv.org/abs/2410.16454)                             |                    **Machine Unlearning**&**Knowledge Recovery**&**Quantization**                     |
| 24.10 |                                                                                                 Google DeepMind                                                                                                 |                   arxiv                   |                                             [Remote Timing Attacks on Efficient Language Model Inference](https://arxiv.org/abs/2410.17175)                                              |                        **Timing Attacks**&**Efficient Inference**&**Privacy**                         |
| 24.10 |                                                                                         Huawei Technologies D√ºsseldorf                                                                                          |                   arxiv                   |                                       [PSY: Posterior Sampling Based Privacy Enhancer in Large Language Models](https://arxiv.org/abs/2410.18824)                                        |                **Privacy Enhancing Technology**&**Posterior Sampling**&**LLM Privacy**                |
| 24.10 |                                                                                             Northwestern University                                                                                             |                   arxiv                   |                         [LanFL: Differentially Private Federated Learning with Large Language Models using Synthetic Samples](https://arxiv.org/abs/2410.19114)                          |                            **Federated Learning**&**Differential Privacy**                            |
| 24.11 |                                                                                        Technical University of Darmstadt                                                                                        |                   arxiv                   |                                [Scaling Up Membership Inference: When and How Attacks Succeed on Large Language Models](https://arxiv.org/abs/2411.00154)                                |                                 **Membership Inference**&**Privacy**                                  |
| 24.11 |                                                                                              University of Toronto                                                                                              |                   arxiv                   |                                            [Privacy Risks of Speculative Decoding in Large Language Models](https://arxiv.org/abs/2411.01076)                                            |                     **Privacy**&**Speculative Decoding**&**Side-channel Attacks**                     |
| 24.11 |                                                                                             Northeastern University                                                                                             |                   arxiv                   |         [Can Humans Oversee Agents to Prevent Privacy Leakage? A Study on Privacy Awareness, Preferences, and Trust in Language Model Agents](https://arxiv.org/abs/2411.01344)          |                **Privacy Awareness**&**Trust in LLMs**&**Privacy Leakage Prevention**                 |
| 24.11 |                                                                                               Guangxi University                                                                                                |                   arxiv                   |                              [A Practical and Privacy-Preserving Framework for Real-World Large Language Model Services](https://arxiv.org/abs/2411.01471)                               |                **Privacy-Preserving Framework**&**Blind Signatures**&**LLM Services**                 |
| 24.11 |                                                                                       University of Massachusetts Amherst                                                                                       |                   arxiv                   |                                       [Data Extraction Attacks in Retrieval-Augmented Generation via Backdoors](https://arxiv.org/abs/2411.01705)                                        |              **Data Extraction**&**Backdoor Attacks**&**Retrieval-Augmented Generation**              |
| 24.11 |                                                                                                      EPFL                                                                                                       |               NeurIPS 2024                |                                          [Membership Inference Attacks against Large Vision-Language Models](https://arxiv.org/abs/2411.02902)                                           |                    **Membership Inference**&**Vision-Language Models**&**Privacy**                    |
| 24.11 |                                                                                             University of Helsinki                                                                                              |  NeurIPS 2024 Foundation Model Workshop   |                                          [Differentially Private Continual Learning using Pre-Trained Models](https://arxiv.org/abs/2411.04680)                                          |                **Differential Privacy**&**Continual Learning**&**Pre-trained Models**                 |
| 24.11 |                                                                                               Zhejiang University                                                                                               |                   arXiv                   |                                         [Mitigating Privacy Risks in LLM Embeddings from Embedding Inversion](https://arxiv.org/abs/2411.05034)                                          |                            **Embedding Inversion**&**Privacy Protection**                             |
| 24.11 |                                                                                              University of Toronto                                                                                              |                   arxiv                   |                                                      [On the Privacy Risk of In-context Learning](https://arxiv.org/abs/2411.10512)                                                      |               **In-context Learning**&**Privacy Risk**&**Membership Inference Attack**                |
| 24.11 |                                                                                                Fudan University                                                                                                 |                   arxiv                   |               [RAG-Thief: Scalable Extraction of Private Data from Retrieval-Augmented Generation Applications with Agent-based Attacks](https://arxiv.org/abs/2411.14110)               |       **Privacy Attacks**&**Retrieval-Augmented Generation**&**Automated Adversarial Attacks**        |
| 24.11 |                                                                                              Texas A&M University                                                                                               |                 NDSS 2025                 |                                                   [LLMPirate: LLMs for Black-box Hardware IP Piracy](https://arxiv.org/abs/2411.16111)                                                   |                   **LLM-based Attack**&**Hardware IP Piracy**&**Piracy Detection**                    |
| 24.11 |                                                                       Imperial College London, Flashbots, Technical University of Munich                                                                        |                   arxiv                   |                 [Efficient and Private: Memorisation under Differentially Private Parameter-Efficient Fine-Tuning in Language Models](https://arxiv.org/abs/2411.15831)                  |           **Differential Privacy**&**Parameter-Efficient Fine-Tuning**&**Privacy Leakage**            |
| 24.12 |                                                                                   University of North Carolina at Chapel Hill                                                                                   |                   arxiv                   |                                     [A Novel Compact LLM Framework for Local, High-Privacy EHR Data Applications](https://arxiv.org/abs/2412.02868)                                      |                        **Privacy-Preserving LLMs**&**Healthcare**&**EHR Data**                        |
| 24.12 |                                                                                 CISPA Helmholtz Center for Information Security                                                                                 |                   arxiv                   |                                [DP-2Stage: Adapting Language Models as Differentially Private Tabular Data Generators](https://arxiv.org/abs/2412.02467)                                 |                         **Differential Privacy**&**Tabular Data Generation**                          |
| 24.12 |                                                                                               New York University                                                                                               |                   arxiv                   |                                              [TruncFormer: Private LLM Inference Using Only Truncations](https://arxiv.org/abs/2412.01042)                                               |                            **Private Inference**&**LLMs**&**Truncations**                             |
| 24.12 |                                                                             Dalhousie University, Canada; Vector Institute, Canada                                                                              |                   arxiv                   |                                       [Can Large Language Models Be Privacy-Preserving and Fair Medical Coders?](https://arxiv.org/abs/2412.05533)                                       |                     **Differential Privacy**&**ICD Classification**&**Fairness**                      |
| 24.12 |                                                                                       UC Santa Barbara, UC Berkeley, Meta                                                                                       |                   arxiv                   |                                             [PrivAgent: Agentic-based Red-teaming for LLM Privacy Leakage](https://arxiv.org/abs/2412.05734)                                             |                      **Privacy Leakage**&**Red-teaming**&**Adversarial Prompts**                      |
| 24.12 |                                                                                       University of California San Diego                                                                                        |                   arxiv                   |                              [Privacy-Preserving Large Language Models: Mechanisms, Applications, and Future Directions](https://arxiv.org/abs/2412.06113)                               |           **Privacy-Preserving Mechanisms**&**Differential Privacy**&**Federated Learning**           |
| 24.12 |                                                                                             Sun Yat-Sen University                                                                                              |                   arxiv                   |                                 [MemHunter: Automated and Verifiable Memorization Detection at Dataset-scale in LLMs](https://arxiv.org/abs/2412.07261)                                  |                             **Memorization Detection**&**Privacy Risks**                              |
| 24.12 |                               Nanyang Technological University, Singapore University of Technology and Design, Zhejiang University, Chengdu University of Information Technology                                |                   arxiv                   |                          [MRP-LLM: Multitask Reflective Large Language Models for Privacy-Preserving Next POI Recommendation](https://arxiv.org/abs/2412.07796)                          |               **Privacy-Preserving**&**Next POI Recommendation**&**Multitask Learning**               |
| 24.12 |                                                                                        Nanyang Technological University                                                                                         |                   arxiv                   |                                                      [A Survey on Private Transformer Inference](https://arxiv.org/abs/2412.08145)                                                       |                    **Transformer Models**&**Private Inference**&**Data Security**                     |
| 24.12 |                                                                                             University of Sheffield                                                                                             |                   arxiv                   |                                            [How Private are Language Models in Abstractive Summarization?](https://arxiv.org/abs/2412.12040)                                             |                **Privacy Preservation**&**Abstractive Summarization**&**PII Leakage**                 |
| 24.12 |                                                                                  University of Science and Technology of China                                                                                  |                   arxiv                   |                                                [RemoteRAG: A Privacy-Preserving LLM Cloud RAG Service](https://arxiv.org/abs/2412.12775)                                                 |            **Privacy-Preserving RAG**&**Differential Privacy**&**Embedding Perturbation**             |
| 24.12 |                                                                                               Lanzhou University                                                                                                |                ICASSP 2025                |                          [EM-MIAs: Enhancing Membership Inference Attacks in Large Language Models through Ensemble Modeling](https://arxiv.org/abs/2412.17249)                          |              **Membership Inference Attacks**&**Privacy Leakage**&**Ensemble Modeling**               |
| 24.12 |                                        Hunan University, National University of Defense Technology, University of Science and Technology of China, Lancaster University                                         |                   arxiv                   |                           [Accelerating Private Large Transformers Inference through Fine-grained Collaborative Computation](https://arxiv.org/abs/2412.16537)                           |                      **Privacy-Preserving Inference**&**Homomorphic Encryption**                      |
| 24.12 |                                   Hokkaido University, China University of Mining and Technology, Institute of Science Tokyo, Hong Kong University of Science and Technology                                    |                   arxiv                   |                                [Privacy in Fine-tuning Large Language Models: Attacks, Defenses, and Future Directions](https://arxiv.org/abs/2412.16504)                                |                             **Privacy**&**Fine-tuning LLMs**&**Attacks**                              |
| 24.12 |                                                                            Zhejiang Laboratory, The Chinese University of Hong Kong                                                                             |                   arxiv                   |                      [DR-ENCODER: Encode Low-Rank Gradients with Random Prior for Large Language Models Differentially Privately](https://arxiv.org/abs/2412.17053)                      |               **Differential Privacy**&**Gradient Compression**&**Federated Learning**                |
| 24.12 | Yandex, Moscow Institute of Physics and Technology (MIPT), HSE University, Skoltech, Together AI, Ivannikov Institute for System Programming of the Russian Academy of Sciences (ISP RAS), Innopolis University |                   arxiv                   |                                  [Label Privacy in Split Learning for Large Models with Parameter-Efficient Training](https://arxiv.org/abs/2412.16669)                                  |             **Label Privacy**&**Split Learning**&**Parameter-Efficient Training (PEFT)**              |
| 24.12 |                                                                         Institute of Computing Technology, Chinese Academy of Sciences                                                                          |                   arxiv                   |                           [Multi-P2A: A Multi-perspective Benchmark on Privacy Assessment for Large Vision-Language Models](https://arxiv.org/abs/2412.19496)                            |                 **Privacy Assessment**&**Vision-Language Models**&**Privacy Leakage**                 |
| 24.12 |                                                                                              University of Alberta                                                                                              |                   arxiv                   |              [SafeSynthDP: Leveraging Large Language Models for Privacy-Preserving Synthetic Data Generation Using Differential Privacy](https://arxiv.org/abs/2412.20641)               |           **Differential Privacy**&**Synthetic Data Generation**&**Large Language Models**            |
| 25.01 |                                                                                               Tsinghua University                                                                                               |                   arxiv                   |                            [Shifting-Merging: Secure, High-Capacity and Efficient Steganography via Large Language Models](https://arxiv.org/abs/2501.00786)                             |            **Secure Steganography**&**High-Capacity Embedding**&**Large Language Models**             |
| 25.01 |                                                                                               Tsinghua University                                                                                               |                   arxiv                   |                                 [A Survey on Responsible LLMs: Inherent Risk, Malicious Use, and Mitigation Strategy](https://arxiv.org/abs/2501.09431)                                  |                          **Responsible LLMs**&**Privacy**&**Hallucination**                           |
| 25.01 |                                                                                       Ben-Gurion University of the Negev                                                                                        |                   arxiv                   |                     [Tag&Tab: Pretraining Data Detection in Large Language Models Using Keyword-Based Membership Inference Attack](https://arxiv.org/abs/2501.08454)                     |          **Membership Inference Attacks**&**Pretraining Data Detection**&**Keyword Entropy**          |
| 25.01 |                                                                                               New York University                                                                                               |            AAAI PPAI Workshop             |                                                      [Entropy-Guided Attention for Private LLMs](https://arxiv.org/abs/2501.03489)                                                       |       **Privacy-Preserving Inference**&**Entropy-Guided Attention**&**Nonlinear Optimization**        |
| 25.01 |                                                                        Institute of Information Engineering, Chinese Academy of Sciences                                                                        |                   arxiv                   |                   [RAG-WM: An Efficient Black-Box Watermarking Approach for Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2501.05249)                   |            **Black-Box Watermarking**&**Retrieval-Augmented Generation**&**IP Protection**            |
| 25.01 |                                                                                         Case Western Reserve University                                                                                         |                   arxiv                   |                                  [Navigating the Designs of Privacy-Preserving Fine-tuning for Large Language Models](https://arxiv.org/abs/2501.04323)                                  |      **Privacy-Preserving Fine-tuning**&**Data Reconstruction Attack Defense**&**GuardedTuning**      |
| 25.01 |                                                                                        Communication University of China                                                                                        |                   arxiv                   |                    [Practical Secure Inference Algorithm for Fine-tuned Large Language Model Based on Fully Homomorphic Encryption](https://arxiv.org/abs/2501.01672)                    |     **Fully Homomorphic Encryption**&**Privacy-Preserving Techniques**&**Large Language Models**      |
| 25.01 |                                                                                              University at Albany                                                                                               |                   arxiv                   |                   [LegalGuardian: A Privacy-Preserving Framework for Secure Integration of Large Language Models in Legal Practice](https://arxiv.org/abs/2501.10915)                    |               **Legal Practice**&**Privacy Preservation**&**Named Entity Recognition**                |
| 25.01 |                                                                                                  IBM Research                                                                                                   |     AAAI 2025 Deployable AI Workshop      |                                                 Deploying Privacy Guardrails for LLMs: A Comparative Analysis of Real-World Applications                                                 |                        **Privacy Guardrails**&**PII Detection**&**Compliance**                        |
| 25.01 |                                                                                         Delft University of Technology                                                                                          |                   arxiv                   |                [How Much Do Code Language Models Remember? An Investigation on Data Extraction Attacks before and after Fine-tuning](https://arxiv.org/abs/2501.17501v1)                 |                 **Code Language Models**&**Data Extraction Attacks**&**Memorization**                 |
| 25.01 |                                                    Technical University of Darmstadt, Max Planck Institute for Intelligent Systems, University of Copenhagen                                                    |                 ICLR 2025                 |                                       [PSA: Differentially Private Steering for Large Language Model Alignment](https://arxiv.org/abs/2501.18532)                                        |            **Differential Privacy**&**Activation Editing**&**Membership Inference Attack**            |
| 25.01 |                                                                                                 Google Research                                                                                                 |                   arxiv                   |                                               [Scaling Laws for Differentially Private Language Models](https://arxiv.org/abs/2501.18914)                                                |                      **Differential Privacy**&**Scaling Laws**&**LLM Training**                       |
| 25.02 |                                                                                               Zhejiang University                                                                                               |                   arxiv                   |                                     [SecPE: Secure Prompt Ensembling for Private and Robust Large Language Models](https://arxiv.org/abs/2502.00847)                                     |             **Privacy-Preserving LLMs**&**Prompt Ensembling**&**Homomorphic Encryption**              |
| 25.02 |                                                                                               Columbia University                                                                                               |                   arxiv                   |                                    [Skewed Memorization in Large Language Models: Quantification and Decomposition](https://arxiv.org/abs/2502.01187)                                    |                    **LLM Memorization**&**Privacy Risks**&**Fine-tuning Analysis**                    |
| 25.02 |                                                                                       University of California, San Diego                                                                                       |                   arxiv                   |                                 [Robust and Secure Code Watermarking for Large Language Models via ML/Crypto Codesign](https://arxiv.org/abs/2502.02068)                                 |                   **Code Watermarking**&**LLM Security**&**Zero-Knowledge Proofs**                    |
| 25.02 |                                                                                         University of British Columbia                                                                                          |                   arxiv                   |                          [SimMark: A Robust Sentence-Level Similarity-Based Watermarking Algorithm for Large Language Models](https://arxiv.org/abs/2502.02787)                          |             **LLM Watermarking**&**Sentence-Level Similarity**&**Adversarial Robustness**             |
| 25.02 |                                                                                     Tune Insight SA, EPFL, Yale University                                                                                      |                   arxiv                   |                                     [Mitigating Unintended Memorization with LoRA in Federated Learning for LLMs](https://arxiv.org/abs/2502.05087)                                      |                       **Federated Learning**&**Privacy-Preserving Techniques**                        |
| 25.02 |                                                                               Beijing University of Posts and Telecommunications                                                                                |                   arxiv                   |                                   [Mitigating Sensitive Information Leakage in LLMs4Code through Machine Unlearning](https://arxiv.org/abs/2502.05739)                                   |                      **Machine Unlearning**&**Privacy Protection**&**LLMs4Code**                      |
| 25.02 |                                                                                           University of South Florida                                                                                           |                   arxiv                   |                [An Interactive Framework for Implementing Privacy-Preserving Federated Learning: Experiments on Large Language Models](https://arxiv.org/abs/2502.08008)                 |                  **Federated Learning**&**Differential Privacy**&**LLM Fine-Tuning**                  |
| 25.02 |                                                                                                     Amazon                                                                                                      |                   arxiv                   |                                   [HAS MY SYSTEM PROMPT BEEN USED? LARGE LANGUAGE MODEL PROMPT MEMBERSHIP INFERENCE](https://arxiv.org/abs/2502.09974)                                   |             **Prompt Membership Inference**&**LLM Security**&**Statistical Verification**             |
| 25.02 |                                                                                              University of Florida                                                                                              |                   arxiv                   |                                       [Dataset Protection via Watermarked Canaries in Retrieval-Augmented LLMs](https://arxiv.org/abs/2502.10673)                                        |                 **Dataset Protection**&**Watermarking**&**Retrieval-Augmented LLMs**                  |
| 25.02 |                                                                         The Hong Kong University of Science and Technology (Guangzhou)                                                                          |                   arxiv                   |                       [MMUNLEARNER: Reformulating Multimodal Machine Unlearning in the Era of Multimodal Large Language Models](https://arxiv.org/abs/2502.11051)                        |                   **Multimodal Machine Unlearning**&**MLLMs**&**Gradient Descent**                    |
| 25.02 |                                                                                Institute of Software Chinese Academy of Sciences                                                                                |                   arxiv                   |                     [Mimicking the Familiar: Dynamic Command Generation for Information Theft Attacks in LLM Tool-Learning System](https://arxiv.org/abs/2502.11358)                     |          **Information Theft Attacks**&**LLM Tool-Learning**&**Dynamic Command Generation**           |
| 25.02 |                                                                                               Zhejiang University                                                                                               |                   arxiv                   |                                        [R.R.: Unveiling LLM Training Privacy through Recollection and Ranking](https://arxiv.org/abs/2502.12658)                                         |                      **Privacy Leakage**&**PII Reconstruction**&**LLM Security**                      |
| 25.02 |                                                                                            Michigan State University                                                                                            |                   arXiv                   |                                                     [Unveiling Privacy Risks in LLM Agent Memory](https://arxiv.org/abs/2502.13172)                                                      |                         **LLM Agent**&**Privacy Risks**&**Memory Extraction**                         |
| 25.02 |                                                                                                MPI‚ÄìSWS, Germany                                                                                                 |                   arXiv                   |                            [Revisiting Privacy, Utility, and Efficiency Trade-offs when Fine-Tuning Large Language Models](https://arxiv.org/abs/2502.13313)                             |                          **Privacy**&**Utility**&**Fine-Tuning Efficiency**                           |
| 25.02 |                                                                                                 Apart Research                                                                                                  |        AAAI 2025 Workshop DATASAFE        |                                   [Evaluating Precise Geolocation Inference Capabilities of Vision Language Models](https://arxiv.org/abs/2502.14412)                                    |                **Vision-Language Models**&**Geolocation Inference**&**Privacy Risks**                 |
| 25.02 |                                                                                  Huazhong University of Science and Technology                                                                                  |                   arxiv                   |                                   [Merger-as-a-Stealer: Stealing Targeted PII from Aligned LLMs with Model Merging](https://arxiv.org/abs/2502.16094)                                    |                   **Model Merging**&**PII Extraction**&**Security Vulnerabilities**                   |
| 25.02 |                                                                                             Northeastern University                                                                                             |                   arxiv                   |                            [Privacy Ripple Effects from Adding or Removing Personal Information in Language Model Training](https://arxiv.org/abs/2502.15680)                            |                      **PII Memorization**&**LLM Privacy**&**Training Dynamics**                       |
| 25.02 |                                                                                                    Microsoft                                                                                                    |                   arxiv                   |                          [Obliviate: Efficient Unmemorization for Protecting Intellectual Property in Large Language Models](https://arxiv.org/abs/2502.15010)                           |           **Unmemorization**&**Intellectual Property Protection**&**Large Language Models**           |
| 25.02 |                                                                                                Yonsei University                                                                                                |                   arxiv                   |                                 [Marking Code Without Breaking It: Code Watermarking for Detecting LLM-Generated Code](https://arxiv.org/abs/2502.18851)                                 |                     **Code Watermarking**&**LLM Detection**&**Software Security**                     |
| 25.02 |                                                                                      South China University of Technology                                                                                       |                   arxiv                   |                         [RewardDS: Privacy-Preserving Fine-Tuning for Large Language Models via Reward Driven Data Synthesis](https://arxiv.org/abs/2502.18517)                          |           **Privacy-Preserving Fine-Tuning**&**Synthetic Data Generation**&**Reward Model**           |
| 25.02 |                                                                                                  IBM Research                                                                                                   |                   arxiv                   |                     [Protecting Users From Themselves: Safeguarding Contextual Privacy in Interactions with Conversational Agents](https://arxiv.org/abs/2502.18509)                     |             **Contextual Privacy**&**Conversational Agents**&**User Privacy Protection**              |
| 25.02 |                                                                                     Indian Institute of Technology Roorkee                                                                                      |                   arxiv                   |                                         [Pruning as a Defense: Reducing Memorization in Large Language Models](https://arxiv.org/abs/2502.15796)                                         |                        **Pruning**&**Memorization Reduction**&**LLM Privacy**                         |
| 25.02 |                                                                                                Emory University                                                                                                 |                   arxiv                   |        [Tokens for Learning, Tokens for Unlearning: Mitigating Membership Inference Attacks in Large Language Models via Dual-Purpose Training](https://arxiv.org/abs/2502.19726)        |            **Membership Inference Attacks**&**Privacy Defense**&**Dual-Purpose Training**             |
| 25.02 |                                                                                            Seoul National University                                                                                            |                   arxiv                   |                     [FAITHUN: Toward Faithful Forgetting in Language Models by Investigating the Interconnectedness of Knowledge](https://arxiv.org/abs/2502.19207)                      |             **Faithful Unlearning**&**Knowledge Interconnectedness**&**Privacy in LLMs**              |
| 25.02 |                                                                                                 Duke University                                                                                                 |                 ICLR 2025                 |                    [Proactive Privacy Amnesia for Large Language Models: Safeguarding PII with Negligible Impact on Model Utility](https://arxiv.org/abs/2502.17591)                     |       **Privacy Protection**&**Machine Unlearning**&**Personal Identifiable Information (PII)**       |
| 25.03 |                                                           Xidian University, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences                                                            |                   arXiv                   |                        [PriFFT: Privacy-preserving Federated Fine-tuning of Large Language Models via Function Secret Sharing](https://arxiv.org/abs/2503.03146)                         |    **Privacy-preserving Federated Learning**&**Function Secret Sharing**&**Large Language Models**    |
| 25.03 |                                                                       Ben-Gurion University of the Negev, Nuclear Research Center ‚Äì Negev                                                                       |                   arXiv                   |                                                     [Token-Level Privacy in Large Language Models](https://arxiv.org/abs/2503.03652)                                                     |              **Token-Level Privacy**&**Large Language Models**&**Differential Privacy**               |
| 25.03 |                                                               The Hong Kong University of Science and Technology (Guangzhou), Tsinghua University                                                               | Workshop on GenAI Watermarking, ICLR 2025 |                              [MARK YOUR LLM: DETECTING THE MISUSE OF OPEN-SOURCE LARGE LANGUAGE MODELS VIA WATERMARKING](https://arxiv.org/abs/2503.04636)                               |                      **Open-Source LLMs**&**Watermarking**&**Misuse Detection**                       |
| 25.03 |                                                                                      University of Maryland, College Park                                                                                       |                   arxiv                   |                                              [Mitigating Memorization in LLMs using Activation Steering](https://arxiv.org/abs/2503.06040)                                               |              **Activation Steering**&**Memorization Mitigation**&**Sparse Autoencoders**              |
| 25.03 |                                                                                                Wuhan University                                                                                                 |                   arxiv                   |                                           [Privacy-Enhancing Paradigms within Federated Multi-Agent Systems](https://arxiv.org/abs/2503.08175)                                           |                        **Federated MAS**&**Privacy Protection**&**EPEAgents**                         |
| 25.03 |                                                                                                  FAIR at Meta                                                                                                   |                   arxiv                   |                                            [AgentDAM: Privacy Leakage Evaluation for Autonomous Web Agents](https://arxiv.org/abs/2503.09780)                                            |                         **LLM Agents**&**Privacy Leakage**&**Web Navigation**                         |
| 25.03 |                                                                                                      UCLA                                                                                                       |           Blogpost @ ICLR 2025            |                                    [Peeking Behind Closed Doors: Risks of LLM Evaluation by Private Data Curators](https://arxiv.org/abs/2503.04756)                                     |                     **Private Evaluation**&**LLM-as-a-Judge**&**Evaluation Bias**                     |
| 25.03 |                                                                                              Princeton University                                                                                               |                 ICLR 2025                 |                                                      [Privacy Auditing of Large Language Models](https://arxiv.org/abs/2503.06808)                                                       |             **Privacy Auditing**&**Membership Inference Attack**&**Differential Privacy**             |
| 25.03 |                                                                                          Huawei Munich Research Center                                                                                          |                   arxiv                   |                        [PrivacyScalpel: Enhancing LLM Privacy via Interpretable Feature Intervention with Sparse Autoencoders](https://arxiv.org/abs/2503.11232)                         |              **Privacy in LLMs**&**Sparse Autoencoders**&**Feature-Level Intervention**               |
| 25.03 |                                                                                               Zhejiang University                                                                                               |                   arxiv                   |                                [Safeguarding LLM Embeddings in End-Cloud Collaboration via Entropy-Driven Perturbation](https://arxiv.org/abs/2503.12896)                                |           **Embedding Privacy**&**End-Cloud Collaboration**&**Entropy-based Perturbation**            |
| 25.03 |                                                                                                 RMIT University                                                                                                 |                   arxiv                   |                                                   [Deep Contrastive Unlearning for Language Models](https://arxiv.org/abs/2503.14900)                                                    |                   **Machine Unlearning**&**Contrastive Learning**&**Data Privacy**                    |
| 25.03 |                                                                                             University of Maryland                                                                                              |                   arxiv                   |                                  [Prada: Black-Box LLM Adaptation with Private Data on Resource-Constrained Devices](https://arxiv.org/abs/2503.14932)                                   |              **Black-Box Adaptation**&**Data Privacy**&**Resource-Constrained Devices**               |
| 25.03 |                                                                                            Seoul National University                                                                                            |                   arxiv                   |                                         [Prompt Flow Integrity to Prevent Privilege Escalation in LLM Agents](https://arxiv.org/abs/2503.15547)                                          |                   **LLM Agents**&**Privilege Escalation**&**Prompt Flow Integrity**                   |
| 25.03 |                                                                               Beijing University of Posts and Telecommunications                                                                                |                   arxiv                   |                                              [Privacy-Aware RAG: Secure and Isolated Knowledge Retrieval](https://arxiv.org/abs/2503.15548)                                              |              **RAG Security**&**Encrypted Embeddings**&**Privacy-Preserving Retrieval**               |
| 25.03 |                                                                                Shanghai Jiao Tong University, University of Kent                                                                                |                   arxiv                   |                         [Using LLMs for Automated Privacy Policy Analysis: Prompt Engineering, Fine-Tuning and Explainability](https://arxiv.org/abs/2503.16516)                         |                     **Privacy Policy**&**Prompt Engineering**&**Explainability**                      |
| 25.03 |                                                                                  Henry Ford Health, Michigan State University                                                                                   |                   arxiv                   |                      [Autonomous Radiotherapy Treatment Planning Using DOLA: A Privacy-Preserving, LLM-Based Optimization Agent](https://arxiv.org/abs/2503.17553)                       |        **Radiotherapy Planning**&**LLM Agent**&**RAG**&**Reinforcement Learning**&**Privacy**         |
| 25.03 |                                                                        The Hong Kong Polytechnic University, Institute of Science Tokyo                                                                         |                   arxiv                   |                                             [Membership Inference Attacks on Large-Scale Models: A Survey](https://arxiv.org/abs/2503.19338)                                             |                **Membership Inference**&**LLMs Privacy**&**Multimodal Model Security**                |
| 25.03 |                                                                    University College London, King‚Äôs College Hospital, King‚Äôs College London                                                                    |                   arxiv                   |                                               [Clean & Clear: Feasibility of Safe LLM Clinical Guidance](https://arxiv.org/abs/2503.20953)                                               |                 **Clinical Guidance**&**Hallucination Mitigation**&**Healthcare LLM**                 |
| 25.03 |                                                                                             UL Research Institutes                                                                                              |                   arxiv                   |                              [Malicious and Unintentional Disclosure Risks in Large Language Models for Code Generation](https://arxiv.org/abs/2503.22760)                               |                 **Code Generation**&**Unintended Memorization**&**Disclosure Risks**                  |
| 25.04 |                                                                                               University of Oslo                                                                                                |                   arxiv                   |                             [Leaking LoRa: An Evaluation of Password Leaks and Knowledge Storage in Large Language Models](https://arxiv.org/abs/2504.00031)                             |                       **Cybersecurity**&**LoRA Fine-tuning**&**Model Editing**                        |
| 25.04 |                                                                               Beijing University of Posts and Telecommunications                                                                                |           USENIX Security 2025            |             [Whispering Under the Eaves: Protecting User Privacy Against Commercial and LLM-powered Automatic Speech Recognition Systems](https://arxiv.org/abs/2504.00858)              |                 **Audio Adversarial Examples**&**Speech Privacy**&**LLM-powered ASR**                 |
| 25.04 |                                                                                                   Amazon AGI                                                                                                    |                   arxiv                   |                                     [SemEval-2025 Task 4: Unlearning Sensitive Content from Large Language Models](https://arxiv.org/abs/2504.02883)                                     |                            **LLM Unlearning**&**Privacy**&**Benchmarking**                            |
| 25.04 |                                                                                             University of Michigan                                                                                              |                   arxiv                   |                                                    [Prœµœµmpt: Sanitizing Sensitive Prompts for LLMs](https://arxiv.org/abs/2504.05147)                                                    |          **Prompt Privacy**&**Format-Preserving Encryption**&**Metric Differential Privacy**          |
| 25.04 |                                                                                         Centre for Frontier AI Research                                                                                         |                   arxiv                   |                              [Position Paper: Rethinking Privacy in RL for Sequential Decision-making in the Age of LLMs](https://arxiv.org/abs/2504.11511)                              |                 **Privacy**&**Reinforcement Learning**&**Sequential Decision-making**                 |
| 25.04 |                                                                                                Korea University                                                                                                 |                IJCNN 2025                 |                                     [GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs](https://arxiv.org/abs/2504.12681)                                      |                  **Machine Unlearning**&**LLM Privacy**&**Multi-domain Forgetting**                   |
| 25.04 |                                                                                              University of Chicago                                                                                              |                   arxiv                   |                                       [How Private is Your Attention? Bridging Privacy with In-Context Learning](https://arxiv.org/abs/2504.16000)                                       |               **In-Context Learning**&**Differential Privacy**&**Attention Mechanism**                |
| 25.04 |                                                                                             Universit√© de Toulouse                                                                                              |          CL4Health @ NAACL 2025           |                                  [PatientDx: Merging Large Language Models for Protecting Data-Privacy in Healthcare](https://arxiv.org/abs/2504.17360)                                  |                       **Model Merging**&**Healthcare Privacy**&**LLMs for EHR**                       |
| 25.04 |                                                                                                Emory University                                                                                                 |               MedInfo 2025                |                           [Large Language Model Empowered Privacy-Protected Framework for PHI Annotation in Clinical Notes](https://arxiv.org/abs/2504.18569)                            |                    **PHI Annotation**&**Privacy-Preserving LLM**&**Clinical NLP**                     |
| 25.05 |                                                                                               Hokkaido University                                                                                               |                   arxiv                   |                                     [Can Differentially Private Fine-tuning LLMs Protect Against Privacy Attacks?](https://arxiv.org/abs/2504.21036)                                     |                     **Differential Privacy**&**Fine-tuning**&**Privacy Attacks**                      |
| 25.05 |                                                                                        University of Southern California                                                                                        |                   arxiv                   |                                         [Preserving Privacy and Utility in LLM-Based Product Recommendations](https://arxiv.org/abs/2505.00951)                                          |             **Recommendation System**&**Privacy Preservation**&**Large Language Models**              |
| 25.05 |                                                                                         Indiana University Bloomington                                                                                          |                   arxiv                   |                       [Safeguard-by-Development: A Privacy-Enhanced Development Paradigm for Multi-Agent Collaboration Systems](https://arxiv.org/abs/2505.04799)                        |                   **Multi-Agent Systems**&**Privacy Protection**&**LLM Frameworks**                   |
| 25.05 |                                                                                   University of North Carolina at Chapel Hill                                                                                   | Transactions on Machine Learning Research |                             [Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation](https://arxiv.org/abs/2505.01456)                             |                **Multimodal LLMs**&**Information Unlearning**&**Security Evaluation**                 |
| 25.05 |                                                                                              University of Houston                                                                                              |                   arxiv                   |             [User Behavior Analysis in Privacy Protection with Large Language Models: A Study on Privacy Preferences with Limited Data](https://arxiv.org/abs/2505.06305v1)              |            **Privacy Preference Modeling**&**Large Language Models**&**Few-shot Learning**            |
| 25.05 |                                                                               Eastern Switzerland University of Applied Sciences                                                                                |                   arxiv                   |                                              [Securing RAG: A Risk Assessment and Mitigation Framework](https://arxiv.org/abs/2505.08728v1)                                              |             **Retrieval-Augmented Generation**&**Security Framework**&**Risk Mitigation**             |
| 25.05 |                                                                                                Peking University                                                                                                |                   arxiv                   |                                      [Improved Algorithms for Differentially Private Language Model Alignment](https://arxiv.org/abs/2505.08849v1)                                       |                  **Differential Privacy**&**Language Model Alignment**&**DP-ADAMW**                   |
| 25.05 |                                                                                           Chinese Academy of Sciences                                                                                           |                   arxiv                   |                             [PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative In-Context Optimization](https://arxiv.org/abs/2505.09921v1)                             |               **Privacy Jailbreak**&**PII Extraction**&**Gradient-Based Optimization**                |
| 25.05 |                                                                                    Institute of Information Engineering, CAS                                                                                    |                S & P 2025                 |                          [Comet: Accelerating Private Inference for Large Language Model by Predicting Activation Sparsity](https://arxiv.org/abs/2505.07239v1)                          |                   **Private Inference**&**MPC**&**Activation Sparsity Prediction**                    |
| 25.05 |                                                                                               Cornell University                                                                                                |                   arxiv                   |         [Cross-Cloud Data Privacy Protection: Optimizing Collaborative Mechanisms of AI Systems by Integrating Federated Learning and LLMs](https://arxiv.org/abs/2505.13292v1)          |               **Federated Learning**&**Cross-Cloud Privacy**&**Large Language Models**                |
| 25.05 |                                                                                            Michigan State University                                                                                            |                   arxiv                   |                             [Beyond Text: Unveiling Privacy Vulnerabilities in Multi-modal Retrieval-Augmented Generation](https://arxiv.org/abs/2505.13957)                             |                         **MRAG**&**Privacy Leakage**&**Multi-modal Attacks**                          |
| 25.05 |                                                                             Mohamed bin Zayed University of Artificial Intelligence                                                                             |                   arxiv                   |                                      [Invisible Entropy: Towards Safe and Efficient Low-Entropy LLM Watermarking](https://arxiv.org/abs/2505.14112)                                      |                   **Watermarking**&**Entropy Prediction**&**Low-Entropy Detection**                   |
| 25.05 |                                                                                       University of Massachusetts Amherst                                                                                       |                   arxiv                   |                                                [Can Large Language Models Really Recognize Your Name?](https://arxiv.org/abs/2505.14549)                                                 |                  **PII Detection**&**Contextual Ambiguity**&**LLM Privacy Failures**                  |
| 25.05 |                                                                                                      HKUST                                                                                                      |                   arxiv                   |           [Context Reasoner: Incentivizing Reasoning Capability for Contextualized Privacy and Safety Compliance via Reinforcement Learning](https://arxiv.org/abs/2505.14585)           |               **Contextual Integrity**&**Legal Compliance**&**Reinforcement Learning**                |
| 25.05 |                                                                                                Yonsei University                                                                                                |                   arxiv                   |                                             [SEPS: A Separability Measure for Robust Unlearning in LLMs](https://arxiv.org/abs/2505.14832v1)                                             |             **Large Language Models**&**Machine Unlearning**&**Mixed Prompt Evaluation**              |
| 25.05 |                                                                                                Yonsei University                                                                                                |                   arxiv                   |                                                       [DUSK: Do Not Unlearn Shared Knowledge](https://arxiv.org/abs/2505.15209v1)                                                        |                       **Machine Unlearning**&**Shared Knowledge**&**Benchmark**                       |
| 25.05 |                                                                                                Hongik University                                                                                                |                   arxiv                   |                                                    [R-TOFU: Unlearning in Large Reasoning Models](https://arxiv.org/abs/2505.15214v1)                                                    |                **Machine Unlearning**&**Large Reasoning Models**&**Chain-of-Thought**                 |
| 25.05 |                                                                                  University of Science and Technology of China                                                                                  |                   arxiv                   |                                  [UniErase: Unlearning Token as a Universal Erasure Primitive for Language Models](https://arxiv.org/abs/2505.15674v1)                                   |                       **Unlearning Token**&**Model Editing**&**LLM Forgetting**                       |
| 25.05 |                                                                                               Beihang University                                                                                                |                   arxiv                   |                                  [A Federated Splitting Framework for LLMs: Security, Efficiency, and Adaptability](https://arxiv.org/abs/2505.15683v1)                                  |                       **Federated Learning**&**Split Learning**&**LLM Privacy**                       |
| 25.05 |                                                                               The Hong Kong University of Science and Technology                                                                                |                   arxiv                   |                                        [DuFFin: A Dual-Level Fingerprinting Framework for LLMs IP Protection](https://arxiv.org/abs/2505.16530v1)                                        |                    **Fingerprinting**&**IP Protection**&**Black-box Verification**                    |
| 25.05 |                                                                                      The Hong Kong Polytechnic University                                                                                       |                   arxiv                   |                                [Unlearning Isn‚Äôt Deletion: Investigating Reversibility of Machine Unlearning in LLMs](https://arxiv.org/abs/2505.16831v1)                                |             **Machine Unlearning**&**Representation Analysis**&**Reversible Forgetting**              |
| 25.05 |                                                                                              University of Florida                                                                                              |                   arxiv                   |                                                  [In-Context Watermarks for Large Language Models](https://arxiv.org/abs/2505.16934v1)                                                   |                 **In-Context Watermarking**&**Prompt Engineering**&**LLM Provenance**                 |
| 25.05 |                                                                                           University of South Florida                                                                                           |                   arxiv                   |                    [Invisible Prompts, Visible Threats: Malicious Font Injection in External Resources for Large Language Models](https://arxiv.org/abs/2505.16957v1)                    |                    **Font Injection**&**Indirect Prompt Attack**&**LLM Security**                     |
| 25.05 |                                                                                          Shanghai Jiao Tong University                                                                                          |                 ICML 2025                 |                                              [An Efficient Private GPT Never Autoregressively Decodes](https://arxiv.org/abs/2505.15252v1)                                               |                     **Private GPT**&**Secure Inference**&**Speculative Decoding**                     |
| 25.05 |                                                                                Ritual, Stanford University, Columbia University                                                                                 | arxiv | [An Attack to Break Permutation-Based Private Third-Party Inference Schemes for LLMs](https://arxiv.org/abs/2505.18332) | **Privacy**&**Permutation**&**Inference Attack** |
| 25.05 |                                                                               University of Science, VNU-HCM, Indiana University                                                                                | arxiv | [Harry Potter is Still Here! Probing Knowledge Leakage in Targeted Unlearned Large Language Models via Automated Adversarial Prompting](https://arxiv.org/abs/2505.17160v1) | **Knowledge Unlearning**&**Adversarial Prompting**&**Knowledge Leakage** |
| 25.05 |                                                                                          Shanghai Jiao Tong University                                                                                          | arxiv | [Automated Privacy Information Annotation in Large Language Model Interactions](https://arxiv.org/abs/2505.20910) | **Privacy Detection**&**LLM Interaction**&**Automated Annotation** |
| 25.05 |                                                                                 Hong Kong University of Science and Technology                                                                                  | arxiv | [Privacy-preserving Prompt Personalization in Federated Learning for Multimodal Large Language Models](https://arxiv.org/abs/2505.22447) | **Federated Learning**&**Prompt Personalization**&**Privacy Protection** |
| 25.05 | Carnegie Mellon University | arxiv | [Breaking the Gold Standard: Extracting Forgotten Data under Exact Unlearning in Large Language Models](https://arxiv.org/abs/2505.24379v1) | **Unlearning**&**Data Extraction**&**Privacy Attack** |
| 25.06 | Michigan State University | arxiv | [Keeping an Eye on LLM Unlearning: The Hidden Risk and Remedy](https://arxiv.org/abs/2506.00359v1) | **LLM Unlearning**&**Stealthy Attack**&**Scope-aware Defense** |
| 25.06 | Southern Illinois University | arxiv | [Evaluating Apple Intelligence's Writing Tools for Privacy Against Large Language Model-Based Inference Attacks: Insights from Early Datasets](https://arxiv.org/abs/2506.03870v1) | **Privacy Protection**&**LLM Inference Attack**&**Apple Intelligence** |
| 25.06 | Hong Kong Polytechnic University | arxiv | [Privacy and Security Threat for OpenAI GPTs](https://arxiv.org/abs/2506.04036v1) | **Custom GPT**&**Instruction Leaking**&**Privacy Risk** |
| 25.06 | University of Arizona | arxiv | [Learning to Diagnose Privately: DP-Powered LLMs for Radiology Report Classification](https://arxiv.org/abs/2506.04450v1) | **Privacy Preserving Large Language Models**&**Radiology Report Classification**&**Differential Privacy** |
| 25.06 | Florida International University | arxiv | [FedShield-LLM: A Secure and Scalable Federated Fine-Tuned Large Language Model](https://arxiv.org/abs/2506.05640v1) | **Federated Learning**&**LLM**&**Homomorphic Encryption**&**LoRA** |
| 25.06 | Harbin Institute of Technology, Kuaishou Technology, Monash University, Fudan University | arxiv | [Simple Yet Effective: Extracting Private Data Across Clients in Federated Fine-Tuning of Large Language Models](https://arxiv.org/abs/2506.06060v1) | **Federated Learning**&**LLM**&**PII Extraction**&**Privacy Attack** |
| 25.06 | Southeast University, Peking University, Institute of Automation (CASIA), Zhejiang University, Nanyang Technological University, Technische Universit√§t Chemnitz, University of California Los Angeles | arxiv | [Dual-Priv Pruning: Efficient Differential Private Fine-Tuning in Multimodal Large Language Models](https://arxiv.org/abs/2506.07077) | **Differential Privacy**&**Multimodal LLM**&**Fine-Tuning**&**Token Pruning** |
| 25.06 | Beijing University of Posts and Telecommunications, Shandong University, Institute of Automation (CAS), State Key Laboratory of Multimodal Artificial Intelligence Systems | arxiv | [LLM Unlearning Should Be Form-Independent](https://arxiv.org/abs/2506.07795) | **LLM Unlearning**&**Form-Dependent Bias**&**Knowledge Editing**&**ROCR** |
| 25.06 | Michigan State University, IBM T. J. Watson Research Center | arxiv | [SoK: Machine Unlearning for Large Language Models](https://arxiv.org/abs/2506.09227v1) | **Machine Unlearning**&**Large Language Model**&**Suppression**&**Removal**&**Evaluation** |
| 25.06 | University of Virginia | arxiv | [Disclosure Audits for LLM Agents](https://arxiv.org/abs/2506.10171v1) | **Privacy Auditing**&**LLM Agent**&**Conversational Manipulation**&**Multi-Turn Attack**&**Contextual Integrity** |
| 25.06 | Supervised Program for Alignment Research (SPAR) | arxiv | [Prompt Attacks Reveal Superficial Knowledge Removal in Unlearning Methods](https://arxiv.org/abs/2506.10236v1) | **Machine Unlearning**&**LLM Safety**&**Prompt Attack**&**Knowledge Removal**&**Evasion Evaluation** |
| 25.06 | UC San Diego | arxiv | [Can We Infer Confidential Properties of Training Data from LLMs?](https://arxiv.org/abs/2506.10364v1) | **Property Inference**&**LLM Privacy**&**Confidential Dataset**&**Shadow Model Attack**&**Word Frequency** |
| 25.06 | University of Rome Tor Vergata, Almawave S.p.A. | ACL 2025 | [Private Memorization Editing: Turning Memorization into a Defense to Strengthen Data Privacy in Large Language Models](https://arxiv.org/abs/2506.10024v1) | **Model Editing**&**LLM Privacy**&**Memorization**&**PII Removal**&**Training Data Extraction** |
| 25.06 | MIT | arxiv | [UCD: Unlearning in LLMs via Contrastive Decoding](https://arxiv.org/abs/2506.12097v1) | **Machine Unlearning**&**Contrastive Decoding**&**Large Language Models** |
| 25.06 | Google Research | arxiv | [Privacy Reasoning in Ambiguous Contexts](https://arxiv.org/abs/2506.12241v1) | **Privacy Reasoning**&**Contextual Ambiguity**&**LLMs**&**Agentic Privacy**&**Context Disambiguation** |
| 25.06 | University of Massachusetts Amherst | arxiv | [OpenUnlearning: Accelerating LLM Unlearning via Unified Benchmarking of Methods and Metrics](https://arxiv.org/abs/2506.12618v1) | **LLM Unlearning**&**Benchmark**&**Evaluation Metrics**&**Privacy**&**SimNPO** |
| 25.06 | CSIRO‚Äôs Data61 | arxiv | [SoK: The Privacy Paradox of Large Language Models: Advancements, Privacy Risks, and Mitigation](https://arxiv.org/abs/2506.12699v1) | **Large Language Models**&**Privacy**&**Systematization of Knowledge**&**PII Leakage**&**Mitigation** |
| 25.06 | Ben-Gurion University of the Negev | arxiv | [LexiMark: Robust Watermarking via Lexical Substitutions to Enhance Membership Verification of an LLM's Textual Training Data](https://arxiv.org/abs/2506.14474v1) | **Watermarking**&**Membership Inference**&**High-Entropy Words**&**LLM Security**&**Semantic Preservation** |
| 25.06 | UC San Diego | arxiv | [Learning-Time Encoding Shapes Unlearning in LLMs](https://arxiv.org/abs/2506.15076v1) | **Unlearning**&**Paraphrasing**&**Data Entanglement**&**Selective Forgetting**&**LLM Fine-tuning** |
| 25.06 | Cornell University | arxiv | [Approximating Language Model Training Data from Weights](https://arxiv.org/abs/2506.15553v1) | **Training Data Approximation**&**Gradient Matching**&**Open-Weights Models**&**Coreset Selection**&**Data Recovery** |
| 25.06 | Parameter Lab, University of Mannheim | arxiv | [Leaky Thoughts: Large Reasoning Models Are Not Private Thinkers](https://arxiv.org/abs/2506.15674v1) | **Privacy Leakage**&**Reasoning Trace**&**LLM Agents**&**Test-Time Compute**&**Prompt Injection** |
| 25.06 | Institute of Computing Technology, Chinese Academy of Sciences | arxiv | [PhantomHunter: Detecting Unseen Privately-Tuned LLM-Generated Text via Family-Aware Learning](https://arxiv.org/abs/2506.15683v1) | **LLM-Generated Text Detection**&**Family-Aware Learning**&**Privately-Tuned LLM**&**Contrastive Learning**&**Mixture-of-Experts** |
| 25.06 | Zhejiang University | arxiv | [MEraser: An Effective Fingerprint Erasure Approach for Large Language Models](https://arxiv.org/abs/2506.12551v1) | **Fingerprint Erasure**&**Backdoor Removal**&**Model Ownership**&**LLM Security**&**LoRA Adapter** |
| 25.06 | Technical University of Munich | ICML 2025 Workshop | [Align-then-Unlearn: Embedding Alignment for LLM Unlearning](https://arxiv.org/abs/2506.13181v1) | **Unlearning**&**Semantic Embedding**&**LLMs**&**Concept Forgetting**&**Privacy** |
| 25.06 | Seoul National University | arxiv | [Privacy-Preserving LLM Interaction with Socratic Chain-of-Thought Reasoning and Homomorphically Encrypted Vector Databases](https://arxiv.org/abs/2506.17336v1) | **Privacy-Preserving**&**Chain-of-Thought**&**Encrypted Retrieval** |
| 25.06 | Xi'an Jiaotong-Liverpool University | arxiv | [Semantic-Preserving Adversarial Attacks on LLMs: An Adaptive Greedy Binary Search Approach](https://arxiv.org/abs/2506.18756) | **Adversarial Attacks**&**Large Language Models**&**Prompt Engineering** |
| 25.06 | Beijing University of Posts and Telecommunications | arxiv | Automated Detection of Pre-training Text in Black-box LLMs | Pre-training Text Detection&Black-box LLMs&Membership Inference |
| 25.06 | Institute of Information Engineering, Chinese Academy of Sciences | arxiv | PrivacyXray: Detecting Privacy Breaches in LLMs through Semantic Consistency and Probability Certainty | Privacy Detection&Large Language Models&Model Inner States |
| 25.06 | China Agricultural University | arxiv | Retrieval-Confused Generation is a Good Defender for Privacy Violation Attack of Large Language Models | Retrieval-Confused Generation&Privacy Violation Attack&Large Language Models |
| 25.06 | University of California, Santa Barbara | arxiv | [MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation](https://arxiv.org/abs/2506.20737v1) | **Multi-Agent Privacy**&**Benchmark**&**LLM Evaluation** |
| 25.06 | Yangzhou University | arxiv | [CodeGuard: A Generalized and Stealthy Backdoor Watermarking for Generative Code Models](https://arxiv.org/abs/2506.20926v1) | **Watermarking**&**Code Model**&**Backdoor** |
| 25.06 | Hong Kong University of Science and Technology | CCS 2025 | [Differentiation-Based Extraction of Proprietary Data from Fine-Tuned LLMs](https://arxiv.org/abs/2506.17353v1) | **Data Extraction**&**SFT Attack**&**Defense** |
| 25.06 | University of Technology Sydney, CSIRO Data61, Zhejiang Lab | arxiv | [SoK: Semantic Privacy in Large Language Models](https://arxiv.org/abs/2506.23603v1) | **Semantic Privacy**&**LLM Privacy**&**Attack** |
| 25.06 | National Taiwan University, National Cheng Kung University, Mohamed bin Zayed University of Artificial Intelligence | RACS 2024 | [Towards Building Private LLMs: Exploring Multi-Node Expert Parallelism on Apple Silicon for Mixture-of-Experts Large Language Model](https://arxiv.org/abs/2506.23635v1) | **Expert Parallelism**&**Private LLM**&**Performance Analysis** |
| 25.07 | University of Utah | arxiv | [Blackbox Dataset Inference for LLM](https://arxiv.org/abs/2507.03619v1) | **Dataset Inference**&**Blackbox**&**Copyright** |
| 25.07 | American Express | arxiv | [Model Inversion Attacks on Llama 3: Extracting PII from Large Language Models](https://arxiv.org/abs/2507.04478v1) | **Model Inversion Attack**&**Privacy**&**PII** |
| 25.07 | Mohamed bin Zayed University of Artificial Intelligence | arxiv | [DP-Fusion: Token-Level Differentially Private Inference for Large Language Models](https://arxiv.org/abs/2507.04531v1) | **Differential Privacy**&**Inference**&**Paraphrasing** |
| 25.07 | University of California, Berkeley | arxiv | [The Landscape of Memorization in LLMs: Mechanisms, Measurement, and Mitigation](https://arxiv.org/abs/2507.05578v1) | **Memorization**&**Privacy**&**Mitigation** |
| 25.07 | The Chinese University of Hong Kong | arxiv | [Entropy-Memorization Law: Evaluating Memorization Difficulty of Data in LLMs](https://arxiv.org/abs/2507.06056v1) | **Memorization**&**Entropy**&**Privacy** |
| 25.07 | Nanyang Technological University | arxiv | [The Man Behind the Sound: Demystifying Audio Private Attribute Profiling via Multimodal Large Language Model Agents](https://arxiv.org/abs/2507.10016v1) | **Audio Privacy**&**MLLM**&**Profiling** |
| 25.07 | University of Wisconsin - Madison | arxiv | [Benchmarking LLM Privacy Recognition for Social Robot Decision Making](https://arxiv.org/abs/2507.16124v1) | **Privacy**&**LLM**&**Robotics** |
| 25.07 | Tsinghua University | arxiv | [LoRA-Leak: Membership Inference Attacks Against LoRA Fine-tuned Language Models](https://arxiv.org/abs/2507.18302v1) | **Membership Inference**&**LoRA**&**Privacy** |
| 25.07 | Shanghai Jiao Tong University | USENIX Security 2025 | [Depth Gives a False Sense of Privacy: LLM Internal States Inversion](https://arxiv.org/abs/2507.16372v1) | **Privacy**&**Inversion Attack**&**LLM** |
| 25.07 | University of Luxembourg | arxiv | [Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement Learning](https://arxiv.org/abs/2507.22565) | **DifferentialPrivacy**&**ReinforcementLearning**&**LLM** |
| 25.07 | Meta | arxiv | [Privacy Artifact ConnecTor (PACT): Embedding Enterprise Artifacts for Compliance AI Agents](https://arxiv.org/abs/2507.21142) | **Privacy**&**Embedding**&**Compliance** |
| 25.08 |                                         The University of Texas at El Paso, Texas A&M University-Central Texas, Cisco Systems Inc., Amazon Web Services                                          |      arxiv      |      [PRvL: Quantifying the Capabilities and Risks of Large Language Models for PII Redaction](https://arxiv.org/abs/2508.05545)      |      **PII Redaction**&**Large Language Models**&**Data Privacy** |
| 25.07 |                                         Monash University                                         |      RecSys      |      [Privacy Risks of LLM-Empowered Recommender Systems: An Inversion Attack Perspective](https://arxiv.org/abs/2508.03703)      |      **Recommender Systems**&**Large Language Models**&**Privacy Risks**&**Model Inversion Attack** |
| 25.07 |                                         Monash University                                         |      RecSys      |      [Privacy Risks of LLM-Empowered Recommender Systems: An Inversion Attack Perspective](https://arxiv.org/abs/2508.03703)      |      **Recommender Systems**&**Large Language Models**&**Privacy Risks**&**Model Inversion Attack** |
| 25.08 | Ko√ß University | arxiv | [Win-k: Improved Membership Inference Attacks on Small Language Models](https://arxiv.org/abs/2508.01268) | **Small language models**&**privacy**&**AI security**&**membership inference attacks**|
| 25.08 | University of Science and Technology of China, State Key Laboratory of Cognitive Intelligence | arxiv | [FPEdit: Robust LLM Fingerprinting through Localized Knowledge Editing](httpshttps://arxiv.org/abs/2508.02092) | **LLM Fingerprinting**&**Knowledge Editing**&**Intellectual Property Protection**&**Backdoor Purification**|
| 25.08 | Beijing University of Posts and Telecommunications | arxiv | [WHISPERING AGENTS: AN EVENT-DRIVEN COVERT COMMUNICATION PROTOCOL FOR THE INTERNET OF AGENTS](https://arxiv.org/abs/2508.02188) | **Covert Communication**&**Steganography**&**Internet of Agents**&**Security**&**Multi-Agent Systems**|
| 25.08 | InspiringGroup @Tsinghua University | arxiv | [Agentic Privacy-Preserving Machine Learning](https://arxiv.org/abs/2508.02836) | **Privacy-Preserving Machine Learning**&**Agentic AI**&**Large Language Models**&**Secure Inference**|
| 25.08 | Emory University, Illinois Institute of Technology | arxiv | [Privacy-Aware Decoding: Mitigating Privacy Leakage of Large Language Models in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.03098) | **Retrieval-Augmented Generation (RAG)**&**Privacy Leakage**&**Differential Privacy**&**Decoding Strategies**|
| 25.08 | Institute of Information Engineering, CAS & School of Cyberspace Security, UCAS, Northeastern University, Nanyang Technological University, Sun Yat-sen University | arxiv | [GEOSHIELD: SAFEGUARDING GEOLOCATION PRIVACY FROM VISION-LANGUAGE MODELS VIA ADVERSARIAL PERTURBATIONS](https://arxiv.org/abs/2508.03209) | **Vision-Language Models**&**Geolocation Privacy**&**Adversarial Perturbations**&**Feature Disentanglement**|
| 25.08 | Carnegie Mellon University, University of Bristol, Clemson University, Portland State University | arxiv | [Galaxy: A Cognition-Centered Framework for Proactive, Privacy-Preserving, and Self-Evolving LLM Agents](https://arxiv.org/abs/2508.03991) | **LLM Agents**&**Cognitive Architecture**&**Proactive Behavior**&**Privacy-Preserving**&**Self-Evolving Systems**|
| 25.08 | Harbin Engineering University | arxiv | [Adaptive Backtracking for Privacy Protection in Large Language Models](https://arxiv.org/abs/2508.06087v1) | **Privacy Protection**&**Adaptive Backtracking**&**Retrieval-Augmented Generation** |
| 25.08 | Universit√© Toulouse Capitole | arxiv | [Leveraging LLMs for Privacy-Aware Predictions in Participatory Budgeting](https://arxiv.org/abs/2508.06577v1) | **Participatory Budgeting**&**Privacy Preservation**&**Computational Social Choice** |
| 25.08 | Carnegie Mellon University | arxiv | [1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent Reasoning](https://arxiv.org/abs/2508.07667v1) | **Contextual Privacy**&**Multi-Agent Reasoning**&**Information Flow** |
| 25.08 | Superset Labs PBC | arxiv | [Can We Trust AI to Govern AI? Benchmarking LLM Performance on Privacy and AI Governance Exams](https://arxiv.org/abs/2508.09036v1) | **AI Governance**&**Privacy Certification**&**LLM Benchmarking** |
| 25.08 | Hainan University | arxiv | [Security Analysis of ChatGPT: Threats and Privacy Risks](https://arxiv.org/abs/2508.09426v1) | **ChatGPT**&**Privacy Risks**&**Security Threats** |
| 25.08 | Zhejiang University | arxiv | [Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference](https://arxiv.org/abs/2508.09442v1) | **KV-cache Privacy**&**Inversion/Collision/Injection Attacks**&**Obfuscation Defense** |
| 25.08 | Georgia Tech | arxiv | [Searching for Privacy Risks in LLM Agents via Simulation](https://arxiv.org/abs/2508.10880v1) | **LLM Agents**&**Privacy Risks**&**Simulation Framework** |
| 25.08 | Technical University of Munich | HAIPS @ CCS 2025 | [LLM-as-a-Judge for Privacy Evaluation? Exploring the Alignment of Human and LLM Perceptions of Privacy in Textual Data](httpshttps://arxiv.org/abs/2508.12158) | **LLM-as-a-Judge**&**Privacy**&**User Study** |
| 25.08 | The Pennsylvania State University | arxiv | [Exposing Privacy Risks in Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2508.17222v1) | **Graph RAG**&**Privacy Risks**&**Data Extraction Attacks** |
| 25.08 | University of Maryland, Baltimore County | arxiv | [Membership Inference Attacks on LLM-based Recommender Systems](https://arxiv.org/abs/2508.18665v1) | **Membership Inference**&**LLM Recommender Systems**&**Privacy Attacks** |
| 25.08 | Santa Clara University | arxiv | [RL-Finetuned LLMs for Privacy-Preserving Synthetic Rewriting](https://arxiv.org/abs/2508.19286v1) | **Privacy-Preserving Rewriting**&**Reinforcement Learning**&**Stylometric Obfuscation** |
| 25.08 | Shandong University | arxiv | [Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered Smartphone Agents](https://arxiv.org/abs/2508.19493v1) | **Smartphone Agents**&**Privacy Awareness**&**SAPA-Bench** |
| 25.09 | Kennesaw State University | IEEE Internet of Things Journal | [A Survey: Towards Privacy and Security in Mobile Large Language Models](https://arxiv.org/abs/2509.02411v1) | **Mobile LLMs**&**Privacy**&**Security** |
| 25.09 | University of Texas at El Paso | arxiv | [AI-in-the-Loop: Privacy Preserving Real-Time Scam Detection and Conversational Scambaiting by Leveraging LLMs and Federated Learning](https://arxiv.org/abs/2509.05362v1) | **Scam Detection**&**Privacy-Preserving AI**&**Federated Learning** |
| 25.09 | Microsoft | arxiv | [Cross-Service Threat Intelligence in LLM Services using Privacy-Preserving Fingerprints](https://arxiv.org/abs/2509.05608v1) | **Prompt Injection**&**Threat Intelligence**&**Privacy-Preserving Fingerprints** |
| 25.09 | Shandong University | arxiv | [Dataset Ownership in the Era of Large Language Models](https://arxiv.org/abs/2509.05921v1) | **Dataset Ownership**&**Watermarking**&**Copyright Protection** |
| 25.09 | IBM Research | arxiv | [Efficient Decoding Methods for Language Models on Encrypted Data](https://arxiv.org/abs/2509.08383v1) | **Homomorphic Encryption**&**Secure LLM Inference**&**Efficient Decoding** |
| 25.09 | Kennesaw State University | arxiv | [DP-FedLoRA: Privacy-Enhanced Federated Fine-Tuning for On-Device Large Language Models](https://arxiv.org/abs/2509.09097v1) | **Federated Learning**&**Differential Privacy**&**LoRA** |
| 25.09 | National University of Defense Technology | arxiv | [ENSI: Efficient Non-Interactive Secure Inference for Large Language Models](https://arxiv.org/abs/2509.09424v1) | **Homomorphic Encryption**&**Secure Inference**&**LLMs** |
| 25.09 | Chinese Academy of Sciences | DASFAA 2025 | [Towards Confidential and Efficient LLM Inference with Dual Privacy Protection](https://arxiv.org/abs/2509.09091v1) | **Trusted Execution Environment**&**Differential Privacy**&**Private Inference** |
| 25.09 | Microsoft | arxiv | [AVEC: Bootstrapping Privacy for Local LLMs](https://arxiv.org/abs/2509.10561) | **Differential Privacy**&**Edge LLMs**&**Verifiable Computation** |
| 25.09 | University of Missouri, SRI International | arxiv | [Privacy-Aware In-Context Learning for Large Language Models](https://arxiv.org/abs/2509.13625) | **Differential Privacy**&**In-Context Learning**&**Synthetic Data Generation** |
| 25.09 | China University of Petroleum | arxiv | [DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models](https://arxiv.org/abs/2509.13702) | **Hallucination Suppression**&**Contrastive Proxy Models**&**Dynamic Inference Steering** |
| 25.09 | University of Maryland Baltimore County | arxiv | [FedMentor: Domain-Aware Differential Privacy for Heterogeneous Federated LLMs in Mental Health](https://arxiv.org/abs/2509.14275) | **Federated Learning**&**Differential Privacy**&**Mental Health LLMs** |
| 25.09 | Purdue University, Alibaba Group | IEEE Bulletin on Data Engineering | [Beyond Data Privacy: New Privacy Risks for Large Language Models](https://arxiv.org/abs/2509.14278) | **LLM Privacy Risks**&**Information Exfiltration**&**Automated Social Engineering** |
| 25.09 | UNC Chapel Hill, The University of Texas at Austin | arxiv | [The Sum Leaks More Than Its Parts: Compositional Privacy Risks and Mitigations in Multi-Agent Collaboration](https://arxiv.org/abs/2509.14284) | **Compositional Privacy Leakage**&**Multi-Agent LLM Systems**&**Collaborative Defense** |
| 25.09 | Studio Legale Fabiano, International Institute of Informatics and Systemics (IIIS) | arxiv | [Affective Computing and Emotional Data: Challenges and Implications in Privacy Regulations, The AI Act, and Ethics in Large Language Models](https://arxiv.org/abs/2509.20153) | **Artificial Intelligence**&**Emotional Data**&**Large Language Models (LLM)**&**Data Protection**&**Privacy**&**Ethics** |
| 25.09 | University of South Florida | arxiv | [RAG Security and Privacy: Formalizing the Threat Model and Attack Surface](https://arxiv.org/abs/2509.20324) | **Retrieval augmented generation**&**large language models**&**privacy**&**differential privacy** |
| 25.09 | Beihang University & University at Buffalo & University of North Texas | arxiv | [Can Federated Learning Safeguard Private Data in LLM Training? Vulnerabilities, Attacks, and Defense Evaluation](https://arxiv.org/abs/2509.20680) | **Federated Learning**&**Privacy Leakage**&**LLM Security** |
| 25.09 | The Arctic University of Norway | arxiv | [GEP: A GCG-Based Method for Extracting Personally Identifiable Information from Chatbots Built on Small Language Models](https://arxiv.org/abs/2509.21192) | **Small Language Models**&**PII Leakage**&**Gradient-based Extraction** |
| 25.09 | Wuhan University & Microsoft | EMNLP 2025 Findings | [Privacy in Action: Towards Realistic Privacy Mitigation and Evaluation for LLM-Powered Agents](https://arxiv.org/abs/2509.17488) | **Privacy Mitigation**&**Model Context Protocol**&**Agent2Agent Framework** |




## üíªPresentations & Talks


## üìñTutorials & Workshops

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |

## üì∞News & Articles
| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.11 | News  |Wild: GPT-3.5 leaked a random dude's photo in the output. | [link](https://twitter.com/thealexker/status/1719896871009694057) |

## üßë‚Äçüè´Scholars
