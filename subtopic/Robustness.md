# Robustness

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating with the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"

## üìëPapers

| Date  |                                                                Institute                                                                 |                                     Publication                                     |                                                                                       Paper                                                                                       |                                             Keywords                                             |
|:-----:|:----------------------------------------------------------------------------------------------------------------------------------------:|:-----------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:------------------------------------------------------------------------------------------------:|
| 23.02 |                                                            Microsoft Research                                                            | ICLR 2023(workshop on Trustworthy and Reliable Large-Scale Machine Learning Models) |                               [On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective](https://arxiv.org/abs/2302.12095)                                |        **Robustness Evaluation**&**Adversarial Robustness**&**Out-of-Distribution (OOD)**        |
| 23.05 |                                                      Harbin Institute of Technology                                                      |                                      NAACL2024                                      |                          [Enhancing Large Language Models Against Inductive Instructions with Dual-critique Prompting](https://arxiv.org/abs/2305.13733)                          |         **Large Language Models**&**Inductive Instructions**&**Dual-critique Prompting**         |
| 23.05 |                                      National Key Laboratory for Multimedia Information Processing                                       |                                      NAACL2024                                      |                                  [DialogVCS: Robust Natural Language Understanding in Dialogue System Upgrade](https://arxiv.org/abs/2305.14751)                                  |      **Natural Language Understanding**&**Dialogue System**&**Multi-label Classification**       |
| 23.06 |                                                University of Illinois at Urbana-Champaign                                                |                                        arxiv                                        |                                  [DECODINGTRUST: A Comprehensive Assessment of Trustworthiness in GPT Models](https://arxiv.org/abs/2306.11698)                                   |                        **Robustness**&**Ethics**&**Privacy**&**Toxicity**                        |
| 23.08 |                                  CISPA Helmholtz Center for Information Security & Tsinghua University                                   |                                        arxiv                                        |           [Robustness Over Time: Understanding Adversarial Examples‚Äô Effectiveness on Longitudinal Versions of Large Language Models](https://arxiv.org/abs/2308.07847)           |                         **Longitudinal Study**&**Robustness Assessment**                         |
| 23.11 |                                                                   CMU                                                                    |                          AACL2023(ART or Safety workshop)                           |                                                        [Measuring Adversarial Datasets](https://arxiv.org/abs/2311.03566)                                                         |                **Adversarial Robustness**&**AI Safety**&**Adversarial Datasets**                 |
| 23.11 |                                                            Amazon Alexa AI-NU                                                            |                                        arXiv                                        |                                           [JAB: Joint Adversarial Prompting and Belief Augmentation](https://arxiv.org/abs/2311.09473)                                            |                 **Adversarial Prompting**&T**oxicity Reduction**&**Robustness**                  |
| 23.11 |                                                            Amazon Alexa AI-NU                                                            |                                      NAACL2024                                      |                                           [JAB: Joint Adversarial Prompting and Belief Augmentation](https://arxiv.org/abs/2311.09473)                                            |                 **Adversarial Prompting**&T**oxicity Reduction**&**Robustness**                  |
| 23.11 |                                                        Michigan State University                                                         |                                 NAACL2024(findings)                                 |                               [A Robust Semantics-based Watermark for Large Language Models against Paraphrasing](https://arxiv.org/abs/2311.08721)                               |                     **Watermark**&**Large Language Models**&**Paraphrasing**                     |
| 24.01 |                                   University of Trento, Concordia University, Mila-Quebec AI Institute                                   |                                        arxiv                                        |                                                     [Are LLMs Robust for Spoken Dialogues?](https://arxiv.org/abs/2401.02297)                                                     |         **Task-Oriented Dialogues**&**Automatic Speech Recognition**&**Error Analysis**          |
| 24.01 |                                             School of Computer Science University of Windsor                                             |                                        arxiv                                        | [Finding a Needle in the Adversarial Haystack: A Targeted Paraphrasing Approach For Uncovering Edge Cases with Minimal Distribution Distortion](https://arxiv.org/abs/2401.11373) |           **Adversarial Attacks**&**Targeted Paraphrasing**&**Reinforcement Learning**           |
| 24.02 |                                                         University of Cambridge                                                          |                                        arxiv                                        |                       [Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment](https://arxiv.org/abs/2402.14016)                       |                       **LLM as a Judge**&**Universal Adversarial Attacks**                       |
| 24.02 |                                                      Technical University of Munich                                                      |                                        arxiv                                        |                         [Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images](https://arxiv.org/abs/2402.14899)                         |    **Multimodal Large Language Models**&**Chain-of-Thought Reasoning**&**Adversarial Images**    |
| 24.03 |                         Beijing Institute of Technology, The University of Sydney, Hong Kong Baptist University                          |                                        arxiv                                        |                                        [Few-Shot Adversarial Prompt Learning on Vision-Language Models](https://arxiv.org/abs/2403.14774)                                         |             **Few-Shot Learning**&**Adversarial Prompt**&**Vision-Language Models**              |
| 24.03 |                                                                   UIUC                                                                   |                                      NAACL2024                                      |                                                       [Fact Checking Beyond Training Set](https://arxiv.org/abs/2403.18671)                                                       |                 **Fact Checking**&**Domain Adaptation**&**Adversarial Training**                 |
| 24.03 |                                       Institute of Data Science, National University of Singapore                                        |                                      NAACL2024                                      |                      [SemRoDe: Macro Adversarial Training to Learn Representations That are Robust to Word-Level Attacks](https://arxiv.org/abs/2403.18423)                       |            **Adversarial Training**&**Word-Level Attacks**&**Robust Representations**            |
| 24.03 |                                                         Georgia State University                                                         |                                 NAACL2024(findings)                                 |                      [RobustSentEmbed: Robust Sentence Embeddings Using Adversarial Self-Supervised Contrastive Learning](https://arxiv.org/abs/2403.11082)                       |            **Sentence Embeddings**&**Adversarial Learning**&**Contrastive Learning**             |
| 24.04 | University of Chinese Academy of Sciences, Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences |                                     COLING 2024                                     |                          [Humanizing Machine-Generated Content: Evading AI-Text Detection through Adversarial Attack](https://arxiv.org/abs/2404.01907)                           |          **Adversarial Attack**&**AI-Text Detection**&**Dynamic Adversarial Learning**           |
| 24.04 |                  Institute for Intelligent Computing, Alibaba Group; School of Information Management, Wuhan University                  |                                        arxiv                                        |                           [Enhance Robustness of Language Models Against Variation Attack through Graph Integration](https://arxiv.org/abs/2404.12014)                            |                       **Chinese Adversarial Attacks**&**Variation Graph**                        |
| 24.05 |                                                       Mila, Universit√© de Montr√©al                                                       |                                        arxiv                                        |                                        [Efficient Adversarial Training in LLMs with Continuous Attacks](https://arxiv.org/abs/2405.15589)                                         |                         **Adversarial Training**&**Continuous Attacks**                          |
| 24.05 |                                                         University of Edinburgh                                                          |                                        arxiv                                        |                    [Evaluating the Adversarial Robustness of Retrieval-Based In-Context Learning for Large Language Models](https://arxiv.org/abs/2405.15984)                     |        **Adversarial Robustness**&**In-Context Learning**&**Retrieval-Augmented Methods**        |
| 24.05 |                                              Tokyo University of Agriculture and Technology                                              |                                        arxiv                                        |                                 [Large Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent](https://arxiv.org/abs/2405.20770)                                  |                   **Adversarial Robustness**&**LLM Agent**&**Textual Attacks**                   |
| 24.06 |                                                           Polytechnic of Porto                                                           |                                      DCAI2024                                       |                                      [Adversarial Evasion Attack Efficiency against Large Language Models](https://arxiv.org/abs/2406.08050)                                      |                     **Adversarial Attacks**&**Robustness**&**Cybersecurity**                     |
| 24.06 |                                                     National University of Singapore                                                     |                                      ICML2024                                       |                    [Helpful or Harmful Data? Fine-tuning-free Shapley Attribution for Explaining Language Model Predictions](https://arxiv.org/abs/2406.04606)                    | **Fine-tuning-free Shapley Attribution**&**Instance Attribution**&**Language Model Predictions** |
| 24.06 |                                                                  KAIST                                                                   |                                        arxiv                                        |                            [Adversarial Style Augmentation via Large Language Model for Robust Fake News Detection](https://arxiv.org/abs/2406.11260)                             |                    **Adversarial Style Augmentation**&**Fake News Detection**                    |
| 24.07 |                                        Hong Kong University of Science and Technology (Guangzhou)                                        |                                        arxiv                                        |                        [On Evaluating The Performance of Watermarked Machine-Generated Texts Under Adversarial Attacks](https://arxiv.org/abs/2407.04794)                         |            **Watermarked Texts**&**Adversarial Attacks**&**Machine-Generated Texts**             |
| 24.07 |                                                                  FAR AI                                                                  |                                        arxiv                                        |                                                  [Exploring Scaling Trends in LLM Robustness](https://arxiv.org/abs/2407.18213)                                                   |                  **Scaling Trends**&**LLM Robustness**&**Adversarial Training**                  |
| 24.07 |                                                          University of Alicante                                                          |                                        arxiv                                        |                        [Detecting and Understanding Vulnerabilities in Language Models via Mechanistic Interpretability](https://arxiv.org/abs/2407.19842)                        |       **Mechanistic Interpretability**&**Adversarial Attacks**&**Vulnerability Detection**       |
| 24.07 |                                                                Microsoft                                                                 |                                        arxiv                                        |                                           [Can LLMs be Fooled? Investigating Vulnerabilities in LLMs](https://arxiv.org/abs/2407.20529)                                           |                     **Vulnerabilities**&**Adversarial Attacks**&**Security**                     |
| 24.08 |                                                Guilin University of Electronic Technology                                                |                                        arxiv                                        |                 [Enhancing Robustness in Large Language Models: Prompting for Mitigating the Impact of Irrelevant Information](https://arxiv.org/abs/2408.10615)                  |                            **Prompt-Tuning**&**Arithmetic Reasoning**                            |
| 24.09 |                                                         Northwestern University                                                          |                                        arxiv                                        |                           [PROMPTFUZZ: Harnessing Fuzzing Techniques for Robust Testing of Prompt Injection in LLMs](https://arxiv.org/abs/2409.14729)                            |                        **Prompt Injection**&**Fuzzing**&**LLM Security**                         |
| 24.10 |                               Hong Kong University of Science and Technology, Beijing Jiaotong University                                |                                        arxiv                                        |             [AnyAttack: Towards Large-scale Self-supervised Generation of Targeted Adversarial Examples for Vision-Language Models](https://arxiv.org/abs/2410.05346)             |         **Adversarial Examples**&**Vision-Language Models**&**Self-supervised Learning**         |
| 24.10 |                                     University of Science and Technology of China, Tencent YouTu Lab                                     |                                 ACM Multimedia 2024                                 |            [Break the Visual Perception: Adversarial Attacks Targeting Encoded Visual Tokens of Large Vision-Language Models](https://doi.org/10.1145/3664647.3680779)            |             **Large Vision-Language Model**&**Adversarial Attack**&**Image Encoder**             |
| 24.11 |                                                     National University of Singapore                                                     |                                        arXiv                                        |                                       [Reasoning Robustness of LLMs to Adversarial Typographical Errors](https://arxiv.org/abs/2411.05345)                                        |               **Chain-of-Thought**&**Adversarial Typo**&**Robustness Evaluation**                |
| 24.12 |                                                           Tsinghua University                                                            |                                        arxiv                                        |                         [Seeker: Towards Exception Safety Code Generation with Intermediate Language Agents Framework](https://arxiv.org/abs/2412.11713)                          |                **Exception Handling**&**Intermediate Language (IL)**&**Deep-RAG**                |
| 25.01 |                                  Conversational AI and Social Analytics (CAISA) Lab, University of Bonn                                  |                                        arxiv                                        |                             [ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving](https://arxiv.org/abs/2501.08203)                              |                 **Robustness Testing**&**Math Problem Solving**&**Noisy Inputs**                 |
| 25.01 |                                                      University of Central Florida                                                       |                                        arxiv                                        |                          [I Can Find You in Seconds! Leveraging Large Language Models for Code Authorship Attribution](https://arxiv.org/abs/2501.08165)                          |        **Code Authorship Attribution**&**Zero-Shot Prompting**&**Adversarial Robustness**        |
| 25.01 |                                                           Macquarie University                                                           |                                        arxiv                                        |                                [SpaLLM-Guard: Pairing SMS Spam Detection Using Open-source and Commercial LLMs](https://arxiv.org/abs/2501.04985)                                 |               **SMS Spam Detection**&**Adversarial Robustness**&**Concept Drift**                |
| 25.03 |                                                                Dynamo AI                                                                 |                             ICBINB Workshop @ ICLR 2025                             |                                    [Know Thy Judge: On the Robustness Meta-Evaluation of LLM Safety Judges](https://arxiv.org/abs/2503.04474)                                     |                 **LLM Safety Evaluation**&**Robustness**&**Adversarial Attacks**                 |
| 25.03 |                                                         Simon Fraser University                                                          |                                        arxiv                                        |                 [Quantifying the Robustness of Retrieval-Augmented Language Models Against Spurious Features in Grounding Data](https://arxiv.org/abs/2503.05587)                 |        **Retrieval-Augmented Generation**&**Spurious Features**&**Robustness Evaluation**        |
| 25.03 |                                                                  Cohere                                                                  |                                        arxiv                                        |                                    [Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts](https://arxiv.org/abs/2503.09347)                                    |                 **LLM-as-a-Judge**&**Safety Evaluation**&**Artifact Robustness**                 |
| 25.04 |                                                           Southeast University                                                           |                                        arxiv                                        |                                [How does Watermarking Affect Visual Language Models in Document Understanding?](https://arxiv.org/abs/2504.01048)                                 |          **Visual Language Models**&**Document Understanding**&**Watermark Robustness**          |
| 25.04 |                                                         Imperial College London                                                          |                         Building Trust Workshop @ ICLR 2025                         |                                    [ENHANCING LLM ROBUSTNESS TO PERTURBED INSTRUCTIONS: AN EMPIRICAL STUDY](https://arxiv.org/abs/2504.02733)                                     |                **LLM Robustness**&**Instruction Perturbation**&**Self-Denoising**                |
| 25.04 |                                               University of North Carolina at Chapel Hill                                                |                                        arxiv                                        |                                 [Breach in the Shield: Unveiling the Vulnerabilities of Large Language Models](https://arxiv.org/abs/2504.03714)                                  |                    **LLM Robustness**&**Stability Measure**&**Model Merging**                    |
| 25.04 |                                                           Tsinghua University                                                            |                                        arxiv                                        |                                      [QAVA: Query-Agnostic Visual Attack to Large Vision-Language Models](https://arxiv.org/abs/2504.11038)                                       |         **Visual Adversarial Attack**&**LVLM Security**&**Query-Agnostic Perturbation**          |
| 25.04 |                                              University of Science and Technology of China                                               |                                      CVPR 2025                                      |                       [R-TPT: Improving Adversarial Robustness of Vision-Language Models through Test-Time Prompt Tuning](https://arxiv.org/abs/2504.11195)                       |        **Vision-Language Models**&**Adversarial Robustness**&**Test-Time Prompt Tuning**         |
| 25.04 |                                                           Texas A&M University                                                           |                                      WOOT 2025                                      |               [Making Acoustic Side-Channel Attacks on Noisy Keyboards Viable with LLM-Assisted Spectrograms‚Äô ‚ÄúTypo‚Äù Correction](https://arxiv.org/abs/2504.11622)                |   **Acoustic Side-Channel Attack**&**Spectrogram Typo Correction**&**LLM-Assisted Inference**    |
| 25.04 |                                                   The Hong Kong Polytechnic University                                                   |                                      KDD 2024                                       |                                     [CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent](https://arxiv.org/abs/2504.13192)                                     |               **Recommender Systems**&**Adversarial Attacks**&**LLMs-based Agent**               |
| 25.05 |                                                          Ben Gurion University                                                           |                                      ISIT 2025                                      |                                          [Optimized Couplings for Watermarking Large Language Models](https://arxiv.org/abs/2505.08878)                                           |              **LLM Watermarking**&**Hypothesis Testing**&**Coupling Optimization**               |
| 25.05 |                                                             Tufts University                                                             |                                        arxiv                                        |                                 [Noise Injection Systemically Degrades Large Language Model Safety Guardrails](https://arxiv.org/abs/2505.13500)                                  |                  **Safety Fine-Tuning**&**Activation Noise**&**LLM Robustness**                  |
| 25.05 |                                                   The Hong Kong Polytechnic University                                                   |                                      ICML 2025                                      |                              [Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?](https://arxiv.org/abs/2505.12871v1)                               |                    **LoRA**&**Training-Time Attacks**&**Robustness Analysis**                    |
| 25.06 | Anhui University | arxiv | [Robustness of Prompting: Enhancing Robustness of Large Language Models Against Prompting Attacks](https://arxiv.org/abs/2506.03627v1) | **LLM Robustness**&**Prompting Attack**&**Error Correction** |
| 25.06 | University of Massachusetts Amherst | arxiv | [Chain-of-Code Collapse: Reasoning Failures in LLMs via Adversarial Prompting in Code Generation](https://arxiv.org/abs/2506.06971v2) | **Code Generation**&**Reasoning Robustness**&**Adversarial Prompting**&**Semantic Perturbation** |
| 25.06 | Peking University, Huawei Technologies | arxiv | [More Vulnerable than You Think: On the Stability of Tool-Integrated LLM Agents](https://arxiv.org/abs/2506.21967v1) | **Tool-Integrated Agents**&**Stability**&**Vulnerability** |
| 25.07 | Queen‚Äôs University | arxiv | [On the Robustness of Verbal Confidence of LLMs in Adversarial Attacks](https://arxiv.org/abs/2507.06489v1) | **Verbal Confidence**&**Adversarial Attack**&**Robustness** |
| 25.07 | UC Berkeley | arxiv | [Small Edits, Big Consequences: Telling Good from Bad Robustness in Large Language Models](https://arxiv.org/abs/2507.15868v1) | **Robustness**&**LLM**&**Sensitivity** |
| 25.08 | York University | IEEE VIS 2025 | [The Perils of Chart Deception: How Misleading Visualizations Affect Vision-Language Models](https://arxiv.org/abs/2508.09716v1) | **Misleading Visualizations**&**Vision-Language Models**&**Chart Reasoning** |
| 25.08 | National University of Singapore, Nanyang Technological University, Tsinghua University | arxiv | [When Audio and Text Disagree: Benchmarking Text Bias in Large Audio-Language Models under Cross-Modal Inconsistencies](https://arxiv.org/abs/2508.15407) | **Audio-Language Models**&**Text Bias**&**Benchmark** |
| 25.09 | Independent researcher | arxiv | [BioBlue: Notable runaway-optimiser-like LLM failure modes on biologically and economically aligned AI safety benchmarks for LLMs with simplified observation format](https://arxiv.org/abs/2509.02655v1) | **AI Safety**&**Runaway Optimisation**&**Alignment Benchmarks** |
| 25.09 | Bocconi University | arxiv | [Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation](https://arxiv.org/abs/2509.08825v1) | **LLM Hacking**&**Data Annotation**&**Statistical Validity** |


## üíªPresentations & Talks


## üìñTutorials & Workshops

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |

## üì∞News & Articles

## üßë‚Äçüè´Scholars