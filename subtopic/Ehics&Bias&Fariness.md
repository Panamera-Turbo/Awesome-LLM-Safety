# Ethics

## Different from the main READMEüïµÔ∏è

- Within this subtopic, we will be updating witha the latest articles. This will help researchers in this area to quickly understand recent trends.
- In addition to providing the most recent updates, we will also add keywords to each subtopic to help you find content of interest more quickly.
- Within each subtopic, we will also update with profiles of scholars we admire and endorse in the field. Their work is often of high quality and forward-looking!"


## üìëPapers

| Date  |                                                                                      Institute                                                                                       |            Publication             |                                                                                    Paper                                                                                    |                                           Keywords                                            |
|:-----:|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------------------:|
| 23.06 |                                                                      University of Illinois at Urbana-Champaign                                                                      |               arxiv                |                               [DECODINGTRUST: A Comprehensive Assessment of Trustworthiness in GPT Models](https://arxiv.org/abs/2306.11698)                                |                      **Robustness**&**Ethics**&**Privacy**&**Toxicity**                       |
| 23.11 |                                                                                Allen Institute for AI                                                                                |               arxiv                |                                   [Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs](https://arxiv.org/abs/2311.04892)                                    |                                   **Bias**&**Stereotypes**                                    |
| 23.11 |                                                                                   Adobe Inc. India                                                                                   |               arxiv                |                     [All Should Be Equal in the Eyes of Language Models: Counterfactually Aware Fair Text Generation](https://arxiv.org/abs/2311.05451)                     |                                    **Fairness**&**Biases**                                    |
| 23.11 |                                                                         National Taiwan University, Meta AI                                                                          |               arXiv                |                          [Step by Step to Fairness: Attributing Societal Bias in Task-oriented Dialogue Systems](https://arxiv.org/abs/2311.06513)                          |               **Task-oriented Dialogue Systems**&**Societal Bias**&**Fairness**               |
| 23.11 |                                                                          UNC Chapel Hill, IBM Research MIT                                                                           |               arxiv                |                             [Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion](https://arxiv.org/abs/2311.07682)                              |                **Model Fusion**&**Bias Reduction**&**Selective Memorization**                 |
| 23.11 |                                                                                      ETH Z√ºrich                                                                                      |               arxiv                |                   [Navigating the Ocean of Biases: Political Bias Attribution in Language Models via Causal Structures](https://arxiv.org/abs/2311.08605)                   |                          **Political Debates**&**Bias Attribution**                           |
| 23.11 |                                                                     University of Pisa, University of Copenhagen                                                                     |               arxiv                |                                     [Social Bias Probing: Fairness Benchmarking for Language Models](https://arxiv.org/abs/2311.09090)                                      |             **Social Biases**&**Fairness Benchmarking**&**Identity Stereotypes**              |
| 23.11 |                                                                                 Tsinghua University                                                                                  |               arxiv                |     [ALIGNMENT IS NOT SUFFICIENT TO PREVENT LARGE LANGUAGE MODELS FROM GENERATING HARMFUL INFORMATION: A PSYCHOANALYTIC PERSPECTIVE](https://arxiv.org/abs/2311.08487)      |                      **Alignment**&**Psychoanalysis Theory**&**Ethics**                       |
| 23.11 |                                                                    University of Toronto, University of Michigan                                                                     |               arxiv                |                     [Aligning with Whom? Large Language Models Have Gender and Racial Biases in Subjective NLP Tasks](https://arxiv.org/abs/2311.09730)                     |                                **Gender Bias**&**Racial Bias**                                |
| 23.11 |                                                    University of Michigan, University of Hawaii at Hilo, Northeastern University                                                     |               arxiv                |                                      [MOKA: Moral Knowledge Augmentation for Moral Event Extraction](https://arxiv.org/abs/2311.09733)                                      |          **Moral Event Extraction**&**MOKA Framewor**k&**External Moral Knowledge**           |
| 23.11 |              Mila - Quebec AI Institute, Universit√© du Qu√©bec √† Montr√©al, Data & Society Research Institute, Mantium, IBM Research, University of California Riverside               |               arxiv                |                                  [Subtle Misogyny Detection and Mitigation: An Expert-Annotated Dataset](https://arxiv.org/abs/2311.09443)                                  |                **Subtle Misogyny**&B**iasly Dataset**&**Dataset Development**                 |
| 23.11 |                                                   Illinois Institute of Technology, University of Illinois Chicago, Cisco Research                                                   |               arxiv                |                             [Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language Models](https://arxiv.org/abs/2311.09428)                             |              **Fairness**&**Abusive Language Detection**&**Adversarial Attacks**              |
| 23.11 |                                                  Cornell University, KTH Royal Institute of Technology, University of Pennsylvania                                                   |               arxiv                |                                              [Auditing and Mitigating Cultural Bias in LLMs](https://arxiv.org/abs/2311.14096)                                              |                          **Cultural Bias**&**Mitigation Strategies**                          |
| 23.11 |                                                                        University College London, Holistic AI                                                                        |               arxiv                |                            [Towards Auditing Large Language Models: Improving Text-based Stereotype Detection](https://arxiv.org/abs/2311.14126)                            |                                   **Stereotype Detection**                                    |
| 23.11 |                                                                           Georgia Institute of Technology                                                                            |               arxiv                |                                 [Evaluating Large Language Models through Gender and Racial Stereotypes](https://arxiv.org/abs/2311.14788)                                  |                **Gender Bias**&**Racial Stereotypes**&**Evaluation Framework**                |
| 23.11 |                                             Polytechnique Montr√©al, √âTS Montr√©al, CISPA-Helmholtz Center for Information Security, Mila                                              |               arxiv                |                                           [Survey on AI Ethics: A Socio-technical Perspective](https://arxiv.org/abs/2311.17228)                                            |                     **AI Ethics&****Trustworthiness**&**Responsibility**                      |
| 23.11 |                                                                                         Meta                                                                                         |             EMNLP2023              |                                   [ROBBIE: Robust Bias Evaluation of Large Generative Language Models](https://arxiv.org/abs/2311.18140)                                    |                         **Bias Evaluation**&**Fairness**&**Toxicity**                         |
| 23.11 |                                                                      Comcast Applied AI, University of Waterloo                                                                      |               arxiv                |                       [What Do Llamas Really Think? Revealing Preference Biases in Language Model Representations](https://arxiv.org/abs/2311.18812)                        |                **Implicit Bias**&**Sociodemographic Bias**&B**ias Mitigation**                |
| 23.11 |                                                             Shanghai Artificial Intelligence Laboratory&Fudan University                                                             |             NAACL2024              |                                         [FLAMES: Benchmarking Value Alignment of LLMs in Chinese](https://arxiv.org/abs/2311.06899)                                         |                            **Value Alignment**&**LLMs**&**Safety**                            |
| 23.11 |                                                     University of Michigan&University of Hawaii at Hilo&Northeastern University                                                      |             NAACL2024              |                                      [MOKA: Moral Knowledge Augmentation for Moral Event Extraction](https://arxiv.org/abs/2311.09733)                                      |              **Moral Knowledge Augmentation**&**Moral Event Extraction**&**NLP**              |
| 23.12 |                                                          University of Auckland, University of Waikato, University of Macau                                                          |               arXiv                |                      [Tackling Bias in Pre-trained Language Models: Current Trends and Under-represented Societies](https://arxiv.org/abs/2312.01509)                       |                             **Governmental Regulations**&**Bias**                             |
| 23.12 |                            Yildiz Technical University, Istanbul Technical University, Maersk Mc-kinney Moeller Institute University of Southern Denmark                             |               arXiv                |                     [Developing Linguistic Patterns to Mitigate Inherent Human Bias in Offensive Language Detection](https://arxiv.org/abs/2312.01787)                      |                         **Offensive Language**&**Data-augmentation**                          |
| 23.12 |                                                                                  TCS Research India                                                                                  |        EMNLP2023(Workshop)         |                     [Towards Mitigating Perceived Unfairness in Contracts from a Non-Legal Stakeholder‚Äôs Perspective](https://arxiv.org/abs/2312.01398)                     |                 **Fairness**&**Non-legal Stakeholders**&**Data Augmentation**                 |
| 23.12 |                                                                                   Korea University                                                                                   |             EMNLP2023              |                            [Improving Bias Mitigation through Bias Experts in Natural Language Understanding](https://arxiv.org/abs/2312.03577)                             |                      **Bias Mitigation**&**Multi-Class Classification**                       |
| 23.12 |                                                          Queensland University of Technology, University of New South Wales                                                          |        EMNLP2023(Workshop)         |           [Measuring Misogyny in Natural Language Generation: Preliminary Results from a Case Study on two Reddit Communities](https://arxiv.org/abs/2312.03330)            |                      **Misogyny**&**Toxicity Classifier**s&**Benchmark**                      |
| 23.12 |                                   Carnegie Mellon University, Pittsburgh, Universidade NOVA de Lisboa, Allen Institute for Artificial Intelligence                                   |             EMNLP2023              |                          [Understanding the Effect of Model Compression on Social Bias in Large Language Models](https://arxiv.org/abs/2312.05662)                          |               **Model Compression**&**Social Bias**&&**Knowledge Distillation**               |
| 23.12 |                                 Eindhoven University of Technology, University of Liverpool, Griffith University, The Pennsylvania State University                                  |               arXiv                |                             [GPTBIAS: A Comprehensive Framework for Evaluating Bias in Large Language Models](https://arxiv.org/abs/2312.06315)                             |           **Bias Evaluation**&**Bias Attack Instructions**&**Intersectional Bias**            |
| 23.12 |                                                                                   IBM Research AI                                                                                    |              AAAI2024              |                        [SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models](https://arxiv.org/abs/2312.07492)                        |                        **Social Bias**&**Question-answering Dataset**                         |
| 23.12 |                                                                       New York University, CUNY Queens College                                                                       |               arxiv                |                         [Red AI? Inconsistent Responses from GPT Models on Political Issues in the US and China](https://arxiv.org/abs/2312.09917)                          |                                     **Bias**&**Politics**                                     |
| 23.12 |                                                                  University of California Los Angeles, Amazon Alexa                                                                  |               arXiv                |     [Are you talking to [‚Äòxem‚Äô] or [‚Äòx‚Äô ‚Äòem‚Äô]? On Tokenization and Addressing Misgendering in LLMs with Pronoun Tokenization Parity](https://arxiv.org/abs/2312.11779)      |                                        **Gender Bias**                                        |
| 23.12 |                                                       University of Oxford, University Canada West, Amazon Web Services (AWS)                                                        |               arxiv                |                                               [Large Language Model (LLM) Bias Index‚ÄîLLMBI](https://arxiv.org/abs/2312.14769)                                               |                          **Bias Quantification**&**Bias Mitigation**                          |
| 23.12 | Institute of Information Engineering Chinese Academy of Sciences, School of Cyber Security University of Chinese Academy of Sciences, JD AI Research Beijing, Independent Researcher |               arXiv                |                                             [A Group Fairness Lens for Large Language Models](https://arxiv.org/abs/2312.15478)                                             |                              **Group Fairness**&**Social Bias**                               |
| 24.01 |                                                                             Hong Kong Baptist University                                                                             |               arxiv                |                         [GOAT-Bench: Safety Insights to Large Multimodal Models through Meme-Based Social Abuse](https://arxiv.org/abs/2401.01523)                          |      **Large Multimodal Models (LMMs)**&**Meme-Based Social Abuse**&**Safety Insights**       |
| 24.01 |                                                                       Beihang University, Westcliff University                                                                       |               arxiv                | [Unveiling Bias in Fairness Evaluations of Large Language Models: A Critical Literature Review of Music and Movie Recommendation Systems](https://arxiv.org/abs/2401.04057) | **Music and Movie Recommendations**&**Recommender Systems**&**Fairness Evaluation Framework** |
| 24.01 |                                                            Pacific Northwest National Laboratory, University of Michigan                                                             |               arxiv                |                    [Whose wife is it anyway? Assessing bias against same-gender relationships in machine translation](https://arxiv.org/abs/2401.04972)                     |                         **Gender Bias**&**Same-Gender Relationships**                         |
| 24.01 |                             Universit√© Jean Monnet Saint-Etienne, CNRS Institut d'Optique Graduate School, T√©l√©com Paris Institut Polytechnique de Paris                             |               arxiv                |                            [An investigation ofstructures responsible for gender bias in BERT and DistilBERT](https://arxiv.org/abs/2401.06495)                             |                                  **Fairness**&**Imbalance**                                   |
| 24.01 |                                                                                University of Toronto                                                                                 |               arXiv                |  [Ethical Artificial Intelligence Principles and Guidelines for the Governance and Utilization of Highly Advanced Large Language Models](https://arxiv.org/abs/2401.10745)  |                                 **Ethical AI**&**Governance**                                 |
| 24.02 |                                                University of Manchester, Idiap Research Institute, National Biomarker Centre CRUK-MI                                                 |               arxiv                |                      [Enhancing Ethical Explanations of Large Language Models through Iterative Symbolic Refinement](https://arxiv.org/abs/2402.00745)                      |                     **Ethical Reasoning**&**Neuro-Symbolic Integration**                      |
| 24.02 |                                                                       Indian Institute of Technology Kharagpur                                                                       |               arxiv                |    [How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries](https://arxiv.org/abs/2402.15302)     |                 **Instruction-centric Responses**&**Ethical Vulnerabilities**                 |
| 24.02 |                                                                                         HBKU                                                                                         |          LREC-COLING 2024          |                      [Can GPT-4 Identify Propaganda? Annotation and Detection of Propaganda Spans in News Articles](https://arxiv.org/abs/2402.17478)                       |                     **Propaganda Span Detection**&**Zero-shot Learning**                      |
| 24.02 |                                                            University of Pisa&University of Edinburgh&Bocconi University                                                             |               arxiv                |                                        [FAIRBELIEF ‚Äì Assessing Harmful Beliefs in Language Models](https://arxiv.org/abs/2402.17389)                                        |                              **Beliefs Assessment**&**Fairness**                              |
| 24.03 |                                      √âcole polytechnique f√©d√©rale de Lausanne, Carnegie Mellon University, University of Maryland College Park                                       |               arxiv                |                              ["Flex Tape Can‚Äôt Fix That": Bias and Misinformation in Edited Language Models](https://arxiv.org/abs/2403.00180)                              |                   **Model Editing**&**Demographic Bias**&**Misinformation**                   |
| 24.03 |                                                          University of Maryland, University of Antwerp, New York University                                                          |               arxiv                |                                        [Evaluating LLMs for Gender Disparities in Notable Persons](https://arxiv.org/abs/2403.09148)                                        |                           **Bias**&**Fairness**&**Hallucinations**                            |
| 24.03 |                                                                                 Tsinghua University                                                                                  |               arxiv                |                                                [Evaluation Ethics of LLMs in Legal Domain](https://arxiv.org/abs/2403.11152)                                                |                            **Legal Domain**&**Ethics Evaluation**                             |
| 24.03 |                                                                    University of Science and Technology of China,                                                                    |               arxiv                |                                      [Locating and Mitigating Gender Bias in Large Language Models](https://arxiv.org/abs/2403.14409)                                       |                              **Causal Intervention**&**Debias**                               |
| 24.03 |                                                                       University of Illinois Urbana-Champaign                                                                        |               arxiv                |                           [A Moral Imperative: The Need for Continual Superalignment of Large Language Models](https://arxiv.org/abs/2403.14683)                            |                               **Superalignment**&**AI Ethics**                                |
| 24.03 |                                                                               Fondazione Bruno Kessler                                                                               |        NAACL2024(findings)         |                                      [NLP for Counterspeech against Hate: A Survey and How-To Guide](https://arxiv.org/abs/2403.20103)                                      |                           **Counterspeech**&**Online Hate**&**NLP**                           |
| 24.04 |                                                                                   Google Research                                                                                    |        NAACL2024(findings)         |                      [Modeling the Sacred: Considerations when Using Religious Texts in Natural Language Processing](https://arxiv.org/abs/2404.14740)                      |                **Religious Texts**&**Natural Language Processing**&**Ethics**                 |
| 24.05 |                                                                     Northwestern University, Rutgers University                                                                      |               arxiv                |                                           [Large Language Model Agent for Fake News Detection](https://arxiv.org/abs/2405.01593)                                            |                         **Fake News Detection**&**Agentic Approach**                          |
| 24.05 |                                                                         Vector Institute, Queen‚Äôs University                                                                         |               arxiv                |                              [BiasKG: Adversarial Knowledge Graphs to Induce Bias in Large Language Models](https://arxiv.org/abs/2405.04756)                               |                    **Adversarial Knowledge Graph**&**Bias**&**AI Safety**                     |
| 24.05 |                                                                                 Zhejiang University                                                                                  |               arxiv                |                             [Large Language Model Bias Mitigation from the Perspective of Knowledge Editing](https://arxiv.org/abs/2405.09341)                              |                           **Bias Mitigation**&**Knowledge Editing**                           |
| 24.05 |                                                                                University of Waterloo                                                                                |               arxiv                |                                         [UnMarker: A Universal Attack on Defensive Watermarking](https://arxiv.org/abs/2405.08363)                                          |                         **Deepfake Watermarking**&**Adversarial ML**                          |
| 24.06 |                                                                      Sa√Ød Business School, University of Oxford                                                                      |               arxiv                |                             [How Ethical Should AI Be? How AI Alignment Shapes the Risk Preferences of LLMs](https://arxiv.org/abs/2406.01168)                              |          **AI Alignment**&**Risk Preferences**&**AI in Finance**&**Underinvestment**          |
| 24.06 |                                                                                University of Catania                                                                                 |               arxiv                |                           [Do Language Models Understand Morality? Towards a Robust Detection of Moral Content](https://arxiv.org/abs/2406.04143)                           |                      **Value Detection**&**Natural Language Inference**                       |
| 24.06 |                                                                  Korea Advanced Institute of Science and Technology                                                                  |         ACL 2024 Findings          |                       [Ask LLMs Directly, "What shapes your bias?": Measuring Social Bias in Large Language Models](https://arxiv.org/abs/2406.04064)                       |           **Social Bias**&**Bias Measurement**&**QA Format**&**Social Perception**            |
| 24.06 |                                                                                  Rutgers University                                                                                  |               arxiv                |                                                  [MoralBench: Moral Evaluation of LLMs](https://arxiv.org/abs/2406.04428)                                                   |                              **Moral Evaluation**&**MoralBench**                              |
| 24.06 |                                                                                     Cornell Tech                                                                                     |               arxiv                |                           [Annotation Alignment: Comparing LLM and Human Annotations of Conversational Safety](https://arxiv.org/abs/2406.06369)                            |                      **Annotation Alignment**&**Conversational Safety**                       |
| 24.06 |                                                                     University of North Carolina at Chapel Hill                                                                      |               arxiv                |                                   [Exploring Safety-Utility Trade-Offs in Personalized Language Models](https://arxiv.org/abs/2406.11107)                                   |    **Safety-Utility Trade-Offs**&**Personalized Language Models**&**Personalization Bias**    |
| 24.06 |                                                                                  Auburn University                                                                                   |               arxiv                |                             [Investigating Annotator Bias in Large Language Models for Hate Speech Detection](https://arxiv.org/abs/2406.11109)                             |                         **Annotator Bias**&**Hate Speech Detection**                          |
| 24.06 |                                                                                 Syracuse University                                                                                  |               arxiv                |                          [Global Data Constraints: Ethical and Effectiveness Challenges in Large Language Model](https://arxiv.org/abs/2406.11214)                          |        **Global Data Constraints**&**Ethical Challenges**&**Effectiveness Challenges**        |
| 24.06 |                                                                                University of Maryland                                                                                |              ACL 2024              |                 [Do Large Language Models Discriminate in Hiring Decisions on the Basis of Race, Ethnicity, and Gender?](https://arxiv.org/abs/2406.10486)                  |                            **Discrimination**&**Hiring Decisions**                            |
| 24.06 |                                                                                University of Maryland                                                                                |               arxiv                |                                 [Large Language Models are Biased Because They Are Large Language Models](https://arxiv.org/abs/2406.13138)                                 |                        **LLM Bias**&**Bias Mitigation**&**Bias in AI**                        |
| 24.06 |                        Shanghai Jiao Tong University, Carnegie Mellon University, Fudan University, Shanghai AI Laboratory, Generative AI Research Lab (GAIR)                        |               arxiv                |                                         [BEHONEST: Benchmarking Honesty of Large Language Models](https://arxiv.org/abs/2406.13261)                                         |                       **LLM Honesty**&**Benchmarking**&**Consistency**                        |
| 24.06 |                                                           Vector Institute, Scotia Bank, Ernst & Young, Queen‚Äôs University                                                           |               arxiv                |                                     [Mitigating Social Biases in Language Models through Unlearning](https://arxiv.org/abs/2406.13551)                                      |                        **Social Biases**&**Unlearning**&**Debiasing**                         |
| 24.06 |                                                                        ELLIS Alicante, University of Alicante                                                                        |               arxiv                |                              [Leveraging Large Language Models to Measure Gender Bias in Gendered Languages](https://arxiv.org/abs/2406.13677)                              |                  **Gender Bias**&**Bias Quantification**&**Spanish Corpora**                  |
| 24.06 |                                          South China University of Technology, Pazhou Laboratory, University of Maryland, Baltimore County                                           |               arxiv                |                          [GenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large Language Models](https://arxiv.org/abs/2406.13925)                          |             **Gender Bias Mitigation**&**Alignment Dataset**&**Bias Categories**              |
| 24.06 |                            CAS Key Laboratory of AI Safety, CAS Key Lab of Network Data Science and Technology, University of Chinese Academy of Sciences                            |               arxiv                |                     [Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective](https://arxiv.org/abs/2406.14023)                      |                **Psychometric Evaluation**&**Bias Attacks**&**Ethical Risks**                 |
| 24.06 |                                                                              University College London                                                                               |               arxiv                |                            [JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models](https://arxiv.org/abs/2406.15484)                            |                       **Gender Bias**&**Hiring Bias**&**Benchmarking**                        |
| 24.06 |                                                                          The University of Texas at Austin                                                                           |               arxiv                |                                 [Navigating LLM Ethics: Advancements, Challenges, and Future Directions](https://arxiv.org/abs/2406.18841)                                  |                    **LLM Ethics**&**Accountable LLM**&**Responsible LLM**                     |
| 24.07 |                                                                               George Mason University                                                                                |               arxiv                |               [Breaking Bias, Building Bridges: Evaluation and Mitigation of Social Biases in LLMs via Contact Hypothesis](https://arxiv.org/abs/2407.02030)                |                           **Social Biases**&**Contact Hypothesis**                            |
| 24.07 |                                                                 Bangladesh University of Engineering and Technology                                                                  |               arxiv                |                    [Social Bias in Large Language Models For Bangla: An Empirical Study on Gender and Religious Bias](https://arxiv.org/abs/2407.03536)                     |                      **Social Bias**&**Gender Bias**&**Religious Bias**                       |
| 24.07 |                                                                                University of Calabria                                                                                |               arxiv                |         [Are Large Language Models Really Bias-Free? Jailbreak Prompts for Assessing Adversarial Robustness to Bias Elicitation](https://arxiv.org/abs/2407.08441)          |                       **Bias**&**Jailbreak**&**Adversarial Robustness**                       |
| 24.07 |                                                                                 University of Oxford                                                                                 |               arxiv                |                          [What an Elegant Bridge: Multilingual LLMs are Biased Similarly in Different Languages](https://arxiv.org/abs/2407.09704)                          |                     **Multilingual LLMs**&**Bias**&**Grammatical Gender**                     |
| 24.07 |                                                                                 Zhejiang University                                                                                  |               arxiv                |                                    [BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs](https://arxiv.org/abs/2407.10241)                                    |                              **Bias Detection**&**Social Bias**                               |
| 24.07 |                                                                                      CVS Health                                                                                      |               arxiv                |                        [An Actionable Framework for Assessing Bias and Fairness in Large Language Model Use Cases](https://arxiv.org/abs/2407.10853)                        |                               **Bias Assessment**&**Fairness**                                |
| 24.07 |                                                                               University of Amsterdam                                                                                |               arxiv                |                             [How Are LLMs Mitigating Stereotyping Harms? Learning from Search Engine Studies](https://arxiv.org/abs/2407.11733)                             |                             **Stereotyping**&**Safety Training**&                             |
| 24.07 |                                                                             American University in Cairo                                                                             |               arxiv                |                           [BiasDPO: Mitigating Bias in Language Models through Direct Preference Optimization](https://arxiv.org/abs/2407.13928)                            |                    **Bias Mitigation**&**Direct Preference Optimization**                     |
| 24.07 |                                                                                       SoftlyAI                                                                                       |               arxiv                |                                                    [Harmful Suicide Content Detection](https://arxiv.org/abs/2407.13942)                                                    |                       **Suicide Content Detection**&**Harmful Content**                       |
| 24.07 |                                                                               University of Cambridge                                                                                |               arxiv                |                   [LLMs left, right, and center: Assessing GPT‚Äôs capabilities to label political bias from web domains](https://arxiv.org/abs/2407.14344)                   |                        **GPT-4**&**Political Bias**&**Data Labeling**                         |
| 24.07 |                                                                                Georgetown University                                                                                 |               arxiv                |                                     [US-China perspectives on extreme AI risks and global governance](https://arxiv.org/abs/2407.16903)                                     |           **Extreme AI Risks**&**Global Governance**&**International Cooperation**            |
| 24.07 |                                                                              Carnegie Mellon University                                                                              |               arxiv                |                 [Examining the Influence of Political Bias on Large Language Model Performance in Stance Classification](https://arxiv.org/abs/2407.17688)                  |                         **Political Bias**&**Stance Classification**                          |
| 24.07 |                                                                                  Xidian University                                                                                   |               arxiv                |                            [The Dark Side of Function Calling: Pathways to Jailbreaking Large Language Models](https://arxiv.org/abs/2407.17915)                            |                             **Function Calling**&**Jailbreaking**                             |
| 24.07 |                                                                                 Universit√§t Hamburg                                                                                  |             AIES 2024              |                  [Decoding Multilingual Moral Preferences: Unveiling LLM‚Äôs Biases Through the Moral Machine Experiment](https://arxiv.org/abs/2407.15184)                   |                   **Moral Preferences**&**Multilingual Analysis**&**Bias**                    |
| 24.07 |                                                                           Florida International University                                                                           |               arxiv                |                                            [Fairness Definitions in Language Models Explained](https://arxiv.org/abs/2407.18454)                                            |                               **Fairness**&**Bias Mitigation**                                |
| 24.07 |                                                                           Barcelona Supercomputing Center                                                                            |               arxiv                |                               [The Power of Prompts: Evaluating and Mitigating Gender Bias in MT with LLMs](https://arxiv.org/abs/2407.18786)                               |                            **Gender Bias**&**Prompt Engineering**                             |
| 24.07 |                                                                               University of Washington                                                                               |             AIES 2024              |                         [Gender, Race, and Intersectional Bias in Resume Screening via Language Model Retrieval](https://arxiv.org/abs/2407.20371)                          |                                 **Gender bias**&**Race bias**                                 |
| 24.08 |                                                                              Seoul National University                                                                               |               arxiv                |                      [Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment](https://arxiv.org/abs/2408.00137)                       |                     **Negative Bias**&**Attention Score**&**Fine-tuning**                     |
| 24.08 |                                                                                  Rutgers University                                                                                  |               arxiv                |                                        [A Taxonomy of Stereotype Content in Large Language Models](https://arxiv.org/abs/2408.00162)                                        |                        **Bias**&**Stereotypes**&**Social Psychology**                         |
| 24.08 |                                                                           Florida International University                                                                           |              CIKM '24              |                                            [Fairness in Large Language Models in Three Hours](https://arxiv.org/abs/2408.00992)                                             |                               **Fairness**&**Bias Mitigation**                                |
| 24.08 |                                                                                      Intel Labs                                                                                      |               arxiv                |                     [Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models](https://arxiv.org/abs/2408.03907)                      |             **Gender Bias**&**Adversarial Prompt Generation**&**LLM Evaluation**              |
| 24.08 |                                                                                 Shandong University                                                                                  |               arxiv                |                                               [Social Debiasing for Fair Multi-modal LLMs](https://arxiv.org/abs/2408.06569)                                                |            **Social Debiasing**&**Multi-modal LLMs**&**Anti-Stereotype Debiasing**            |
| 24.08 |                                                                                 King Saud University                                                                                 |               arxiv                |                      [Covert Bias: The Severity of Social Views' Unalignment Towards Implicit and Explicit Opinion](https://arxiv.org/abs/2408.08212)                       |                             **Covert Bias**&**Implicit Opinion**                              |
| 24.08 |                                                                      Shanghai University of Engineering Science                                                                      |               arxiv                |           [Promoting Equality in Large Language Models: Identifying and Mitigating the Implicit Bias based on Bayesian Theory](https://arxiv.org/abs/2408.10608)            |                    **Implicit Bias**&**Bayesian Theory**&**Bias Removal**                     |
| 24.08 |                                                                                 Zhejiang University                                                                                  |               arxiv                |                                   [Editable Fairness: Fine-Grained Bias Mitigation in Language Models](https://arxiv.org/abs/2408.11843)                                    |                          **Bias Mitigation**&**Knowledge Retention**                          |
| 24.08 |                                                                    University of Science and Technology of China                                                                     |               arxiv                |                  [GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender Bias in Large Language Models](https://arxiv.org/abs/2408.12494)                  |              **Gender Bias**&**Large Language Models**&**Algorithmic Fairness**               |
| 24.08 |                                                                                 Stanford University                                                                                  |               arxiv                |                                         [Uncovering Biases with Reflective Large Language Models](https://arxiv.org/abs/2408.13464)                                         |                             **Bias Detection**&**Reflective AI**                              |
| 24.08 |                                                                            University of Western Ontario                                                                             |               arxiv                |                   [Bias in LLMs as Annotators: The Effect of Party Cues on Labelling Decision by Large Language Models](https://arxiv.org/abs/2408.15895)                   |                          **Bias**&**Political Cues**&**Annotation**                           |
| 24.09 |                                                                         The Chinese University of Hong Kong                                                                          |               arxiv                |                        [Testing and Evaluation of Large Language Models: Correctness, Non-Toxicity, and Fairness](https://arxiv.org/abs/2409.00551)                         |                         **Correctness**&**Non-Toxicity**&**Fairness**                         |
| 24.09 |                                                                                 University of Trento                                                                                 |               arxiv                |                                          [More is More: Addition Bias in Large Language Models](https://arxiv.org/abs/2409.02569)                                           |                             &**Addition Bias**&**Cognitive Bias**                             |
| 24.09 |                                                                       Nanjing University, Southeast University                                                                       |               arxiv                |                                       [AGR: Age Group Fairness Reward for Bias Mitigation in LLMs](https://arxiv.org/abs/2409.04340)                                        |     **Age Bias**&**LLM Alignment**&**Reinforcement Learning with Human Feedback (RLHF)**      |
| 24.09 |                                                               MIT Center for Constructive Communication, MIT Media Lab                                                               |               arxiv                |                                 [On the Relationship between Truth and Political Bias in Language Models](https://arxiv.org/abs/2409.05283)                                 |                              **Truthfulness**&**Political Bias**                              |
| 24.09 |                                                                      AppCubic, Georgia Institute of Technology                                                                       |               arxiv                |                           [Securing Large Language Models: Addressing Bias, Misinformation, and Prompt Attacks](https://arxiv.org/abs/2409.08087)                           |                 **Misinformation**&**Jailbreak Attacks**&**Prompt Injection**                 |
| 24.09 |                                                                            Technical University of Munich                                                                            |               arxiv                |                                      [Understanding Knowledge Drift in LLMs through Misinformation](https://arxiv.org/abs/2409.07085)                                       |                    **Knowledge Drift**&**Misinformation**&**Uncertainty**                     |
| 24.09 |                                                                                University of Calabria                                                                                |               arxiv                |                      [Safeguarding Decentralized Social Media: LLM Agents for Automating Community Rule Compliance](https://arxiv.org/abs/2409.08963)                       |                **Community Rule Compliance**&**Decentralized Social Networks**                |
| 24.09 |                                                                               University of California                                                                               |               arxiv                |              [Unveiling Gender Bias in Large Language Models: Using Teacher's Evaluation in Higher Education As an Example](https://arxiv.org/abs/2409.09652)               |                            **Gender Bias**&**Teacher Evaluations**                            |
| 24.09 |                                                                              University College London                                                                               |    NeurIPS 2024 SoLaR Workshop     |                     [HEARTS: A Holistic Framework for Explainable, Sustainable and Robust Text Stereotype Detection](https://arxiv.org/abs/2409.11579)                      |              **Explainable AI**&**Text Stereotype Detection**&**Sustainability**              |
| 24.09 |                                                                                        Bosch                                                                                         |               arxiv                |                               [Do the Right Thing, Just Debias! Multi-Category Bias Mitigation Using LLMs](https://arxiv.org/abs/2409.16371)                                |             **Bias Mitigation**&**Reinforcement Learning**&**Debiasing Dataset**              |
| 24.10 |                                                                                University of Michigan                                                                                |        EMNLP 2024 Findings         |                             [Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions](https://arxiv.org/abs/2410.02584)                              |                   **Implicit Bias**&**Multi-Agent LLM**&**Bias Mitigation**                   |
| 24.10 |                                                              Diwan of Royal Court, Royal Holloway, University of London                                                              |               arxiv                |                                         [AI-Enhanced Ethical Hacking: A Linux-Focused Experiment](https://arxiv.org/abs/2410.05105)                                         |                             **Ethical Hacking**&**Cybersecurity**                             |
| 24.10 |                                                                        University of Geneva, Google DeepMind                                                                         |               arxiv                |                             [The Moral Turing Test: Evaluating Human-LLM Alignment in Moral Decision-Making](https://arxiv.org/abs/2410.07304)                              |                       **Moral Decision-Making**&**Human-AI Alignment**                        |
| 24.10 |                                                                   Harbin Institute of Technology, Anhui University                                                                   |               arxiv                |                                 [Mitigating Gender Bias in Code Large Language Models via Model Editing](https://arxiv.org/abs/2410.07820)                                  |                     **Gender Bias**&**Code Generation**&**Model Editing**                     |
| 24.10 |                                                                             IIT-CNR, University of Pisa                                                                              |               arxiv                |                 [Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic Analysis of Annotators and Targets](https://arxiv.org/abs/2410.07991)                 |                                   **Hate Speech**&**Bias**                                    |
| 24.10 |                                                                    East China Normal University, Fudan University                                                                    |             ECAI 2024              |                       [MindScope: Exploring Cognitive Biases in Large Language Models through Multi-Agent Systems](https://arxiv.org/abs/2410.04452)                        |                         **Cognitive Biases**&**Multi-Agent Systems**                          |
| 24.10 |                                                                                Algoverse AI Research                                                                                 |    NeurIPS 2024, SoLaR workshop    |                [Fine-Tuning Language Models for Ethical Ambiguity: A Comparative Study of Alignment with Human Responses](https://arxiv.org/abs/2410.07826)                 |                             **Ethical Ambiguity**&**Fine-Tuning**                             |
| 24.10 |                                                                                    UMass Amherst                                                                                     |               arxiv                |                                              [Bias Similarity Across Large Language Models](https://arxiv.org/abs/2410.12010)                                               |                    **Bias**&**LLMs similarity**&**Training data leakage**                     |
| 24.10 |                                                                                      Enkrypt AI                                                                                      |               arxiv                |                        [Investigating Implicit Bias in Large Language Models: A Large-Scale Study of Over 50 LLMs](https://arxiv.org/abs/2410.12864)                        |                      **Implicit bias**&**LLMs bias**&**Bias mitigation**                      |
| 24.10 |                                                                     √âcole Polytechnique, LINAGORA, NTUA, MBZUAI                                                                      |               arxiv                |                             [Bias in the Mirror: Are LLMs Opinions Robust to Their Own Adversarial Attacks?](https://arxiv.org/abs/2410.13517)                              |                 **Bias resilience**&**LLMs robustness**&**Multilingual bias**                 |
| 24.10 |                                                                                 Texas A&M University                                                                                 |        EMNLP Findings 2024         |                                      [Evaluating Gender Bias of LLMs in Making Morality Judgements](https://arxiv.org/abs/2410.09992)                                       |                  **Gender bias**&**Morality judgements**&**Bias detection**                   |
| 24.10 |                                                                                Dublin City University                                                                                |             FLLM 2024              |                                                    [Capturing Bias Diversity in LLMs](https://arxiv.org/abs/2410.12839)                                                     |                  **Bias diversity**&**Customised LLMs**&**Demographic bias**                  |
| 24.10 |                                                                               Northeastern University                                                                                |               arxiv                |                  [Eliciting Uncertainty in Chain-of-Thought to Mitigate Bias against Forecasting Harmful User Behaviors](https://arxiv.org/abs/2410.14744)                  |          **Uncertainty Estimation**&**Conversation Forecasting**&**Bias Mitigation**          |
| 24.10 |                                                                                Penn State University                                                                                 |               arxiv                |            [Hey GPT, Can You be More Racist? Analysis from Crowdsourced Attempts to Elicit Biased Content from Generative AI](https://arxiv.org/abs/2410.15467)             |                    **Bias Elicitation**&**LLM Bias**&**User Interaction**                     |
| 24.10 |                                                                                University of Virginia                                                                                |               arxiv                |                                     [Does Differential Privacy Impact Bias in Pretrained NLP Models?](https://arxiv.org/abs/2410.18749)                                     |                        **Differential Privacy**&**Model Bias**&**NLP**                        |
| 24.10 |                                                                              University of Strathclyde                                                                               |               arxiv                |                                    [PRISM: A Methodology for Auditing Biases in Large Language Models](https://arxiv.org/abs/2410.18906)                                    |                            **Bias Auditing**&**Political Compass**                            |
| 24.10 |                                                                                TU Dortmund University                                                                                |               arxiv                |                  [Is GPT-4 Less Politically Biased than GPT-3.5? A Renewed Investigation of ChatGPT‚Äôs Political Biases](https://arxiv.org/abs/2410.21008)                   |                 **Political Bias**&**Personality Traits**&**LLM Evaluation**                  |
| 24.10 |                                                                              Michigan State University                                                                               |               arxiv                |                                       [Smaller Large Language Models Can Do Moral Self-Correction](https://arxiv.org/abs/2410.23496)                                        |                **Moral Self-Correction**&**Safety Alignment**&**Social Norms**                |
| 24.10 |                                                                                        MBZUAI                                                                                        |               arxiv                |                                [Desert Camels and Oil Sheikhs: Arab-Centric Red Teaming of Frontier LLMs](https://arxiv.org/abs/2410.24049)                                 |                    **LLM Bias**&**Arab Stereotypes**&**Jailbreak Attacks**                    |
| 24.10 |                                                                                  Rutgers University                                                                                  |    NeurIPS 2024 SoLaR Workshop     |                                           [Gender Bias in LLM-generated Interview Responses](https://arxiv.org/abs/2410.20739v2)                                            |                  **Gender Bias**&**LLM Interview Responses**&**Stereotypes**                  |
| 24.11 |                                                                            Kyushu Institute of Technology                                                                            |               arXiv                |                                      [Large-scale Moral Machine Experiment on Large Language Models](https://arxiv.org/abs/2411.06790)                                      |                          **Moral Judgments**&**Autonomous Driving**                           |
| 24.11 |                                                                                Sun Yat-sen University                                                                                |               arXiv                |               [Quantifying Risk Propensities of Large Language Models: Ethical Focus and Bias Detection through Role-Play](https://arxiv.org/abs/2411.08884)                |                    **Ethical Risk**&**Bias Detection**&**LLM Evaluation**                     |
| 24.11 |                                                                            South China Normal University                                                                             |             IJCAI 2024             |                                         [Prompt-enhanced Network for Hateful Meme Classification](https://arxiv.org/abs/2411.07527)                                         |         **Hateful Meme Classification**&**Prompt Learning**&**Contrastive Learning**          |
| 24.11 |                                                                       Darmstadt University of Applied Sciences                                                                       |               arxiv                |                                             [EVALUATING GENDER BIAS IN LARGE LANGUAGE MODELS](https://arxiv.org/abs/2411.09826)                                             |                            **Gender Bias**&**Gender Distribution**                            |
| 24.11 |                                                                                  Minerva University                                                                                  |               arxiv                |                   [Assessing Gender Bias in LLMs: Comparing LLM Outputs with Human Perceptions and Official Statistics](https://arxiv.org/abs/2411.13738)                   |                        **Gender Bias**&**Human Perception Comparison**                        |
| 24.11 |                                                                                  Peking University                                                                                   |               arxiv                |    [Looking Beyond Text: Reducing Language Bias in Large Vision-Language Models via Multimodal Dual-Attention and Soft-Image Guidance](https://arxiv.org/abs/2411.14279)    |          **Large Vision-Language Models**&**Language Bias**&**Multimodal Attention**          |
| 24.11 |                                                                              School of Computing, KAIST                                                                              |    NeurIPS 2024 SoLaR Workshop     |                       [Different Bias Under Different Criteria: Assessing Bias in LLMs with a Fact-Based Approach](https://arxiv.org/abs/2411.17338)                        |           **Bias Evaluation**&**Fact-Based Criteria**&**Demographic Distribution**            |
| 24.11 |                                                                     University of Delaware&University of Bristol                                                                     | NeurIPS 2024 Neural Model Workshop |                                      [Inducing Human-like Biases in Moral Reasoning Language Models](https://arxiv.org/abs/2411.15386)                                      |                          **Moral Reasoning**&**BrainScore**&**fMRI**                          |
| 24.11 |                                                                            Technical University of Munich                                                                            |               arxiv                |                               [Profiling Bias in LLMs: Stereotype Dimensions in Contextual Word Embeddings](https://arxiv.org/abs/2411.16527)                               |                 **Bias Profiling**&**Stereotype Dimensions**&**Gender Bias**                  |
| 24.11 |                                                                     AiM Future&Maum AI, Sungkyunkwan University                                                                      |               arxiv                |                        [Debiasing Classifiers by Amplifying Bias with Latent Diffusion and Large Language Models](https://arxiv.org/abs/2411.16079)                         |              **Debiasing**&**Latent Diffusion Models**&**Classifier Robustness**              |
| 24.11 |                                                                            University of Zagreb, Preamble                                                                            |               arxiv                |                        [AI Ethics by Design: Implementing Customizable Guardrails for Responsible AI Development](https://arxiv.org/abs/2411.14442)                         |                   **AI Ethics**&**Guardrails**&**Customizable Frameworks**                    |
| 24.12 |                                                                               Kyoto University, Japan                                                                                |               arxiv                |                             [Cognitive Biases in Large Language Models: A Survey and Mitigation Experiments](https://arxiv.org/abs/2412.00323)                              |                        **Cognitive Biases**&**Mitigation Techniques**                         |
| 24.12 |                                                                      Department of Arts, University of Bologna                                                                       |             AIAA 2024              |                                       [Examining Multimodal Gender and Content Bias in ChatGPT-4O](https://arxiv.org/abs/2411.19140)                                        |                  **Gender Bias**&**Content Bias**&**Multimodal Generation**                   |
| 24.12 |                                                                                        Apple                                                                                         |               arxiv                |                         [Evaluating Gender Bias Transfer between Pre-trained and Prompt-Adapted Language Models](https://arxiv.org/abs/2412.03537)                          |                      **Gender Bias**&**Prompt Adaptation**&**Fairness**                       |
| 24.12 |                                                                           Universit√† degli Studi di Genova                                                                           |               arxiv                |                                        [Do Large Language Models Show Biases in Causal Learning?](https://arxiv.org/abs/2412.10509)                                         |                **Causal Learning**&**Illusion of Causality**&**Bias in LLMs**                 |
| 24.12 |                                                                                    Virginia Tech                                                                                     |               arxiv                |                [Biased or Flawed? Mitigating Stereotypes in Generative Language Models by Addressing Task-Specific Flaws](https://arxiv.org/abs/2412.11414)                 |          **Stereotype Mitigation**&**Instruction Tuning**&**Reading Comprehension**           |
| 24.12 |                                                                        Coburg University of Applied Sciences                                                                         |               arxiv                |                                     [Improved Models for Media Bias Detection and Subcategorization](https://arxiv.org/abs/2412.11835)                                      |        **Media Bias Detection**&**Fine-Tuned Models**&**Synthetic Data Augmentation**         |
| 24.12 |                                                                   University of Edinburgh, Heriot-Watt University                                                                    |            COLING 2025             |                             [The Only Way is Ethics: A Guide to Ethical Research with Large Language Models](https://arxiv.org/abs/2412.16022)                              |                 **Ethical Research**&**LLM Guidelines**&**Project Lifecycle**                 |
| 24.12 |                                                      Sun Yat-sen University, Zhejiang University, City University of Hong Kong                                                       |             AAAI 2025              |               [Mitigating Social Bias in Large Language Models: A Multi-Objective Approach Within a Multi-Agent Framework](https://arxiv.org/abs/2412.15504)                |     **Social Bias Mitigation**&**Multi-Agent Framework**&**Multi-Objective Optimization**     |
| 24.12 |                                                                 Loyola Chicago University, Arizona State University                                                                  |               arxiv                |                                             [Identifying Cyberbullying Roles in Social Media](https://arxiv.org/abs/2412.16417)                                             |                     **Cyberbullying**&**Role Detection**&**Social Media**                     |
| 24.12 |                                                                                University of Florida                                                                                 |               arxiv                |                [Nationality, Race, and Ethnicity Biases in and Consequences of Detecting AI-Generated Self-Presentations](https://arxiv.org/abs/2412.18647)                 |                   **Human-AI Interaction**&**AI-Detection**&**Stereotypes**                   |
| 24.12 |                                                                                Heriot-Watt University                                                                                |               arxiv                |                                           [GFG - Gender-Fair Generation: A CALAMITA Challenge](https://arxiv.org/abs/2412.19168)                                            |            **Gender-Fair Language**&**Machine Translation**&**Inclusive Language**            |
| 25.01 |                                                                                 New York University                                                                                  |               arxiv                |                                        [Analyzing the Ethical Logic of Six Large Language Models](https://arxiv.org/abs/2501.08951)                                         |                           **Ethical Reasoning**&**Moral Dilemmas**                            |
| 25.01 |                                                                              Xi‚Äôan Jiaotong University                                                                               |               arxiv                |                                  [Unveiling Provider Bias in Large Language Models for Code Generation](https://arxiv.org/abs/2501.07849)                                   |             **LLM System Security**&**Bias and Fairness**&**Digital Monopolies**              |
| 25.01 |                                                                               University of Edinburgh                                                                                |               arxiv                |                                       [LLMs Reproduce Stereotypes of Sexual and Gender Minorities](https://arxiv.org/abs/2501.05926)                                        |            **Stereotype Content Model**&**Bias in LLMs**&**Social Representation**            |
| 25.01 |                                                                            Pennsylvania State University                                                                             |               arxiv                |                                       [FairCode: Evaluating Social Bias of LLMs in Code Generation](https://arxiv.org/abs/2501.05396)                                       |                  **Bias Evaluation**&**Code Generation**&**Social Fairness**                  |
| 25.01 |                                                                           Georgia Institute of Technology                                                                            |               arxiv                |                   [On The Origin of Cultural Biases in Language Models: From Pre-training Data to Linguistic Phenomena](https://arxiv.org/abs/2501.04662)                   |     **Cultural Bias in LLMs**&**Cross-Linguistic Analysis**&**Arabic-English Benchmarks**     |
| 25.01 |                                                                                CVS Health Corporation                                                                                |               arxiv                |                      [LangFair: A Python Package for Assessing Bias and Fairness in Large Language Model Use Cases](https://arxiv.org/abs/2501.03112)                       |                **Bias Detection**&**Algorithmic Fairness**&**LLM Governance**                 |
| 25.01 |                                                                                King‚Äôs College London                                                                                 |             NAACL 2025             |                                      Watching the AI Watchdogs: A Fairness and Robustness Analysis of AI Safety Moderation Classifiers                                      |                 **AI Safety Moderation**&**Fairness Analysis**&**Robustness**                 |
| 25.01 |                                                                                     IBM Research                                                                                     |      AAAI 2025 PPAI Workshop       |                                                         Adaptive PII Mitigation Framework for Large Language Models                                                         |               **PII Mitigation**&**Adaptive Framework**&**Privacy Compliance**                |
| 25.01 |                                                                             The University of Manchester                                                                             |               arxiv                |                            [Understanding and Mitigating Gender Bias in LLMs via Interpretable Neuron Editing](https://arxiv.org/abs/2501.14457)                            |                    **Gender Bias**&**Neuron Editing**&**Interpretable AI**                    |
| 25.01 |                                                University of Illinois at Urbana-Champaign, Amazon AWS, Technical University of Munich                                                |             ICLR 2025              |            [Examining Alignment of Large Language Models Through Representative Heuristics: The Case of Political Stereotypes](https://arxiv.org/abs/2501.14294)            |              **LLM Alignment**&**Political Bias**&**Representative Heuristics**               |
| 25.02 |                                                                       National Technical University of Athens                                                                        |               arxiv                |                            [Bias Beware: The Impact of Cognitive Biases on LLM-Driven Product Recommendations](https://arxiv.org/abs/2502.01349)                            |           **LLM Recommendation**&**Cognitive Biases**&**Adversarial Manipulation**            |
| 25.02 |                                                                          Universidad Polit√©cnica de Madrid                                                                           |               arxiv                |                           [Towards Safer Chatbots: A Framework for Policy Compliance Evaluation of Custom GPTs](https://arxiv.org/abs/2502.01436)                           |                 **LLM Policy Compliance**&**Custom GPTs**&**Safety Auditing**                 |
| 25.02 |                                                                                    LY Corporation                                                                                    |               arxiv                |                                [Vulnerability Mitigation for Safety-Aligned Language Models via Debiasing](https://arxiv.org/abs/2502.02153)                                |               **LLM Safety Alignment**&**Debiasing**&**Token-Level Correction**               |
| 25.02 |                                                                                Iowa State University                                                                                 |               arxiv                |                               [On Fairness of Unified Multimodal Large Language Model for Image Generation](https://arxiv.org/abs/2502.03429)                               |                 **Multimodal LLMs**&**Bias in Image Generation**&**Fairness**                 |
| 25.02 |                                                                               Chulalongkorn University                                                                               |   CompJobs Workshop @ AAAI 2025    |                       [Mitigating Language Bias in Cross-Lingual Job Retrieval: A Recruitment Platform Perspective](https://arxiv.org/abs/2502.03220)                       |           **Cross-Lingual Job Retrieval**&**Language Bias**&**Multitask Learning**            |
| 25.02 |                                                                             Sapienza University of Rome                                                                              |               arxiv                |                                     [Decoding AI Judgment: How LLMs Assess News Credibility and Bias](https://arxiv.org/abs/2502.04426)                                     |                            **News Credibility**&**Political Bias**                            |
| 25.02 |                                                                           Dalian University of Technology                                                                            |               arxiv                |            [Unveiling the Capabilities of Large Language Models in Detecting Offensive Language with Annotation Disagreement](https://arxiv.org/abs/2502.06207)             |       **Offensive Language Detection**&**Annotation Disagreement**&**LLMs Evaluation**        |
| 25.02 |                                                                              Carnegie Mellon University                                                                              |               arxiv                |                      [Fairness in Multi-Agent AI: A Unified Framework for Ethical and Equitable Autonomous Systems](https://arxiv.org/abs/2502.07254)                       |           **Fairness in Multi-Agent Systems**&**AI Ethics**&**Autonomous Systems**            |
| 25.02 |                                                                          Rochester Institute of Technology                                                                           |               arxiv                |   [HOPE VS. HATE: UNDERSTANDING USER INTERACTIONS WITH LGBTQ+ NEWS CONTENT IN MAINSTREAM US NEWS MEDIA THROUGH THE LENS OF HOPE SPEECH](https://arxiv.org/abs/2502.09004)   |                         **Hope Speech**&**LGBTQ+**&**Political Bias**                         |
| 25.02 |                                                                               University of Strasbourg                                                                               |               arxiv                |                           [Man Made Language Models? Evaluating LLMs‚Äô Perpetuation of Masculine Generics Bias](https://arxiv.org/abs/2502.10577)                            |                    **Gender Bias**&**Masculine Generics**&**LLM Fairness**                    |
| 25.02 |                                                                               ShanghaiTech University                                                                                |               arxiv                |                 [Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models](https://arxiv.org/abs/2502.11559)                  |                            **Gender Bias Mitigation**&**Fairness**                            |
| 25.02 |                                                                               ShanghaiTech University                                                                                |               arxiv                |             [DR.GAP: Mitigating Bias in Large Language Models using Gender-Aware Prompting with Demonstration and Reasoning](https://arxiv.org/abs/2502.11603)              |                            **Gender Bias Mitigation**&**Fairness**                            |
| 25.02 |                                                                                      Microsoft                                                                                       |               arxiv                |                                                         [LLM Safety for Children](https://arxiv.org/abs/2502.12552)                                                         |                 **LLM Safety**&**Child Protection**&**Content Harm Taxonomy**                 |
| 25.02 |                                                                       Horace Mann School, Columbia University                                                                        |               arXiv                |                            [Towards Equitable AI: Detecting Bias in Using Large Language Models for Marketing](https://arxiv.org/abs/2502.12838)                            |                              **Bias Detection**&**Marketing AI**                              |
| 25.02 |                                                     Carnegie Mellon University, Columbia University, Instituto Superior T√©cnico                                                      |        NAACL Findings 2025         |                              [Rejected Dialects: Biases Against African American Language in Reward Models](https://arxiv.org/abs/2502.12858)                               |                   **Bias**&**Reward Models**&**African American Language**                    |
| 25.02 |                                                                             Vrije Universiteit Amsterdam                                                                             |              WWW 2025              |                          [Detecting Linguistic Bias in Government Documents Using Large Language Models](https://doi.org/10.1145/3696410.3714526)                           |                          **Bias Detection**&**Government Documents**                          |
| 25.02 |                                                                              Carnegie Mellon University                                                                              |               arxiv                |                                            [Mitigating Bias in RAG: Controlling the Embedder](https://arxiv.org/abs/2502.17390)                                             |          **Bias Mitigation**&**Retrieval-Augmented Generation**&**Embedding Models**          |
| 25.02 |                                                                                Santa Clara University                                                                                |               arxiv                |                                                [Evaluating Social Biases in LLM Reasoning](https://arxiv.org/abs/2502.15361)                                                |                    **Social Biases**&**LLM Reasoning**&**Bias Evaluation**                    |
| 25.02 |                                                                          National Institute of Informatics                                                                           |               arxiv                |             [7 Points to Tsinghua but 10 Points to Ê∏ÖÂçé? Assessing Large Language Models in Agentic Multilingual National Bias](https://arxiv.org/abs/2502.17945)             |                 **Multilingual Bias**&**LLM Fairness**&**Decision-Making AI**                 |
| 25.02 |                                                                               George Mason University                                                                                |               arxiv                |                                   [Beneath the Surface: How Large Language Models Reflect Hidden Bias](https://arxiv.org/abs/2502.19749)                                    |                    **Hidden Bias**&**LLM Fairness**&**Bias Benchmarking**                     |
| 25.02 |                                                                                University of Virginia                                                                                |               arxiv                |                     [Sensing and Steering Stereotypes: Extracting and Applying Gender Representation Vectors in LLMs](https://arxiv.org/abs/2502.19721)                     |            **Gender Bias**&**Representation Engineering**&**Activation Steering**             |
| 25.02 |                                                                               Fraunhofer IAIS & Lamarr                                                                               |               arxiv                |                          [Detecting Linguistic Indicators for Stereotype Assessment with Large Language Models](https://arxiv.org/abs/2502.19160)                           |            **Stereotype Detection**&**Linguistic Indicators**&**Fairness in LLMs**            |
| 25.03 |                                                                                      IIT Jammu                                                                                       |               arXiv                |                               [Jailbreaking Generative AI: Empowering Novices to Conduct Phishing Attacks](https://arxiv.org/abs/2503.01395)                                |                    **Jailbreaking**&**Generative AI**&**Phishing Attacks**                    |
| 25.03 |                                                                     University of Tsukuba, Politecnico di Milano                                                                     |               arXiv                |                         [Analyzing the Safety of Japanese Large Language Models in Stereotype-Triggering Prompts](https://arxiv.org/abs/2503.01947)                         |             **Japanese Large Language Models**&**Stereotype Bias**&**AI Safety**              |
| 25.03 |                                                                          Beijing Foreign Studies University                                                                          |               arXiv                |                                                     [Implicit Bias in LLMs: A Survey](https://arxiv.org/abs/2503.02776)                                                     |                        **Implicit Bias**&**Implicit Association Test**                        |
| 25.03 |                                                                        App Inventor Foundation & App-In Club                                                                         |  Social Impact of AI @ AAAI 2025   |                       [Structured Reasoning for Fairness: A Multi-Agent Approach to Bias Detection in Textual Data](https://arxiv.org/abs/2503.00355)                       |                 **Bias Detection**&**Multi-Agent Systems**&**Fairness in AI**                 |
| 25.03 |                                                                              Institute of Science Tokyo                                                                              |               arxiv                |                           [Intent-Aware Self-Correction for Mitigating Social Biases in Large Language Models](https://arxiv.org/abs/2503.06011)                            |         **Self-Correction**&**Social Bias Mitigation**&**Chain-of-Thought Reasoning**         |
| 25.03 |                                                                                        Amazon                                                                                        |               arxiv                |                          [Fine-Grained Bias Detection in LLM: Enhancing Detection Mechanisms for Nuanced Biases](https://arxiv.org/abs/2503.06054)                          |                     **Bias Detection**&**Nuanced Bias**&**LLM Fairness**                      |
| 25.03 |                                                                                    IIIT Hyderabad                                                                                    |               arxiv                |         [Sometimes the Model doth preach: Quantifying Religious Bias in Open LLMs through Demographic Analysis in Asian Nations](https://arxiv.org/abs/2503.07510)          |             **Religious Bias**&**Demographic Profiling**&**Open LLM Evaluation**              |
| 25.03 |                                                                          University of Southern California                                                                           |               arxiv                |                            [VISBIAS: Measuring Explicit and Implicit Social Biases in Vision-Language Models](https://arxiv.org/abs/2503.07575)                             |               **Vision-Language Models**&**Implicit Bias**&**Social Fairness**                |
| 25.03 |                                                                                University of Virginia                                                                                |               arxiv                |                                              [Benchmarking Group Fairness in Reward Models](https://arxiv.org/abs/2503.07806)                                               |                    **Group Fairness**&**Reward Models**&**LLM Alignment**                     |
| 25.03 |                                                                                Clarksburg High School                                                                                |               arxiv                |                                       [An Evaluation of LLMs for Detecting Harmful Computing Terms](https://arxiv.org/abs/2503.09341)                                       |           **Harmful Terms**&**Model Architecture**&**Inclusive Language Detection**           |
| 25.03 |                                                                                Universidad de Sevilla                                                                                |               arxiv                |                            [Red Teaming Contemporary AI Models: Insights from Spanish and Basque Perspectives](https://arxiv.org/abs/2503.10192)                            |                           **Red Teaming**&**Spanish/Basque Safety**                           |
| 25.03 |                                                                         University of California, San Diego                                                                          |       TrustNLP @ NAACL 2025        |                                    [BIASEDIT: Debiasing Stereotyped Language Models via Model Editing](https://arxiv.org/abs/2503.08588)                                    |             **Model Editing**&**Bias Mitigation**&**Stereotyped Language Models**             |
| 25.03 |                                                                                    IIIT Hyderabad                                                                                    |               arxiv                |                       [No LLM is Free From Bias: A Comprehensive Study of Bias Evaluation in Large Language Models](https://arxiv.org/abs/2503.11985)                       |                           **Bias Evaluation**&**Prompting Methods**                           |
| 25.03 |                                                                                 University of Chile                                                                                  |               arxiv                |     [Do Chains-of-Thoughts of Large Language Models Suffer from Hallucinations, Cognitive Biases, or Phobias in Bayesian Reasoning?](https://arxiv.org/abs/2503.15268)      |                **Bayesian Reasoning**&**Cognitive Bias**&**Chain-of-Thought**                 |
| 25.03 |                                                                                        Emotia                                                                                        |             ICMLC 2025             |                            [LLMs, Virtual Users, and Bias: Predicting Any Survey Question Without Human Data](https://arxiv.org/abs/2503.16498)                             |                   **Survey Prediction**&**LLM Bias**&**Censorship Impact**                    |
| 25.03 |                                                                                Stony Brook University                                                                                |               arxiv                |                         [The Risks of Using Large Language Models for Text Annotation in Social Science Research](https://arxiv.org/abs/2503.22040)                         |                  **Text Annotation**&**Social Science**&**Epistemic Risks**                   |
| 25.03 |                                                                                  Xiamen University                                                                                   |               arxiv                |               [A Multi-Agent Framework with Automated Decision Rule Optimization for Cross-Domain Misinformation Detection](https://arxiv.org/abs/2503.23329)               |       **Misinformation Detection**&**Cross-Domain Transfer**&**Multi-Agent Framework**        |
| 25.03 |                                                                                 Tsinghua University                                                                                  |               arxiv                |            [Mapping Geopolitical Bias in 11 Large Language Models: A Bilingual, Dual-Framing Analysis of U.S.‚ÄìChina Tensions](https://arxiv.org/abs/2503.23688)             |         **Geopolitical Bias**&**Language Model Evaluation**&**U.S.‚ÄìChina Relations**          |
| 25.03 |                                                                               Independent Researchers                                                                                |               arxiv                |                               [BEATS: Bias Evaluation and Assessment Test Suite for Large Language Models](https://arxiv.org/abs/2503.24310)                                |                 **Bias Evaluation**&**Ethical AI**&**Factuality Assessment**                  |
| 25.04 |                                                                                      Althire AI                                                                                      |               arxiv                |                            [Detecting and Mitigating Bias in LLMs through Knowledge Graph-Augmented Training](https://arxiv.org/abs/2504.00310)                             |                  **Bias Mitigation**&**Knowledge Graphs**&**Fairness in AI**                  |
| 25.04 |                                                                                Algoverse AI Research                                                                                 |               arxiv                |                                 [FAIRE: Assessing Racial and Gender Bias in AI-Driven Resume Evaluations](https://arxiv.org/abs/2504.01420)                                 |                   **Bias Evaluation**&**Resume Screening**&**Fair Hiring**                    |
| 25.04 | Northeastern University |               arxiv                | [Unequal Opportunities: Examining the Bias in Geographical Recommendations by Large Language Models](https://arxiv.org/abs/2504.05325) | **Geographical Bias**&**LLM Auditing**&**Demographic Fairness** |
| 25.04 | ML Alignment & Theory Scholars | arxiv | [Among Us: A Sandbox for Agentic Deception](https://arxiv.org/abs/2504.04072) | **Deception**&**AI Safety**&**LLM Agents** |
| 25.04 | Georgia Institute of Technology | arxiv | [Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack Narratives Targeting Mental Health Groups](https://arxiv.org/abs/2504.06160) | **Bias Auditing**&**Mental Health Stigmatization**&**LLM Safety** |
| 25.04 | University of Calabria | arxiv | [Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge](https://arxiv.org/abs/2504.07887) | **Bias Evaluation**&**Adversarial Robustness**&**LLM-as-a-Judge** |
| 25.04 | Virginia Tech | arxiv | [Reasoning Towards Fairness: Mitigating Bias in Language Models through Reasoning-Guided Fine-Tuning](https://arxiv.org/abs/2504.05632) | **Bias Mitigation**&**Reasoning Traces**&**Fairness in LLMs** |
| 25.04 | Ostim Technical University | arxiv | [Language-Dependent Political Bias in AI: A Study of ChatGPT and Gemini](https://arxiv.org/abs/2504.06436) | **Political Bias**&**Language Variation**&**LLMs** |
| 25.04 | Griffin Hospital | arxiv | [Bias in Large Language Models Across Clinical Applications: A Systematic Review](https://arxiv.org/abs/2504.02917) | **LLM Bias**&**Clinical AI**&**Systematic Review** |
| 25.04 | University of Amsterdam | arxiv | [Cognitive Debiasing Large Language Models for Decision-Making](https://arxiv.org/abs/2504.04141) | **LLM Debiasing**&**Decision-Making**&**Cognitive Bias** |
| 25.04 | University of Massachusetts Lowell | NAACL 2025 | [EqualizeIR: Mitigating Linguistic Biases in Retrieval Models](https://arxiv.org/abs/2504.07115) | **Linguistic Bias**&**Information Retrieval**&**Weak Learner Regularization** |
| 25.04 | Johns Hopkins University | arxiv | [Assessing Judging Bias in Large Reasoning Models: An Empirical Study](https://arxiv.org/abs/2504.09946) | **Judging Bias**&**Large Reasoning Models**&**Model-as-a-Judge** |
| 25.04 | Virginia Tech | arxiv | [LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety in Large Language Models](https://arxiv.org/abs/2504.10430) | **Persuasion Safety**&**Unethical Strategy**&**PERSUSAFETY** |
| 25.04 | University of Science and Culture | arxiv | [Building Trustworthy Multimodal AI: A Review of Fairness, Transparency, and Ethics in Vision-Language Tasks](https://arxiv.org/abs/2504.13199) | **Vision-Language Tasks**&**Fairness**&**Transparency**&**Ethics** |
| 25.04 | Huazhong University of Science and Technology | arxiv | [On the Feasibility of Using MultiModal LLMs to Execute AR Social Engineering Attacks](https://arxiv.org/abs/2504.13209) | **Augmented Reality**&**Multimodal LLMs**&**Social Engineering** |
| 25.04 | Zhejiang University | arxiv | [FairSteer: Inference Time Debiasing for LLMs with Dynamic Activation Steering](https://arxiv.org/abs/2504.14492) | **Inference-Time Debiasing**&**LLM Fairness**&**Activation Steering** |
| 25.04 | ETH Zurich | arxiv | [Multilingual Performance Biases of Large Language Models in Education](https://arxiv.org/abs/2504.17720) | **Multilingual Bias**&**Educational Tasks**&**LLM Evaluation** |
| 25.04 | Luxembourg Institute of Science and Technology | arxiv | [Mind the Language Gap: Automated and Augmented Evaluation of Bias in LLMs for High- and Low-Resource Languages](https://arxiv.org/abs/2504.18560) | **Bias Evaluation**&**Multilingual NLP**&**Low-Resource Languages** |
| 25.04 | Ahmedabad University | arxiv | [Who Gets the Callback? Generative AI and Gender Bias](https://arxiv.org/abs/2504.21400) | **Large Language Models (LLMs)**&**Algorithmic Bias**&**Gender Discrimination** |


## üíªPresentations & Talks


## üìñTutorials & Workshops

| Date  |   Type    |       Title        |                         URL                          |
|:-----:|:---------:|:------------------:|:----------------------------------------------------:|
| 23.10 | Tutorials | Awesome-LLM-Safety | [link](https://github.com/ydyjya/Awesome-LLM-Safety) |

## üì∞News & Articles

## üßë‚Äçüè´Scholars
